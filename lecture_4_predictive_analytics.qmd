---
title: ""
format: 
  revealjs:
    slide-number: true
    css: [assets/frankfurt.css, assets/reveal.css, assets/header.css]
    include-before-body: assets/header.html
    include-after-body: assets/header.js
---

## Navigation 

<div style="font-size: 2em; display:flex; flex-direction:column; font-weight:700; line-height:1.4;">
  <p style="color:#ff0000;margin:0;">
    4&nbsp; Predictive Analytics
  </p>

  <p style="color:#ff0000; margin:0; margin-left:80px;">
    4.1&nbsp; Subject of Predictive Analytics
  </p>

  <p style="margin-left:80px;">4.2&nbsp; The Analytics Process</p>
  <p style="margin-left:80px;">4.3&nbsp; Data Preparation</p>
  <p style="margin-left:80px;">4.4&nbsp; Methods, Algorithms and Applications</p>
  <p style="margin-left:120px;">4.4.1&nbsp; Classification</p>
  <p style="margin-left:120px;">4.4.2&nbsp; Regression</p>
</div>

---

## First Application of Data Analytics

<h2 style="text-align:center">Dr\. John Snow's Map of the 1854 London Cholera Outbreak</h2>

<div class="descriptive-wrapper">
![](images/slides_104.png)
</div>

<p style="text-align:center">Source: http://www\.udel\.edu/johnmack/frec480/cholera</p>


---


## Use Cases of Data Analytics \- Examples

<div class="descriptive-wrapper">
![](images/slides_98_new.png)
</div>


---


## Fundamental Skills of Data Analytics
<div class="descriptive-wrapper">
![](images/slides_105.png)
</div>


---


## The Data Analytics Process

<div class="descriptive-wrapper">
![](images/slides_106.png)
</div>

<span style="color:#000000">Source</span>  <span style="color:#000000">: http://www\.datasciencecentral\.com/profiles/blogs/data\-science\-simplified\-principles\-and\-process</span>


---


## Need for Knowledge about the Algorithms

<p style="font-weight:700; font-size:1.8em;">The distance between using Excel and VBA for modeling in credit scoring, for example, and using machine learning algorithms and R or Python to enhance the results, is not that great, compared to the distance between someone running a packaged algorithm they don’t really understand and someone who understands the mathematical and statistical operations within an algorithm and can optimize or adapt it as needed – and do so in the context of their deep industry experience.
</p>
Source: Dataiku \- Data Science for Banking and Insurance


---


## Statistics vs\. Machine Learning

 <span style="font-size:1.7em"><strong>Statistics</strong> about finding valid conclusions about the underlying applied theory\, and on the interpretation of parameters in their models\. It insists on proper and rigorous methodology\, and is comfortable with making and noting assumptions\. It cares about how the data was collected and the resulting properties of the estimator or experiment \(e\.g\. p\-value\)\. The focus is on hypothesis testing\.</span>

<span style="font-size:1.7em"><strong>Machine Learning \(ML\)</strong> aims to derive practice\-relevant findings from existing data and to apply the trained models to data not previously seen \(prediction\)\. It tries to predict or classify with the most accuracy\. It cares deeply about scalability and uses the predictions to make decisions\. Much of ML is motivated by problems that need to have answers\. ML is happy to treat the algorithm as a black box as long as it works\. </span>


---


## Statistical Regression vs\. Machine Learning Algorithms

<div class="descriptive-wrapper">
![](images/slides_107.png)
</div>

---

## Explanation vs\. Prediction \(I\)

<span style="color:#000000; font-size:2em">Question 1:	I have a headache\. If I take an aspirin now\, will it go away?</span>

<span style="color:#000000; font-size:2em; margin-top:0.5em">Question 2:	I had a headache\, but it passed\. Was it because I took an aspirin two hours ago? Had I not taken such an aspirin\, would I still have a headache?</span>

<div style="color:#000000; font-size:2em;margin-top:2em">The first case is a typical "predictive" question\. You are calculating the effect of a hypothetical intervention\.</div>

<div style="color:#000000; font-size:2em; margin-top:0.5em">The second case is a typical "explanatory" question\. You are calculating the effect of a counterfactual intervention\.</div>

::: {.notes}

Which one is Explanation, which Prediction?

counterfactual : den Tatsachen widersprechend

:::


---


## Explanation vs\. Prediction \(II\)

<div style="font-size: 1.4em;font-weight:600; line-height:1.2; color:#000000;">
  <h3 style=" margin-bottom:10px;">Explanation :</h3>
  <ul class="disc-list" style="margin-left:70px;">
    <li>Explanation is about understanding relationships and why certain things happen.</li>
    <li>It requires an understanding of cause and effect.</li>
    <li>Tests of causal hypotheses are fundamental.</li>
    <li>Measures of significance are central.</li>
    <li>A good explanatory model may also have predictive power.</li>
  </ul>

  <h3 style=" margin-top:30px; margin-bottom:10px;">Prediction :</h3>
  <ul class="disc-list" style="margin-left:70px;">
    <li>Prediction is about anticipating and forecasting what may happen in the future.</li>
    <li>Correlations are important in this context (but correlation does not imply causation).</li>
    <li>Therefore, predictive models may have no real explanatory power.</li>
    <li>For robust prediction, knowledge of causality is preferable.</li>
    <li>The main task is to find a model that optimally approximates reality and minimizes overfitting.</li>
    <li>Accuracy is measured using out-of-sample data.</li>
  </ul>

</div>



---

## Correlation

<div style="text-align:center;">
![](images/slides_112.png)
</div>

<p style="font-size:1.3em;text-align:center">Source: Organic Trade Association\, 2011 Organic lndustry Survey\, U\.p\. Department of Education\, Ofﬁce of Special Education Programs\, Data Analysis System \(DANS\)
</p>
<p style="color:#ff0000;font-size:1.6em;text-align:center">Organic food sales and the rate of autism seem to have a very strong correlation‚ but no one is suggesting that one causes the other\!</p>


---


## Correlation vs\. Causality \(I\)

<div style="text-align:center;">
![](images/slides_107_new.png)
</div>

<div style="font-size:1.7em;font-weight:600">
<p>Correlation: Two data series behave "similar"</p>
<p>Causality: Principle of Cause and Effect</p>
</div>


---


## Correlation vs\. Causality \(II\)

<div class="descriptive-wrapper">
![](images/slides_114.png)
</div>



---


## Correlation vs\. Causality \(III\)

<div style="font-weight:600;font-size:4em;text-align:center">

<p>But:</p>

<p>Sometimes it is better to know/predict something  even if we can not explain it instead of doing nothing!</p>
</div>


---


## Statistical Estimation

<div class="descriptive-wrapper">
![](images/slides_116.png)
</div>

Source: http://www\.dxbydt\.com/the\-size\-of\-your\-sample


---

## Navigation

<div style="font-size: 2em; display:flex; flex-direction:column; font-weight:700; line-height:1.4;">
  <p style="color:#ff0000; margin:0;">
    4&nbsp; Predictive Analytics
  </p>

  <p style="margin:0; margin-left:80px;">
    4.1&nbsp; Subject of Predictive Analytics
  </p>

  <p style="margin-left:80px;color:#ff0000;">4.2&nbsp; The Analytics Process</p>
  <p style="margin-left:80px;">4.3&nbsp; Data Preparation</p>
  <p style="margin-left:80px;">4.4&nbsp; Methods, Algorithms and Applications</p>
  <p style="margin-left:120px;">4.4.1&nbsp; Classification</p>
  <p style="margin-left:120px;">4.4.2&nbsp; Regression</p>
</div>


---

## Definitions

<div style="font-size:1.7em;font-weight:600">
A  <span style="color:#ff0000">method</span>  is a composition of formalized principles that form the basis for a stringent calculation process\.

An  <span style="color:#ff0000">algorithm</span>  is a procedure or set of steps or rules to accomplish a task\. It is usually the implementation of a method\. Algorithms are used to build models\.

In the given context\, a  <span style="color:#ff0000">model</span>  is the description of the relationship between variables\. It is used to create output data from given input data\, for example to make predictions\.

<span style="color:#ff0000">Fitting</span>  a model means that you estimate the model using the observed data\. You are using your data as evidence to help approximate the real\-world mathematical process that generated the data\. Fitting the model often involves optimization methods and algorithms\, such as maximum likelihood estimation\, to help get the parameters\.

<span style="color:#ff0000">Overfitting</span>  is the term used to mean that you used a dataset to estimate the parameters of your model\, but your model isn’t that good at capturing reality beyond your sampled data\.

</div>
<span style="color:#000000">Source: </span>  <span style="color:#000000">Schutt</span>  <span style="color:#000000">/O’Neil \(2013\):  Doing Data Science\.</span>


---


## Traditional Analytics Process

![](images/slides_117.png)

---


## Example Regression \- Fitting the model

<div class="descriptive-wrapper">
![](images/slides_121.png)
</div>

---

## Example Regression \- Testing the model

<div class="descriptive-wrapper">
![](images/slides_123.png)
</div>

---


## Data Errors and their Consequences

<div class="descriptive-wrapper">
![](images/slides_125_new.png)
</div>

---


## Modern Analytics Process

![](images/slides_126_new.png)


---


## Best Fit vs\. Best Generalization

<div class="descriptive-wrapper">
![](images/slides_129.png)
</div>


---


## Over\- and Underfitting

<div style="text-align:center;">
![](images/slides_131.png)
</div>


<p style="color:#ff0000;font-size:1.6em;border:1px solid #ff0000;padding:10px !important">Due to the problem of overfitting\, the main goal is to maximize the prediction quality and not to fit the data that is used for the model estimation as well as possible\. This is equivalent to minimizing the risk that the model will have weak predictive ability\.</p>

::: {.notes}

Fibonacci Sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55
Measurement Error: 0, 1, 1, 2, 3, 6, 8, 13, 20, 34, 55

:::


---


## The Bias\-Variance Tradeoff

<div style="display:flex;">
<div style="font-size:1.5em;font-weight:600;width:60%">
The prediction error is influenced by three components:

<p style="text-align:center">Error = Bias \+ Variance \+ Noise</p>

Bias is the inability of the used method to learn the relevant relations between the inputs and the outputs\. It reflects the method quality\, e\.g\. if a method only produces linear models\.

Variance is represents the deviation resulting from the sensitivity of the created model to small fluctuations in the data\.

Typically\, there is a tradeoff between bias and variance\.

Noise is everything that arises from random variations in the data\. It cannot be controlled\.

</div>
<div style="width:40%" class="descriptive-wrapper">
![](images/slides_132_new.png)
</div>
</div>

---


## Summarizing: Statistics vs\. Data Analytics

<div class="descriptive-wrapper">
![](images/slides_133_new.png)
</div>

---


## Which Method should I choose?

<div style="font-size:1.7em;font-weight:700">
The choice of the method of data analysis depends on the one hand on the scope of application\, but on the other hand on the interrelationships of the data to be analyzed\.

In the Big Data area\, data spaces are often highly\-dimensional\, making it difficult to visualize the interrelationships\.

For this reason\, the choice of the method can often not be made ex ante\. In these cases\, different methods are competitively tried to select the most suitable one\.
</div>


---

## Linear World

<div class="descriptive-wrapper">
![](images/slides_134_new.png)
</div>


---

## Quadratic World
 
<div class="descriptive-wrapper">
![](images/slides_135_new.png)
</div>


---


## Nonlinear World \(Type 1\)

<div class="descriptive-wrapper">
![](images/slides_136_new.png)
</div>

---


## Nonlinear World \(Type 2\)

<div class="descriptive-wrapper">
![](images/slides_137_new.png)
</div>


---


## Nonlinear World \(Type 3\)

<div class="descriptive-wrapper">
![](images/slides_138_new.png)
</div>


---


## Nonlinear World \(Type 4\)

<div class="descriptive-wrapper">
![](images/slides_139_new.png)
</div>


---

## The Data Analytics Process \- Technical View

<div class="descriptive-wrapper">
![](images/slides_133.png)
</div>

<span style="color:#000000">Source: http://blogs\.msdn\.microsoft\.com/martinkearn/2016/03/01/machine\-learning\-is\-for\-muggles\-too/</span>


---

## Navigation

<div style="font-size: 2em; display:flex; flex-direction:column; font-weight:700; line-height:1.4;">
  <p style="color:#ff0000;margin:0;">
    4&nbsp; Predictive Analytics
  </p>

  <p style="margin:0; margin-left:80px;">
    4.1&nbsp; Subject of Predictive Analytics
  </p>

  <p style="margin-left:80px;">4.2&nbsp; The Analytics Process</p>
  <p style="margin-left:80px;color:#ff0000;">4.3&nbsp; Data Preparation</p>
  <p style="margin-left:80px;">4.4&nbsp; Methods, Algorithms and Applications</p>
  <p style="margin-left:120px;">4.4.1&nbsp; Classification</p>
  <p style="margin-left:120px;">4.4.2&nbsp; Regression</p>
</div>


---


## Data Preparation and Enrichment

<p style="font-weight:600;font-size:1.5em">The data collection and preparation phase is the most labor\-intensive one\, consuming on average between 60\-80% of a data scientist’s time\. It’s critical therefore to select a tool that can automate or at least speed the workflows associated with data preparation\.
</p>
<div style="text-align:center">
![](images/slides_134.png)
</div>

<p style="text-align:right"> 2016 Dataiku\, Inc\.</p>


---

## Data Cleaning

<div>
<div>
<h3 style="font-size:1.4em;font-weight-700;">1.\ Proof of correctness of the data</h3>
<ul class="disc-list" style="font-size:1.4em">
<li>
examine for irregular outliers (e.g. Age=236) 
</li>
<li>examine for typographical errors (e.g. Frankfrut)</li>
<li>examine for different writing styles (e.g. behavior/behaviour)</li>
<li>---</li>
</ul>
</div>
<div>
<h3 style="font-size:1.4em;font-weight-700;">2.\ Handling missing values</h3>
![](images/slides_140_new.png)
</div>
</div>

---


## Missing Values Strategies

<div class="descriptive-wrapper">
![](images/slides_141_new.png)
</div>

---

## Sampling

<div style="font-weight:700;font-size:1.6em">
A population can be defined as including all people or items with the characteristic one wishes to understand\.

Sampling is about to find a representative subset of that population\.

Data represents the traces of the real\-world processes\, and exactly which traces we gather are decided by our sampling method\.

There are two sources of randomness and uncertainty:

<ol>

<li>the randomness and uncertainty underlying the process itself\, and
</li>
<li>the uncertainty associated with the underlying sampling method\.
</li>
</ol>
</div>


---


## Sampling in Times of Big Data

<div style="font-size:1.7em; font-weight:700">
Question:

Is there any need for sampling in times of Big Data? Why not "N=ALL"?

Answer:

Data is not objective\! Data does not speak for itself\. Data is just a quantitative echo of the events of our society\.

Examples:

<ul class="disc-list">

<li>
When analyzing the probability of customers terminating the relationship\, a very small proportion of terminating customers \(e\.g\. 0\.2%\) on the whole may result in a bias\.
</li>
<li>
When analyzing political attitudes via social media data\, there might be a bias if people with specific attitudes are posting more frequently\.
</li>
</ul>

</div>

---


## Reasons for Sampling

<ul class="disc-list" style="font-size:1.7em;font-weight:600">
<li>
The volume of data is too large to capture and process
</li>
<li>
Design the analytics process using a subset of the data for performance reasons\. Later use the complete data set\.
</li>
<li>
The data set doesn't perfectly represent the target population\.
</li>
<li>
The data set is imbalanced\.
</li>
<li>
Use sampling to partition into training and test data\.
</li>
<li>
\.\.\.
</li>
</ul>


---


## Popular Methods of Sampling \(I\)

<div class="descriptive-wrapper">
![](images/slides_135.png)
</div>

---


## Popular Methods of Sampling \(II\)

<div class="descriptive-wrapper">
![](images/slides_167.png)
</div>

--- 


## Systematic Sampling

<div class="descriptive-wrapper">
![](images/slides_176.png)
</div>


---

## Random Sampling

<div class="descriptive-wrapper">
![](images/slides_177.png)
</div>

---


## Proportional Sampling

<div class="descriptive-wrapper">
![](images/slides_178.png)
</div>



---

## Downsampling

<div class="descriptive-wrapper">
![](images/slides_179.png)
</div>


---

## SMOTE

<div style="display:flex">
<div style="width:60%">
<p style="color:#000000; font-size:1.6em;font-weight:700">SMOTE \(Synthetic Minority Oversampling Technique\) is an oversampling technique where the synthetic samples are generated for the minority class\. </p>
<p style="color:#000000; font-size:1.6em;font-weight:700">At first the total number of oversampling observations N is set up\. Usually\, it is selected such that the resulting class distribution is 1:1\. Now\, the iteration starts by first selecting a minority class instance at random\. Next\, the k nearest neighbors for that instance are obtained\. For every neighbor calculate the difference as distance and multiply this difference by a random value between 0 and 1\.Adding the result to the chosen instance creates a new synthetic instance\. This is done until the number of needed instances is reached\. </p>
</div>

<div style="float:right;width:40%">
![](images/slides_180.png)
</div>
</div>


<p style="float:right">Source: https://github\.com/minoue\-xx/Oversampling\-Imbalanced\-Data</p>

---


## Feature Engineering

<div style="font-size:1.4em;font-weight:700;margin-bottom:3em">
<span style="color:#000000">Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work\. If feature engineering is done correctly\, it increases the predictive power of machine learning algorithms by creating features from raw data that help facilitate the machine learning process\. </span>

<span style="color:#000000">A feature \(variable\, attribute\) is depicted by a column in a dataset\. Considering a generic two\-dimensional dataset\, each observation is depicted by a row and each feature by a column\, which will have a specific value for an observation:</span>
<div style="text-align:center">
![](images/slides_144_new.png)
</div>
<span style="color:#000000">Features can be of two major types\. </span>  <span style="color:#ff0000">Raw features</span>  <span style="color:#000000"> are obtained directly from the dataset with no extra data manipulation or engineering\. </span>  <span style="color:#ff0000">Derived features</span>  <span style="color:#000000"> are usually obtained from feature engineering\, where we extract features from existing data attributes\. A simple example would be creating a new feature “Age” from an employee dataset containing "Birthdate"\.</span>

</div>

Sources:	Sarkar\, D\.: Understanding Feature Engineering\, towardsdatascience\.com and Shekhar\, A\.: What Is Feature Engineering for Machine Learning?\, medium\.com\.


---


## Variants of Feature Engineering

<div style="font-size:1.2em">
  <h3>1.\ Transformation</h3>
  <ul class="disc-list">
    <li>convert features (e.g., birth date → age)</li>
    <li>build lag structures (e.g., time-lags)</li>
    <li>normalization / standardization / scaling</li>
  </ul>

  <h3>2.\ Type Conversion</h3>
  <ul class="disc-list">
    <li>if numerical type is needed, transform categorical into numerical data using dummy features</li>
    <li>if categorical type is needed or more informative, discretize numerical features (e.g., income → poor / rich classes)</li>
  </ul>

  <h3>3.\ Feature Combination</h3>
  <ul class="disc-list">
    <li>create interaction features (e.g., school_score = num_schools × median_school  
      with num_schools = number of schools within 5 miles of a property and  
      median_school = median quality score of those schools)
    </li>
    <li>combine categories (e.g., when there are very few observations or too many dummy features)</li>
  </ul>

  <h3>4.\ Feature Composition</h3>
  <ul class="disc-list">
    <li>build ratios (e.g., returns from prices)</li>
    <li>Principal Component Analysis (Dimensionality Reduction)</li>
  </ul>

</div>


---

## Scaling

<div style="font-size:1.4em;font-weight:700">
<p style="color:#000000">Most datasets contain features highly varying in magnitudes\, units and range\.</p>
<p>Most machine learning algorithms have problems with this because they use distance measures or calculate gradients\. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes and gradients may end up taking a long time or are not accurately calculable\.</p>
<p style="color:#000000">To overcome this effect\, we scale the features to bring them to the same level of magnitudes\. The two most discussed scaling methods are Normalization and Standardization\. </p>

<div>
![](images/slides_146_new.png)
</div>
</div>


---


## Type Conversion \(Encoding\)

<div style="font-size:1.4em;font-weight:700">
<span style="color:#000000">Many machine learning algorithms cannot work with categorical data directly\. To convert categorical data to numbers\, there exist two variants:</span>

<span style="color:#ff0000">Label encoding</span>  <span style="color:#000000"> refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them\. Every categorical value is assigned to one numerical value\, e\.g\. young \-> 1\, </span>  <span style="color:#000000">middle\_age</span>  <span style="color:#000000"> \-> 2\, old \-> 3\. This only works in specific situations where you have somewhat continuous\-like data\, e\.g\. if the categorical feature is ordinal\.</span>

<span style="color:#ff0000">One hot encoding</span>  <span style="color:#000000"> is a representation of a categorical variable as binary vectors\. Every categorical value is assigned to an artificial binary variable\. If the corresponding categorical value occurs in a data row the value of its binary replacement is equal to 1 else 0\, e\.g\. </span>
<div style="text-align:center">
![](images/slides_147_new.png)
</div>
<span style="color:#000000">It is usual when creating dummy variables to have one less variable than the number of categories present to avoid perfect collinearity \(dummy variable trap\)\.</span>
</div>


---


## Example of Feature Engineering \(I\)

<div style="font-size:1.4em;font-weight:700">
<p style="color:#000000">Data sets often contain date/time features\. These features are rarely useful in their original form because they only contain ongoing values\. However\, they can be useful for extracting cyclical factors\, such as weekly or seasonal effects\. Suppose\, we are given a data "flight date time vs status"\. Then\, given the date\-time data\, we have to predict the status of the flight\.</p>
<div style="text-align:center">
![](images/slides_183.png)
</div>
<p style="color:#000000">But the status of the flight may depend on the hour of the day\, not on the date\-time\. To analyze this\, we will create the new feature " Hour\_Of\_Day"\. Using the "Hour\_Of\_Day" feature\, the machine will learn better as this feature is directly related to the status of the flight\.</p>

</div>

<p style="float:right">Source: Shekhar\, A\.: What Is Feature Engineering for Machine Learning?\, medium\.com\.</p>


---


## Example of Feature Engineering \(II\)

<div style="font-size:1.4em;font-weight:700">
<span style="color:#000000">Suppose we are given the latitude\, longitude and other data with the objective to predict the target feature "</span>  <span style="color:#000000">Price\_Of\_House</span>  <span style="color:#000000">"\. Latitude and longitude are not of use in this context if they are alone\. So\, we will combine the latitude and the longitude to make one feature\. </span>

<span style="color:#000000">In other cases\, it might be appropriate to transform latitude and longitude into categories which reflect regions\, for example</span>
<div style="text-align:center">
![](images/slides_185.png)
</div>
</div>


---


## Example of Feature Engineering \(III\)

<div style="font-size:1.2em;font-weight:700;">
<span style="color:#000000">Suppose we are given a feature "</span>  <span style="color:#000000">Marital\_Status</span>  <span style="color:#000000">" and other data with the objective to classify customers into "Creditworthy" and "</span>  <span style="color:#000000">Not\_Creditworthy</span>  <span style="color:#000000">"\. In the data set the martial status has many different values\, for example </span>

<span style="color:#000000">&#x25CF; single living alone</span>

<span style="color:#000000">&#x25CF; single living with his parents</span>

<span style="color:#000000">&#x25CF; married living together</span>

<span style="color:#000000">&#x25CF; married living separately</span>

<span style="color:#000000">&#x25CF; divorced</span>

<span style="color:#000000">&#x25CF; divorced but living together</span>

<span style="color:#000000">&#x25CF; registered partnerships</span>

<span style="color:#000000">&#x25CF; living in marriage\-like community</span>

<span style="color:#000000">&#x25CF; widowed</span>

<span style="color:#000000">&#x25CF; \.\.\.</span>

<span style="color:#000000">To avoid a transformation into too many and maybe dominating dummy features\, we can group the similar classes\, e\.g\. in single\, married\, widowed\.</span>

<span style="color:#000000">If there exist some remaining sparse classes which cannot be assigned in a meaningful way they can be joined into a single "other" class\.</span>
</div>


---


## Partitioning the Data



<div style="font-size:1.5em;font-weight:600">
<p>
The partitioning of the data in <span style="color:#ff0000">Training and Test Data</span>  has the aim to proof if the analytical results can be generalized. The analysis (e.g. the development of a classifier) is carried out on the basis of training data. Subsequently, the results are applied to the test data. If the results are significantly worse than the training data, the model is not generalizable, which is called overfitting.
</p>
<div style="text-align:center">
![](images/slides_186.png)
</div>
<p>
The partitioning of the data in training and test data can be carried out in the following ways: 
<ul class="disc-list">
<li>By random/stratified/... sampling (problem with the repeatability)
</li>
<li>according to a list
</li>
<li>according to rules (e.g. the first/last 50 records or every twelfth)
</li>
</ul>
</p>
</div>


---


## Applying Training and Test Data


<div style="text-align:center">
![](images/slides_152_new.png)
</div>

<p style="text-align:right">Source: http://www.cs.kent.edu/~jin/BigData/Lecture10-ML-Classification.pptx</p>


---

## Partitioning

![](images/slides_187.png)


---


## Exploratory Data Analysis

<div style="font-size:1.5em;font-weight:600">
In Exploratory Data Analysis \(EDA\)\, there is no hypothesis and there is no model\.

People are not very good at looking at a column of numbers or a whole data table and then determining important characteristics of the data\. EDA techniques have been devised as an aid in this situation\.

Reasons for EDA:

&#x25CF; gain intuition about the data

&#x25CF; make comparisons between distributions

&#x25CF; sanity checking \(making sure the data is on the scale you expect\, in the format you thought it should be\)

&#x25CF; find out where data is missing or if there are outliers

&#x25CF; summarize the data

Exploratory data analysis is generally cross\-classed in two ways\. First\, each method is either non\-graphical or graphical\. And second\, each method is either univariate or multivariate\.

</div>

---


## Univariate Non\-Graphical EDA

<div style="font-size:1.5em;font-weight:600">
Non\-graphical exploratory data analysis is the first step when beginning to analyze the data\. This preliminary data analysis step focuses on four points:

&#x25CF; measures of central tendency\, i\.e\. mean and median\. The median\, known as 50th percentile\, is more resistant to outliers\.

&#x25CF; measures of spread\, i\.e\. variance\, standard deviation\, and interquartile range

&#x25CF; the shape of the distribution

&#x25CF; the existence of outliers

The characteristics of interest for a categorical variable are simply the range of values and the frequency of occurrence for each value\.

<div style="text-align:center">
![](images/slides_188.png)
</div>
</div>


---


## Tests on Outliers

<div style="font-size:1.5em;font-weight:600">
Outlier are data objects\, which are clearly different from the others\.

Usually\, the detection of outliers is an unsupervised process\, because they are not known before analyses\.

In the case of  <span style="color:#ff0000">numerical attributes</span>  the Interquartil Range can be used\. Here\, an outlier is defined if the attribute lies outside the interval
<div style="text-align:center">
![](images/slides_187_new.png)
</div>



Usually\, k has a value between 1\.5 and 3\. The bigger k\, the more different  the values must be to be classified as outliers\.

Can be visualized by a Box\-and\-Whisker Plot:
<div style="text-align:right">
![](images/slides_189.png)
</div>
</div>


---


## Handling Outliers

<div style="font-size:1.4em;font-weight:600">
* Outlier have to be  <span style="color:#ff0000">eliminated</span>  if they
  * 1.\ would bias the analysis\, e\.g\. if 9 persons have an age between 20 and 30 and the 10th person is 80 years old\.
  * 2.\ are erogenous data\, e\.g\. as a result of input errors or a defect sensor\.
* It is not always acceptable to drop an observation just because it is an outlier\.  They can be legitimate observations and are sometimes interesting ones\.  It’s important to investigate the nature of the outlier before deciding\.
* In those cases where you shouldn’t drop the outlier\, one option is to try a transformation\.  <span style="color:#ff0000">Log transformations </span> pull in high numbers\.  This can reduce the impact of a single point if the outlier is an independent variable\.
</div>

<div style="text-align:center">
![](images/slides_190.png)
</div>


---


## Univariate Graphical EDA

<div style="font-size:1.4em;font-weight:600">
Non\-graphical and graphical EDA methods complement each other\, they have the same focus\. While the non\-graphical methods are quantitative and objective\, they do not give a full picture of the data\. The distribution of a variable tells us what values the variable takes and how often each value occurs\.

Types of displays:

for numerical variables: Histograms\, Boxplots\, Quantile\-normal plots\, …

for categorical variables: Pie charts\, Bar graphs\, …
</div>

<div style="display:flex;justify-content:center">
<div style="height:600px;width:400px">
![](images/slides_193.png)
</div>
<div style="height:600px;width:400px">
![](images/slides_191.png)
</div>
<div style="height:600px;width:400px">
![](images/slides_192.png)
</div>
</div>


---

## Multivariate Non\-Graphical EDA

<div style="font-size:1.4em;font-weight:600">
Multivariate non\-graphical EDA techniques generally show the relationship between two or more variables in the form of either cross\-tabulation for categorical variables or correlation statistics for numerical variables\.
</div>
<div style="display:flex;justify-content:center;margin-top:4em">
<div class="Multivariate-wrapper" >
![](images/slides_194.png)
</div>
<div class="Multivariate-wrapper">
![](images/slides_195.png)
</div>
</div>


---


## Multivariate Graphical EDA

<div style="font-size:1.5em;font-weight:600">
Multivariate graphical EDA techniques are scatterplots for numerical variables\, Barcharts for categorical variables\, or Boxplots for mixed types\.

<div style="text-align:center">
![](images/slides_196.png)
</div>
</div>



---

## Touring Diagram

<div class="descriptive-wrapper">
![](images/slides_199.gif)
</div>

---

## Navigation

<div style="font-size: 2em; display:flex; flex-direction:column; font-weight:700; line-height:1.4;">
  <p style="color:#ff0000;margin:0;">
    4&nbsp; Predictive Analytics
  </p>

  <p style="margin:0; margin-left:80px;">
    4.1&nbsp; Subject of Predictive Analytics
  </p>

  <p style="margin-left:80px;">4.2&nbsp; The Analytics Process</p>
  <p style="margin-left:80px;">4.3&nbsp; Data Preparation</p>
  <p style="margin-left:80px;color:#ff0000;">4.4&nbsp; Methods, Algorithms and Applications</p>
  <p style="margin-left:120px;">4.4.1&nbsp; Classification</p>
  <p style="margin-left:120px;">4.4.2&nbsp; Regression</p>
</div>


---


## Categories in Machine Learning

<div style="text-align:center">
![](images/slides_200.png)
</div>


---

## Supervised Learning

<div style="text-align:center">
![](images/slides_203.png)
</div>



---

## Unsupervised Learning

<div style="text-align:center">
![](images/slides_207.png)
</div>


---


## Supervised and Unsupervised Learning


<div style="text-align:center">
![](images/slides_209.png)
</div>


---

## Use Cases Quiz

<div style="text-align:center">
![](images/slides_210.png)
</div>

---


## Reinforcement Learning

<div style="font-size:1.5em;font-weight:600">
The solution to many of the problems in our lives cannot be automated\. This is not because current computers are too slow\, but simply because it is too difficult for humans to determine what the program should do\.

Supervised learning is a general method for training an approximator\. However\, supervised learning requires sample input\-output pairs from the domain to be learned\.

For example\, we might not know the best way to program a computer to recognize an infrared picture of a tank\, but we do have a large collection of infrared pictures\, and we do know whether each picture contains a tank or not\. Supervised learning could look at all the examples with answers\, and learn how to recognize tanks in general\.

Unfortunately\, there are many situations where we don’t know the correct answers that supervised learning requires\. For example\, in a self\-driving car\, the question would be the set of all sensor readings at a given time\, and the answer would be how the controls should react during the next millisecond\.

For these cases there exist a different approach known as reinforcement learning\.
</div>


---


## Reinforcement Learning

<div class="volumeImageWrapper" style="text-align:center">
![](images/slides_211.jpg)
</div>
<div style="font-size:1.5em;font-weight:600">
The agent learns how to achieve a given goal by trial\-and\-error interactions with its environment by maximizing a reward\.
</div>


---

## AlphaGo

<div style="font-size:1.6em;font-weight:600">
Go is one of the hardest games in the world for AI because of the huge number of different game scenarios and moves\. The number of potential legal board positions is greater than the number of atoms in the universe\.

<div style="display:flex;">
<div style="width:50%">
The core of AlphaGo is a deep neural network\. It was initially trained to learn playing by using a database of around 30 million recorded historical moves\. After the training\, the system was cloned and it was trained further playing large numbers of games against other instances of itself\, using reinforcement learning to improve its play\. During this training AlphaGo learned new strategies which were never played by humans\.

A newer version named AlphaGo Zero skips the step of being trained and learns to play simply by playing games against itself\, starting from completely random play\.
</div>
<div style="width:50%">
![](images/slides_212.png)
</div>
</div>

</div>

::: {.notes}

Chess 2^64 legal positions, Go 2^120!!!

:::


---

## Libratus

<div style="font-size:1.4em;font-weight:600">
An artificial intelligence called Libratus has beaten four of the world’s best poker players in a grueling 20\-day tournament in January 2017\.

Poker is more difficult because it’s a game with imperfect information\. With chess and Go\, each player can see the entire board\, but with poker\, players don’t get to see each other’s hands\. Furthermore\, the AI is required to bluff and correctly interpret misleading information in order to win\.

“We didn’t tell Libratus how to play poker\. We gave it the rules of poker and said ‘learn on your own’\.” The AI started playing randomly but over the course of playing trillions of hands was able to refine its approach and arrive at a winning strategy\.
<div class="volumeImageWrapper" style="float:right">
![](images/slides_214.jpg)
</div>
</div>



---


## Types of Artificial Intelligence

<div style="font-size:1.7em;font-weight:600">
<span style="color:#ff0000">Discriminative AI</span>  is designed to differentiate and classify input\, but not to create new content\. Examples include image or speech recognition\, credit scoring or stock price prediction\.

<span style="color:#ff0000">Generative AI</span>  is able to generate new content based on existing information and user specifications\. This includes texts\, images\, videos\, program code\, etc\. The generated content can often hardly be distinguished from human\-generated content\. As things stand at present\, however\, they are pure recombinations of learned knowledge\.

Well\-known examples of generative AI are language models for generating text\, such as GPT\-3 or GPT\-4\, and the chatbot ChatGPT based on them\, or image generators such as Stable Diffusion and DALL\-E\.
</div>


---

## ChatGPT

<div style="font-size:1.8em;font-weight:600">
ChatGPT is a generative AI that produces human\-like text and communicates with humans\.

The "GPT" in ChatGPT comes from the language model of the same name\, which was extended for ChatGPT with various components for communication and quality assurance\.

GPT is based on a huge neural network that essentially represents the language model\. While the first GPT\-3 has 175 billion parameters\, the newer GPT\-4 already has 1 trillion parameters\. Compared to GPT\-3\, GPT\-4 is therefore more intelligent\, can deal with more extensive questions and conversations and makes fewer factual errors\.
</div>


---


## ChatGPT \- Approach

<div style="font-size:1.6em;font-weight:600">
ChatGPT generates its response word by word via a sequence of probabilities\, with each new word depending on the previous ones\.

<div style="text-align:center">
![](images/slides_216.png)
</div>

The most probable word is not always selected; instead\, randomization takes place\. This means that different variants can be created for the same task\.
<div style="text-align:center">
![](images/slides_217.png)
</div>
</div>


---


## ChatGPT \- Semantic Spaces \(I\)

<div style="text-align:center">
![](images/slides_218.png)
</div>


---


## ChatGPT \- Semantic Spaces \(II\)

<div style="text-align:center">
![](images/slides_176_new.png)
</div>

---


## ChatGPT \- Evaluation Component

<div style="text-align:center">
![](images/slides_220.png)
</div>


---

## Navigation

<div style="font-size: 1.6em; display:flex; flex-direction:column; font-weight:700; line-height:1;">
  <p style="margin:0;">
    4&nbsp; Predictive Analytics
  </p>

  <p style="margin:0; margin-left:80px;">
    4.1&nbsp; Subject of Predictive Analytics
  </p>

  <p style="margin:0;margin-left:80px;">4.2&nbsp; The Analytics Process</p>
  <p style="margin:0;margin-left:80px;">4.3&nbsp; Data Preparation</p>
  <p style="margin:0;margin-left:80px;">4.4&nbsp; Methods, Algorithms and Applications</p>
  <p style="margin:0;margin-left:120px;color:#ff0000;">4.4.1&nbsp; Classification</p>
  <p style="margin:0;margin-left:160px;">4.4.1.1&nbsp; K-Nearest Neighbors</p>
  <p style="margin:0;margin-left:160px;">4.4.1.2&nbsp; Evaluating the Quality of Classification</p>
  <p style="margin:0;margin-left:160px;">4.4.1.3&nbsp; Decision Tree Approaches</p>
  <p style="margin:0;margin-left:160px;">4.4.1.4&nbsp; Logistic Regression</p>
  <p style="margin:0;margin-left:160px;">4.4.1.5&nbsp; Neural Networks</p>
  <p style="margin:0;margin-left:160px;">4.4.1.6&nbsp; Resampling</p>
  <p style="margin:0;margin-left:160px;">4.4.1.7&nbsp; Ensemble Learning</p>
  <p style="margin:0;margin-left:120px;">4.4.2&nbsp; Regression</p>
</div>


---


## Introductory Example

<div style="font-size:1.4em;font-weight:600">
Credit\-Scoring is a typical example for a classification problem\. A bank wants to determine the creditworthiness of a customer\.

<div style="float:left;width:60%">
Assume you have the age\, income\, and a creditworthiness category of "yes" or "no" for a bunch of people and you want to use the age and income to predict the creditworthiness for a new person\.

You can plot people as points on the plane and label people with an empty circle if they have low credit ratings\.

What if a new guy comes in who is 49years old and who makes 53\,000 Euro? What is his likely credit rating label?
</div>

<div style="float:right;">
![](images/slides_221.png)
</div>
</div>


---


<div style="font-size: 1.6em; display:flex; flex-direction:column; font-weight:700; line-height:1;">
  <p style="margin:0;">
    4&nbsp; Predictive Analytics
  </p>

  <p style="margin:0; margin-left:80px;">
    4.1&nbsp; Subject of Predictive Analytics
  </p>

  <p style="margin:0;margin-left:80px;">4.2&nbsp; The Analytics Process</p>
  <p style="margin:0;margin-left:80px;">4.3&nbsp; Data Preparation</p>
  <p style="margin:0;margin-left:80px;">4.4&nbsp; Methods, Algorithms and Applications</p>
  <p style="margin:0;margin-left:120px;">4.4.1&nbsp; Classification</p>
  <p style="margin:0;margin-left:160px;color:#ff0000;">4.4.1.1&nbsp; K-Nearest Neighbors</p>
  <p style="margin:0;margin-left:160px;">4.4.1.2&nbsp; Evaluating the Quality of Classification</p>
  <p style="margin:0;margin-left:160px;">4.4.1.3&nbsp; Decision Tree Approaches</p>
  <p style="margin:0;margin-left:160px;">4.4.1.4&nbsp; Logistic Regression</p>
  <p style="margin:0;margin-left:160px;">4.4.1.5&nbsp; Neural Networks</p>
  <p style="margin:0;margin-left:160px;">4.4.1.6&nbsp; Resampling</p>
  <p style="margin:0;margin-left:160px;">4.4.1.7&nbsp; Ensemble Learning</p>
  <p style="margin:0;margin-left:120px;">4.4.2&nbsp; Regression</p>
</div>

---


## k\-Nearest Neighbors


<div style="font-size:1.6em;font-weight:600">
* k\-Nearest Neighbors \(k\-NN\) is an algorithm that can be used when you have a bunch of objects that have been classified or labeled in some way\, and other similar objects that have not gotten classified or labeled yet\, and you want a way to automatically label them\.
* The intuition behind k\-NN is to consider the most similar other items defined in terms of their attributes\, look at their labels\, and give the unassigned item the majority vote\. If there’s a tie\, you randomly select among the labels that have tied for first\.
* <span style="color:#1a1a1a">Procedure of k\-NN:</span>
  * <span style="color:#1a1a1a">1. Determine parameter k \(= number of nearest neighbors\)</span>
  * <span style="color:#1a1a1a">2. Calculate the distances between the new object and all known labeled  objects\.</span>
  * <span style="color:#1a1a1a">3. Choose the k objects from all known labeled objects having the smallest distance to the new object as nearest neighbors\.</span>
  * <span style="color:#1a1a1a">4. Count the frequencies of the classes of the nearest neighbors\.</span>
  * <span style="color:#1a1a1a">5. Assign the new object to the most frequent class\.</span>
</div>

---

## Measuring Similarity

<div style="text-align:center">
![](images/slides_182_new.png)
</div>

---

## Unnormalized vs. Normalized

<div style="text-align:center">
![](images/slides_183_new.png)
</div>

---

## Example (I)

<div style="text-align:center">
![](images/slides_184_new.png)
</div>

---

## Example (II)

<div style="text-align:center">
![](images/slides_185_new.png)
</div>

---

## Example (III)

<div style="font-size:1.4em;font-weight:600">
3\.	Choose the k  <span style="color:#1a1a1a">nearest neighbors</span>
<div style="font-size:1em !important;font-weight:400!important; border:1px solid; margin:30px">
| <span style="color:#000000">Customer</span> | <span style="color:#000000">Age</span> | <span style="color:#000000">Monthly</span>  <span style="color:#000000">Income</span> | <span style="color:#000000">Monthly</span>  <span style="color:#000000">Costs</span> | <span style="color:#000000">Creditworthy</span> | <span style="color:#000000">Distance</span> |
| :-: | :-: | :-: | :-: | :-: | :-: |
| <span style="color:#000000">A</span> | <span style="color:#000000">0\.0000</span> | <span style="color:#000000">0\.0303</span> | <span style="color:#000000">0\.0400</span> | <span style="color:#000000">yes</span> | <span style="color:#000000">0\.4347</span> |
| <span style="color:#000000">C</span> | <span style="color:#000000">0\.1714</span> | <span style="color:#000000">0\.3333</span> | <span style="color:#000000">0\.3600</span> | <span style="color:#000000">yes</span> | <span style="color:#000000">0\.1726</span> |
| <span style="color:#000000">E</span> | <span style="color:#000000">0\.3143</span> | <span style="color:#000000">0\.1818</span> | <span style="color:#000000">0\.2000</span> | <span style="color:#000000">no</span> | <span style="color:#000000">0\.2010</span> |
| <span style="color:#000000">F</span> | <span style="color:#000000">0\.4286</span> | <span style="color:#000000">0\.3939</span> | <span style="color:#000000">0\.6000</span> | <span style="color:#000000">no</span> | <span style="color:#000000">0\.4482</span> |
| <span style="color:#000000">G</span> | <span style="color:#000000">0\.4857</span> | <span style="color:#000000">0\.2121</span> | <span style="color:#000000">0\.1200</span> | <span style="color:#000000">yes</span> | <span style="color:#000000">0\.3090</span> |
| <span style="color:#000000">X</span> | <span style="color:#000000">0\.2286</span> | <span style="color:#000000">0\.3636</span> | <span style="color:#000000">0\.2000</span> | <span style="color:#000000">?</span> | <span style="color:#000000"> </span> |
</div>

4\.	Count the numbers of class members

<span style="color:#ff3300">3 x yes</span> ; 2 x no

5\.	Assign object to most frequent class

<span style="color:#ff3300">Customer is creditworthy\!</span>
</div>



---

## Creation and Use of Models

<div style="text-align:center">
![](images/slides_188_new.png)
</div>

---


## Calculating Accuracies

<div style="text-align:center">
![](images/slides_189_new.png)
</div>


---


## Determining Parameter k

<div style="font-size:1.7em;font-weight:600;float:left;">
1\.	Split the original labeled dataset into training and test data\.

2\.	Pick an evaluation metric\. Misclassification rate or accuracy are good ones\.

3\.	Run k\-NN a few times\, changing k and checking the evaluation measure\.

4\.	Optimize k by picking the one with the best evaluation measure\.
</div>
<div style="float:right">
| <span style="color:#1a1a1a">k</span> | <span style="color:#1a1a1a">Accuracy</span>  <span style="color:#1a1a1a"> </span> |
| :-: | :-: |
| <span style="color:#1a1a1a">1</span> | <span style="color:#1a1a1a">0\.720</span> |
| <span style="color:#1a1a1a">2</span> | <span style="color:#1a1a1a">0\.685</span> |
| <span style="color:#1a1a1a">3</span> | <span style="color:#1a1a1a">0\.740</span> |
| <span style="color:#1a1a1a">4</span> | <span style="color:#1a1a1a">0\.745</span> |
| <span style="color:#1a1a1a">5</span> | <span style="color:#ff0000">0\.770</span> |
| <span style="color:#1a1a1a">6</span> | <span style="color:#1a1a1a">0\.740</span> |
| <span style="color:#1a1a1a">7</span> | <span style="color:#1a1a1a">0\.750</span> |
| <span style="color:#1a1a1a">8</span> | <span style="color:#1a1a1a">0\.750</span> |
| <span style="color:#1a1a1a">9</span> | <span style="color:#1a1a1a">0\.765</span> |
| <span style="color:#1a1a1a">10</span> | <span style="color:#1a1a1a">0\.760</span> |
</div>




---

## Navigation 

<div style="font-size: 1.6em; display:flex; flex-direction:column; font-weight:700; line-height:1;">
  <p style="margin:0;">
    4&nbsp; Predictive Analytics
  </p>

  <p style="margin:0; margin-left:80px;">
    4.1&nbsp; Subject of Predictive Analytics
  </p>

  <p style="margin:0;margin-left:80px;">4.2&nbsp; The Analytics Process</p>
  <p style="margin:0;margin-left:80px;">4.3&nbsp; Data Preparation</p>
  <p style="margin:0;margin-left:80px;">4.4&nbsp; Methods, Algorithms and Applications</p>
  <p style="margin:0;margin-left:120px;">4.4.1&nbsp; Classification</p>
  <p style="margin:0;margin-left:160px;">4.4.1.1&nbsp; K-Nearest Neighbors</p>
  <p style="margin:0;margin-left:160px;color:#ff0000;">4.4.1.2&nbsp; Evaluating the Quality of Classification</p>
  <p style="margin:0;margin-left:160px;">4.4.1.3&nbsp; Decision Tree Approaches</p>
  <p style="margin:0;margin-left:160px;">4.4.1.4&nbsp; Logistic Regression</p>
  <p style="margin:0;margin-left:160px;">4.4.1.5&nbsp; Neural Networks</p>
  <p style="margin:0;margin-left:160px;">4.4.1.6&nbsp; Resampling</p>
  <p style="margin:0;margin-left:160px;">4.4.1.7&nbsp; Ensemble Learning</p>
  <p style="margin:0;margin-left:120px;">4.4.2&nbsp; Regression</p>
</div>


---


## Evaluating the quality of Classification \(I\)

<div style="font-size:1.6em;font-weight:600">
True positives \(TP\)\, true negatives \(TN\)\, false positives \(FP\)\, and false negatives \(FN\)\, are the four different possible outcomes of a single prediction for a two\-class case\. A false positive is when the outcome is incorrectly classified as "yes"\, when it is in fact "no"\. A false negative is when the outcome is incorrectly classified as negative\, when it is in fact positive\. True positives and true negatives are obviously correct classifications\.
</div>

<div style="text-align:center">
![](images/slides_223.png)
</div>


---


## Evaluating the quality of Classification \(II\)

<div style="font-size:1.6em;font-weight:600">
Test metrics are used to assess how accurately the model predicts the known values:
<div style="text-align:center">
![](images/slides_192_new.png)
</div>
Most classification algorithms pursue to minimize the misclassification rate\. They implicitly assume that all misclassification errors cost equally\. In many real\-world applications\, this assumption is not true\. Cost\-sensitive learning takes costs\, such as the misclassification cost\, into consideration\. Using costs\, the error rate can be calculated via:
<div style="text-align:center">
![](images/slides_1922_new.png)
</div>
</div>


---


## Evaluating the quality of Classification \(III\)

<div style="font-size:1.3em;font-weight:600">
Misclassification rate and accuracy can be misleading\, for example in the case of imbalanced samples\. Extreme case:

<div style="text-align:center">
![](images/slides_193_new.png)
</div>

For problems like\, this additional measures are required to evaluate a classifier\.

<span style="color:#ff0000">Sensitivity</span>  \(true positive rate\, recall\) measures the proportion of positives that are correctly identified as such\.  <span style="color:#ff0000">Specificity</span>  \(true negative rate\) measures the proportion of negatives that are correctly identified as such\.
<div style="text-align:center">
![](images/slides_193_2_new.png)
</div>

Using both measures\, we can compute the  <span style="color:#ff0000">Balanced Accuracy</span>
<div style="text-align:center">
![](images/slides_193_3_new.png)
</div>
</div>


---


## Problem of Imbalancing and Accuracy

<div style="font-size:1.6em;font-weight:600">
Assume the following case: A credit card company wants to create a fraud detection system to include it into their transactional systems\. The outcomes should be "Accept" \(Y\) and "Reject" \(N\)\. Because fraud rarely occurs\, the data set consists of 320 observations for Y and 139 for N\. They are partitioned into training and test set\. Finally\, the model is trained and tested\.

Because of the majority of the Y class\, the training process concentrates on these cases because their correct classification promises the highest accuracy\.

The results of the test of the model is consequently:

<div>
![](images/slides_194_new.png)
</div>

Thus\, the model is blind for the N cases\. But these are the ones of primary interest for the company\.
</div>


---


## Evaluating the quality of Classification \(IV\)

<div style="display:flex; font-size:1.3em;font-weight:600">
<div style="width:50%">
<span style="color:#ff0000">Precision</span>  measures the proportion of predicted positives who are true positives\. A precision of 0\.5 means that whenever the model classifies a positive\, there is a 50% chance of it really being a positive\.The higher the precision the smaller the number of false positives\.
<div>
![](images/slides_195_new.png)
</div>
<span style="color:#ff0000">Recall</span>  measures the percentage of positives the model is able to catch\. It is defined as the number of true positives divided by the total number of positives in the dataset\. A recall of 50% would mean that 50% of the positives had been predicted as such by the model while the other 50% of positives have been missed by the model\.
</div>
<div style="width:50%;text-align:center;">
![](images/slides_224.png)
</div>

</div>
<p style="text-align:right">Source: Wikipedia</p>


---


## Evaluating the quality of Classification \(V\)

<div style="font-size:1.6em;font-weight:600">
The F1 Score can be interpreted as the weighted average of both precision and recall\. The main idea of the F1 Score is to strike a balance between both precision and recall and measure it in a single metric\.
<div style="text-align:center">
![](images/slides_196_new.png)
</div>
A F1 score reaches its best value at 1 \(perfect precision and recall\) and worst at 0\.

It is commonly used in cases of high class imbalance\.
</div>


---


## Creation and Use of Models

<div style="text-align:center">
![](images/slides_197_new.png)
</div>

---


## Navigation 

<div style="font-size: 1.6em; display:flex; flex-direction:column; font-weight:700; line-height:1;">
  <p style="margin:0;">
    4&nbsp; Predictive Analytics
  </p>

  <p style="margin:0; margin-left:80px;">
    4.1&nbsp; Subject of Predictive Analytics
  </p>

  <p style="margin:0;margin-left:80px;">4.2&nbsp; The Analytics Process</p>
  <p style="margin:0;margin-left:80px;">4.3&nbsp; Data Preparation</p>
  <p style="margin:0;margin-left:80px;">4.4&nbsp; Methods, Algorithms and Applications</p>
  <p style="margin:0;margin-left:120px;">4.4.1&nbsp; Classification</p>
  <p style="margin:0;margin-left:160px;">4.4.1.1&nbsp; K-Nearest Neighbors</p>
  <p style="margin:0;margin-left:160px;">4.4.1.2&nbsp; Evaluating the Quality of Classification</p>
  <p style="margin:0;margin-left:160px;color:#ff0000;">4.4.1.3&nbsp; Decision Tree Approaches</p>
  <p style="margin:0;margin-left:160px;">4.4.1.4&nbsp; Logistic Regression</p>
  <p style="margin:0;margin-left:160px;">4.4.1.5&nbsp; Neural Networks</p>
  <p style="margin:0;margin-left:160px;">4.4.1.6&nbsp; Resampling</p>
  <p style="margin:0;margin-left:160px;">4.4.1.7&nbsp; Ensemble Learning</p>
  <p style="margin:0;margin-left:120px;">4.4.2&nbsp; Regression</p>
</div>


---


## Which one is better?

<div style="text-align:center">
![](images/slides_199_new.png)
</div>

---


## Introductory Example

<div style="text-align:center">
![](images/slides_225.png)
</div>

::: {.notes}

To choose the relevant features, you can apply several methods and after this you may train a selection model using for example logistic regression. The whole process would be very time-consuming and even costly. As an alternative, you can apply a decision tree algorithm which is a stepwise or recursive classification mechanism. 

Proportions in the leafs can be interpreted as probabilities.


:::


---


## Decision Trees \(I\)

<div style="font-size:1.6em;font-weight:600">
Decision trees belong to the hierarchical methods of classification\. They analyze step\-by\-step \(recursive partitioning\)\.

A decision tree consists of nodes and borders\. The topmost node \(without any parent node\) is called "root"\. A node without a child node is called "leaf"\. Nodes that have parent and  child nodes are called "interior nodes"\. The interior nodes represent the splitting of the included object sets\. An interior node has at least two child nodes \(sons\)\. If every interior node has exactly two child nodes\, the tree is called a "binary tree"\.

A decision tree method starts at the root\, which includes all objects\. The different features are compared \(with an adequate measure\) regarding their suitability of classification\. The most appropriate feature determines the branching of the current set of objects: regarding this feature\, the current set of objects is divided into disjoint subsets \(partitioning\)\. This method is now used recursively to the created child nodes \(subsets\)\.
</div>


---


## Decision Trees \(II\)

<div style="font-size:1.6em;font-weight:600">
<p>
Graphically\, decision tree models divide the dataspace in a large number of subspaces and search for the variables which are able to split the dataspace with the greatest homogeneity\. We can think of the decision tree as a map of different path\. For a distinct combination of predictor variables and their observed values\, we would enter a specific path\, which gives the classification in the leaf of the decision tree\.
</p>
<div style="float:left;width:50%">
The decision tree approach does not require any assumption about the functional form of variables or distributions\. Furthermore in contrast to parametric models like linear regressions\, the decision tree algorithm can model multiple structures as well as complex relationships within the data\, which would be difficult to replicate in a linear model\.
</div>
<div class="volumeImageWrapper" style="float:right">
![](images/slides_241.png)
</div>
</div>



---


## Decision Trees \(III\)

<div class="descriptive-wrapper">
![](images/slides_242.png)
</div>

<p style="text-align:center">Source: http://iopscience\.iop\.org/article/10\.1088/1749\-4699/5/1/015004</p>

---


## Overview of important Decision Tree Methods

<table class="comparison-table">
  <tr>
    <th>Name</th>
    <th>CART</th>
    <th>ID3</th>
    <th>C5.0</th>
    <th>CHAID</th>
    <th>Random Forests</th>
  </tr>
  <tr>
    <td><b>Idea</b></td>
    <td>Choose the attribute with the highest information content</td>
    <td>One of the first methods from Quinlan; uses the concept of information gain</td>
    <td>Like ID3 based on the concept of information gain</td>
    <td>Choose the attribute that is most dependent on the target variable</td>
    <td>Construct many trees with different sets of features and samples (randomly). Result by voting.</td>
  </tr>
  <tr>
    <td><b>Measure used</b></td>
    <td>Gini-Index</td>
    <td>Information gain (entropy)</td>
    <td>Ratio of information gain</td>
    <td>Chi-square split</td>
    <td>Optional, mostly Gini-Index</td>
  </tr>
  <tr>
    <td><b>Type of Splitting</b></td>
    <td>Binary</td>
    <td>Complete, pruning</td>
    <td>Complete, pruning</td>
    <td>Complete, pruning</td>
    <td>Complete</td>
  </tr>
</table>

---

## Introductory Example

<div class="descriptive-wrapper">
![](images/slides_205_new.png)
</div>

---

## Splitting with Entropy in ID3

<div class="descriptive-wrapper">
![](images/slides_206_new.png)
</div>

---

## Calculating the Information Gain

<div style="font-size:1.6em;font-weight:600">
The information gain is a measure, that shows (by combination of the entropies) the appropriateness of an attribute for splitting:
<div style="text-align:center">
![](images/slides_207_1_new.png)
</div>
where m = number of values (here two: light, strong), ti = number of data sets with strong or light wind (8 resp. 6), t = total number of data sets (14) and entropy(t) = entropy before splitting.
<div style="text-align:center">
![](images/slides_207_2_new.png)
</div>
</div>

---


## Decision using ID3

<div style="font-size:1.6em;font-weight:600">
Information gain \(outlook\) = 0\.246

Information gain \(humidity\) = 0\.151

Information gain \(wind\) = 0\.048

Information gain \(temperature\) = 0\.029

We choose the attribute with the largest information gain \(here: outlook\) for the first splitting\.

As solution we obtain the following tree:

<div style="text-align:center;margin-top:2em">
![](images/slides_243.png)
</div>
</div>


---


## Decision using C5\.0

<div style="font-size:1.3em;font-weight:600">
<p>ID3 tends to favor attributes that have a large number of values, resulting in larger trees. For example, if we have an attribute that has a distinct value for each record, then the entropy is 0, thus the information gain is maximal. </p>
<p>To compensate for this, C5.0 is a further development that uses the information gain ratio as a splitting criterion:</p>
<div style="text-align:center">
![](images/slides_209_new.png)
</div>
<p>In the case of our example the GainRatio of Windy is</p>
<div style="text-align:center">
![](images/slides_209_2_new.png)
</div>
<p>
and the GainRatio of Outlook is 
</p>
<div style="text-align:center">
![](images/slides_209_2_new.png)
</div>
</div>

---


## Handling Numerical Attributes

<div style="font-size:1.6em;font-weight:600">

<p>Numerical attributes are usually splitted binary. In contrast to categorical attributes many possible splitting points exist .</p>
<p>The splitting point with the highest information gain is looked for. For this, the potential attribute is sorted according to its values first and then all possible splitting point and the corresponding information gains are calculated. In extreme cases there exists n-1 possibilities. </p>
<div style="text-align:center">
![](images/slides_210_new.png)
</div>
</div>

---


## The CART Algorithm

<div style="font-size:1.6em;font-weight:600">
<p>The CART algorithm (Classification And Regression Trees) constructs trees that have only binary splits. Like C5.0, it is able to handle categorical and  numerical attributes. </p>
<p>As a measure for the impurity of a node t, CART uses the Gini Index. In the case of two classes the Gini Index is defined as:</p>
<div>
![](images/slides_211_new.png)
</div>
</div>

---


## Splitting in CART

<div style="text-align:center">
![](images/slides_212_new.png)
</div>

---


## Coherence between Entropy and Gini Index

<div style="text-align:center">
![](images/slides_213_new.png)
<p style="color:#000000;font-size:1.6em"> __Remark: Entropy has been scaled from \(0\, 1\) to \(0\, 0\.5\)\!__ </p>
</div>

---

## Overfitting (I)

<div style="font-size:1.6em;font-weight:600">
<div>
<p>Most decision tree algorithms partition training data until every node contains objects of a single class, or until further partitioning is impossible because two objects have the same value for each attribute but belong to different classes. If there are no such conflicting objects, the decision tree will correctly classify all training objects.</p>
</div>
<div>
<p style="float:left;width:65%">If tree performance is measured from the number of correctly classified cases it is com-mon to find that the training data gives an over-optimistic guide to future performance,i.e. with new data. A tree should exhibit generalization, i.e. work well with data other than those used to generate it. When the tree grows during training it often shows a decrease in generalization. This is because the deeper nodes are fitting noise in the training data not representative over the entire universe from which the training set was sampled. This is called 'overfitting'.
</p>
<div style="float:right">
![](images/slides_244.png)
</div>
</div>
</div>


---


Overfitting \(II\)

The Iearner overfits to correctly classify‚ the noisy data objects

Noisy or dirty data objects

![](images/slides_245.png)


---


Random Forest \(I\)

<span style="color:#000000">Random forest is an ensemble classifier that consists of many decision trees\. </span>

<span style="color:#000000">For every tree a  subset of the data objects and a subset of features is randomly chosen\. Then the tree is constructed usually using the Gini Index\. </span>

<span style="color:#000000">In the end\, a simple majority vote is taken for prediction\.</span>

![](images/slides_246.png)

<span style="color:#000000">Algorithm</span>  <span style="color:#000000">:</span>

<span style="color:#000000">1\.	Create n samples from the original data\. Frequent sample size is 2/3\.</span>

<span style="color:#000000">2\.	For each of the samples\, grow a tree\, with the following modification: at each node\, rather than choosing the best split among all predictors\, randomly sample m\* of the m predictors and choose the best split from among those variables\.</span>

<span style="color:#000000">3\.	Predict by aggregating the predictions of the n trees \(majority votes\)\.</span>


---


<span style="color:#000000"> __Random Forest \(II\)__ </span>

<span style="color:#000000">Voting\-Principle of Random Forest:</span>

<span style="color:#000000">To</span>  <span style="color:#000000"> </span>  <span style="color:#000000">avoid</span>  <span style="color:#000000"> </span>  <span style="color:#000000">overfitting</span>  <span style="color:#000000"> </span>  <span style="color:#000000">effects</span>  <span style="color:#000000">\, </span>  <span style="color:#000000">the</span>  <span style="color:#000000"> </span>  <span style="color:#000000">size</span>  <span style="color:#000000"> </span>  <span style="color:#000000">and</span>  <span style="color:#000000"> </span>  <span style="color:#000000">the</span>  <span style="color:#000000"> </span>  <span style="color:#000000">depth</span>  <span style="color:#000000"> </span>  <span style="color:#000000">of</span>  <span style="color:#000000"> </span>  <span style="color:#000000">the</span>  <span style="color:#000000"> </span>  <span style="color:#000000">trees</span>  <span style="color:#000000"> </span>  <span style="color:#000000">can</span>  <span style="color:#000000"> </span>  <span style="color:#000000">be</span>  <span style="color:#000000"> </span>  <span style="color:#000000">restricted</span>  <span style="color:#000000">\. </span>

![](images/slides_247.png)


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.1\.1	K\-Nearest Neighbors

4\.4\.1\.2	Evaluating the Quality of Classification

4\.4\.1\.3	Decision Tree Approaches

<span style="color:#c00000">4\.4\.1\.4	Logistic Regression</span>

4\.4\.1\.5	Neural Networks

4\.4\.1\.6	Resampling

4\.4\.1\.7	Ensemble Learning

4\.4\.2	Regression


---


Introductory Example

![](images/slides_248.png)

![](images/slides_249.png)

<span style="color:#000000"> __websites \(features\)__ </span>

<span style="color:#000000">1=visited</span>

<span style="color:#000000">0=not visited</span>

<span style="color:#000000"> __ad \(target\)__ </span>

<span style="color:#000000">1=clicked</span>

<span style="color:#000000">0=not clicked</span>

<span style="color:#ff0000">Giant sparse matrix\!</span>

<span style="color:#ff0000">One matrix for every ad\!</span>

::: {.notes}

Giant sparse matrix! A sparse matrix is a matrix in which most of the elements are zero.
One matrix for every ad!
Can be solved by Naïve Bayes! Looking for an alternative

:::


---


Why not classical linear regression?

It is possible to implement  a linear regression on such a dataset where Y=\{0\,1\}\.

Problems:

The predicted values of the linear model can be greater than 1 or less than 0

e is not normally distributed because Y takes on only two values

The error terms are heteroscedastic \(the error variance is not constant for all values of X\)

Source: Bichler \(2015\): Course Business Analytics\, TU München


---


Logistic regression \(I\)

Logistic regression is a regression model where the dependent variable is categorical\. The classical logistic regression is a binary classifier\, where the dependent variable has two states\. The output of a logistic regression model ranges between 0 and 1\.

Logistic regression uses the logistic function \(or Sigmoid function\) because it can take an input with any value from negative to positive infinity\, whereas the output always takes values between zero and one and hence is interpretable as a probability\.

It is defined as:

![](images/slides_250.png)


---


Logistic regression \(II\)

If we set

the logistic function can now be written as:

We interpret F\(x\) as the conditional probability that the class attribute has the value 1 with the given input vector x\.

The coefficients ß0 and ß can be estimated via Maximum Likelihood Estimation\.

The parameter ß0 represents the unconditional probability of "Y=1" knowing nothing about the feature vector x\.

The parameter vector β defines the slope of the logit function\. It determines the extent to which certain features contribute for increased or decreased likelihood to "Y=1"\.

The output of a logistic model is a probability\. To use this for classification purposes:

If the predicted probability is > 0\.5 the label is 1

and otherwise 0\.

::: {.notes}

odd = Chance

:::


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.1\.1	K\-Nearest Neighbors

4\.4\.1\.2	Evaluating the Quality of Classification

4\.4\.1\.3	Decision Tree Approaches

4\.4\.1\.4	Logistic Regression

<span style="color:#c00000">4\.4\.1\.5	Neural Networks</span>

4\.4\.1\.6	Resampling

4\.4\.1\.7	Ensemble Learning

4\.4\.2	Regression


---


Functionality of Human Neurons

![](images/slides_251.png)

![](images/slides_252.png)

A Look into the Nervous System

Design of a Neuron


---


An Easy Example \(I\)

<span style="color:#000000"> __f\(x\) = Activation function__ </span>

<span style="color:#000000"> __e\.g\. __ </span>

<span style="color:#000000"> __where t = Stimulus threshold__ </span>


---


An Easy Example \(II\)

<span style="color:#000000"> __f\(x\) = Activation function__ </span>

<span style="color:#000000"> __e\.g\. __ </span>

<span style="color:#000000"> __where t = Stimulus threshold__ </span>


---


Functionality of a Neuron

![](images/slides_253.jpg)


---


<span style="color:#000000"> __For the case of n inputs\, we can rewrite the neuron's function to__ </span>

<span style="color:#000000"> __with b = \-t\. b is known as the perceptron's bias\. The result of this function would then be fed into an activation function to produce a labeling__ </span>

![](images/slides_254.png)

<span style="color:#000000"> __This results in a linear classifier\. Finally\, we have to pick a line that best separates the labeled data\. The training of the perceptron consists of feeding it multiple training samples and calculating the output for each of them\. After each sample\, the weights w are adjusted in such a way so as to minimize the output error\, defined for example as accuracy or MSE\.__ </span>

<span style="color:#000000">Source: http://www\.toptal\.com/machine\-learning/an\-introduction\-to\-deep\-learning\-from\-perceptrons\-to\-deep\-networks</span>


---


The Multilayer Perceptron

<span style="color:#000000"> __The single perceptron approach has a major drawback: it can only learn linear functions\. To address this problem\, we’ll need to use a multilayer perceptron\, also known as feedforward neural network\. Here\, we add layers between the input and the output layer\, so\-called hidden layers__ </span>  <span style="color:#000000">\. The hidden layer is where the network stores it's internal abstract representation of the training data\.</span>

<span style="color:#000000">Input Neurons</span>  <span style="color:#000000">: </span>  <span style="color:#000000">receive</span>  <span style="color:#000000"> </span>  <span style="color:#000000">signals</span>  <span style="color:#000000"> </span>  <span style="color:#000000">from</span>  <span style="color:#000000"> </span>  <span style="color:#000000">the</span>  <span style="color:#000000"> </span>  <span style="color:#000000">outer</span>  <span style="color:#000000"> </span>  <span style="color:#000000">world</span>  <span style="color:#000000">\.</span>

<span style="color:#000000">Hidden Neurons</span>  <span style="color:#000000">: </span>  <span style="color:#000000">have</span>  <span style="color:#000000"> an internal </span>  <span style="color:#000000">representation</span>  <span style="color:#000000"> </span>  <span style="color:#000000">of</span>  <span style="color:#000000"> </span>  <span style="color:#000000">the</span>  <span style="color:#000000"> </span>  <span style="color:#000000">outer</span>  <span style="color:#000000"> </span>  <span style="color:#000000">world</span>  <span style="color:#000000">\.</span>

<span style="color:#000000">Output Neurons</span>  <span style="color:#000000">: pass </span>  <span style="color:#000000">signals</span>  <span style="color:#000000"> </span>  <span style="color:#000000">to</span>  <span style="color:#000000"> </span>  <span style="color:#000000">the</span>  <span style="color:#000000"> </span>  <span style="color:#000000">outer</span>  <span style="color:#000000"> </span>  <span style="color:#000000">world</span>  <span style="color:#000000">\.</span>


---


Types of Activation Functions

<span style="color:#000000">A linear composition of linear functions is still just a linear function\, so most neural networks use non\-linear activation functions:</span>

<span style="color:#000000"> __tangens__ </span>  <span style="color:#000000"> __ __ </span>  <span style="color:#000000"> __hyperbolicus__ </span>

<span style="color:#000000">logistic</span>  <span style="color:#000000">function</span>  <span style="color:#000000">\(sigmoid\)</span>


---


Design of a Multilayer Perceptron


---




* The Backpropagation algorithm is used for calculating the weights\. In a training phase\, the weights are iteratively calculated using training data sets in such a way that the difference between the calculated and the expected \(true\) results is minimized\. Because the simultaneous calculation of all weights is not possible\, they must be found via a learning process\. The backpropagation algorithm looks for the minimum of the error function in weight space using the method of gradient descent\.
* The procedure in principle:
  * \(1\)	Define the initial weights
  * \(2\)	Put the training set into the input layer
  * \(3\)	Calculate the result \(value of the output layer\) via successive processing one layer after the other
  * \(4\)	Compare the output values and target values and calculate the difference
  * \(5\)	Iterate steps \(2\) to \(4\) for every training set
  * \(6\)	Calculate the total error\. Adjust the weights beginning with the output layer towards the input layer \(backpropagation\)
  * \(7\)	Iterate steps \(2\) to \(6\) until the total error reaches the defined error\-level or the number of maximum iterations is reached\.

---


Adjusting the Weights \(I\)

<span style="color:#000000">The error of a training set </span>  <span style="color:#000000">i</span>  <span style="color:#000000"> is calculated using the quadratic deviation between the values </span>  <span style="color:#000000">o</span>  <span style="color:#000000">ij</span>  <span style="color:#000000"> of the neurons of the output layer and their corresponding true values </span>  <span style="color:#000000">t</span>  <span style="color:#000000">ij</span>  <span style="color:#000000">\. </span>

<span style="color:#000000">The sum of the errors of all h training objects is the total error value E:</span>

![](images/slides_256.png)


---


Adjusting the Weights \(II\)

<span style="color:#000000">The function E has to be minimized\. Because it depends on the output neurons </span>  <span style="color:#000000">o</span>  <span style="color:#000000">j</span>  <span style="color:#000000">\, it automatically depends on their weights to the </span>  <span style="color:#000000">precedent layer\(s\)</span>  <span style="color:#000000">:</span>

<span style="color:#000000">Thus\, the weights have to be found where E is minimal\. </span>

<span style="color:#000000">Examples of Error functions with two weights:</span>

![](images/slides_257.png)

![](images/slides_258.png)


---


Adjusting the Weights \(III\)

<span style="color:#000000">To minimize the error \(cost\) function E the backpropagation algorithm uses the method of gradient descent</span>  <span style="color:#000000">\. This method searches those weights\, where the </span>  <span style="color:#000000">vector containing the partial first derivatives of the error function        </span>  <span style="color:#000000">\(gradient\) </span>  <span style="color:#000000">is equal to the zero vector </span>  <span style="color:#000000">\(minimum\):</span>

<span style="color:#000000">To adjust the weight </span>  <span style="color:#000000">w</span>  <span style="color:#000000">ij</span>  <span style="color:#000000">\, which connects neurons </span>  <span style="color:#000000">i</span>  <span style="color:#000000"> </span>  <span style="color:#000000">to</span>  <span style="color:#000000"> j\, the formula is:</span>

<span style="color:#000000">where </span>  <span style="color:#000000">a </span>  <span style="color:#000000">represents a predefined learning rate</span>  <span style="color:#000000">\, </span>  <span style="color:#000000">which defines the step length of each iteration in the negative gradient direction </span>  <span style="color:#000000">and x</span>  <span style="color:#000000">i</span>  <span style="color:#000000"> denote the output value of neuron </span>  <span style="color:#000000">i</span>  <span style="color:#000000">\.</span>

<span style="color:#000000">The adjusted weight is then computed via</span>

![](images/slides_259.png)


---


Principle of Gradient Descent \(I\)

<span style="color:#000000">Gradient descent is used to find the minimum of the error function</span>  <span style="color:#000000">\. It works iterative\. In an 1\-dimensional world\, we define the error by</span>

<span style="color:#000000">The error function is at minimum if the error is equal to zero\.</span>

<span style="color:#000000">The prediction is the result of a combination of input and weight</span>

<span style="color:#000000">The weight as the dynamic component is now adjusted until the error is at minimum\. Starting</span>  <span style="color:#000000"> with an initial weight\, gradient descent jumps step by step into the minimum by adjusting the weight\. The adjustment is done by calculating the direction </span>  <span style="color:#000000">and</span>  <span style="color:#000000"> the amount for a step via</span>

<span style="color:#000000">Now\,  the weight is adjusted via</span>

<span style="color:#000000">After repeating this several times\, the minimum</span>  <span style="color:#000000">is reached\.</span>


---


Principle of Gradient Descent \(II\)

<span style="color:#000000">The formula </span>

<span style="color:#000000">represents the derivative of the error to the weight\.</span>

<span style="color:#000000">A derivative is a term that is calculated as the slope \(or gradient\) of a graph at a particular point\. The slope is described by drawing a tangent line to the graph at the point\. So\, if we are able to compute this tangent line\, we might be able to compute the desired direction to reach the minima\.</span>

<span style="color:#000000">Since the weight only indirectly affects the error\, the chain rule must be applied</span>


---


Principle of Gradient Descent \(III\)

<span style="color:#000000">Gradient Descent isn't perfect\. When the gradients are too big we might overshoot so much that we're even farther away than we started</span>

<span style="color:#000000">This problem is destructive because overshooting this far means we land at an even steeper slope in the opposite direction\. This causes us to overshoot again even farther\.</span>

<span style="color:#000000">If the gradients are too big\, we can make them smaller\. We do this by multiplying them by a single number between 0 and 1 \(such as 0\.01\)\. This fraction is typically named alpha\.</span>

<span style="color:#000000">Thus\, the adjustment of the weights is done by</span>

Source: https://iamtrask\.github\.io/2015/07/27/python\-network\-part2/


---


Backpropagation Step by Step \(I\)

<span style="color:#000000">In the following\,</span>  <span style="color:#000000"> the backpropagation process will be demonstrated using a simple Neural </span>  <span style="color:#000000">Network consisting of  three layers: Input layer with two inputs neurons\, one hidden layer with two neurons\, and output layer with a single neuron: </span>

<span style="color:#000000">Our initial weights will be: w</span>  <span style="color:#000000">1</span>  <span style="color:#000000"> = 0\.11\, w</span>  <span style="color:#000000">2</span>  <span style="color:#000000"> = 0\.21\, w</span>  <span style="color:#000000">3</span>  <span style="color:#000000"> = 0\.12\, w</span>  <span style="color:#000000">4</span>  <span style="color:#000000"> = 0\.08\, w</span>  <span style="color:#000000">5</span>  <span style="color:#000000"> = 0\.14 and w</span>  <span style="color:#000000">6</span>  <span style="color:#000000"> = 0\.15\.</span>

![](images/slides_262.png)

![](images/slides_263.png)

Source: http://hmkcode\.github\.io/ai/backpropagation\-step\-by\-step


---


Backpropagation Step by Step \(II\)

<span style="color:#000000">Our dataset has one sample with two inputs and one output with the values inputs=\[2\, 3\] and output=\[</span>  <span style="color:#ff0000">1</span>  <span style="color:#000000">\]\. We will use given weights and inputs to predict the output\. Inputs are multiplied by weights; the results are then </span>  <span style="color:#000000">passed forward to next layer:</span>

<span style="color:#000000"> </span>  <span style="color:#ff0000">For reasons of simplification\, </span>  <span style="color:#ff0000">no activation function is used </span>  <span style="color:#ff0000">in the neurons\.</span>

![](images/slides_264.png)

Source: http://hmkcode\.github\.io/ai/backpropagation\-step\-by\-step


---


Backpropagation Step by Step \(III\)

<span style="color:#000000">The network output\, or prediction\, is not even close to actual output\. We can calculate the difference or the error as following:</span>

<span style="color:#000000">Our main goal of the training is to reduce the error or the difference between prediction and actual output\. Since actual output is constant\, “not changing”\, the only way to reduce the error is to change prediction value\. The question now is\, how to change prediction value?</span>

![](images/slides_265.png)

Source: http://hmkcode\.github\.io/ai/backpropagation\-step\-by\-step


---


Backpropagation Step by Step \(IV\)

<span style="color:#000000">By decomposing prediction into its basic elements we can find that weights are the variable elements affecting prediction value\. To change prediction value\, we need to adjust the weights:</span>

<span style="color:#000000">We</span>  <span style="color:#000000"> do this </span>  <span style="color:#000000">using Backpropagation\. To find a local minimum of a function using gradient descent\, one takes steps proportional to the negative of the gradient of the function at the current point:</span>

<span style="color:#000000">For example\, we update w</span>  <span style="color:#000000">6</span>  <span style="color:#000000">:</span>

![](images/slides_266.png)

![](images/slides_267.png)

_We can picture gradient descent optimization as a hiker \(the weight coefficient\) who wants to climb down a mountain \(cost function\) into a valley \(cost minimum\)\, and each step is determined by the steepness of the slope \(gradient\) and the leg length of the hiker \(learning rate\)\._

![](images/slides_269.png)

Source: http://hmkcode\.github\.io/ai/backpropagation\-step\-by\-step


---


Backpropagation Step by Step \(V\)

<span style="color:#000000">The derivation of the error function is evaluated by applying the chain rule:</span>

<span style="color:#000000">To update w</span>  <span style="color:#000000">6</span>  <span style="color:#000000"> we can apply the following formula:</span>

<span style="color:#000000">Similarly\, we can derive the update formula for w5 and any other weights existing between the output and the hidden layer:</span>

![](images/slides_271.png)

![](images/slides_272.png)

Source: http://hmkcode\.github\.io/ai/backpropagation\-step\-by\-step


---


Backpropagation Step by Step \(VI\)

<span style="color:#000000">When moving backward to update w</span>  <span style="color:#000000">1</span>  <span style="color:#000000">\, w</span>  <span style="color:#000000">2</span>  <span style="color:#000000">\, w</span>  <span style="color:#000000">3</span>  <span style="color:#000000"> and w</span>  <span style="color:#000000">4</span>  <span style="color:#000000"> existing between input and hidden layer\, the partial derivative for the error function with respect to w</span>  <span style="color:#000000">1</span>  <span style="color:#000000">\, for example\, will be as following:</span>

<span style="color:#000000">We can find the update formula for the remaining weights w</span>  <span style="color:#000000">2</span>  <span style="color:#000000">\, w</span>  <span style="color:#000000">3</span>  <span style="color:#000000"> and w</span>  <span style="color:#000000">4</span>  <span style="color:#000000"> in the same way\.</span>

Source: http://hmkcode\.github\.io/ai/backpropagation\-step\-by\-step


---


Backpropagation Step by Step \(VII\)

<span style="color:#000000">In summary\, the update formulas for all weights will be:</span>

<span style="color:#000000">We can rewrite the update formulas in matrices:</span>

![](images/slides_274.png)

![](images/slides_275.png)

Source: http://hmkcode\.github\.io/ai/backpropagation\-step\-by\-step


---


Backpropagation Step by Step \(VIII\)

<span style="color:#000000">With the derived formulas we can now adjust the weights:</span>

![](images/slides_276.png)

Source: http://hmkcode\.github\.io/ai/backpropagation\-step\-by\-step


---


Backpropagation Step by Step \(IX\)

<span style="color:#000000">\.\.\. and use the new weights to recalculate the example:</span>

<span style="color:#000000">The new prediction 0\.26 is bit closer to the output than the previously predicted one 0\.191\. We repeat now the same process until error is close or equal to zero\.</span>

![](images/slides_277.png)

Source: http://hmkcode\.github\.io/ai/backpropagation\-step\-by\-step


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.1\.1	K\-Nearest Neighbors

4\.4\.1\.2	Evaluating the Quality of Classification

4\.4\.1\.3	Decision Tree Approaches

4\.4\.1\.4	Logistic Regression

4\.4\.1\.5	Neural Networks

<span style="color:#c00000">4\.4\.1\.6	Resampling</span>

4\.4\.1\.7	Ensemble Learning

4\.4\.2	Regression


---


Problems with fixed Training and Test Samples

Method 1       optimize

Test data is used for two things:

<span style="color:#ff0000">Optimize the model training</span>

<span style="color:#0070c0">Select the best model via testing the model quality</span>

Method 2       optimize

Method 3       optimize

This contradicts the idea of independent testing and results in:

Endogenization of the test data

Selection Bias

…                   optimize

<span style="color:#ff0000">Rule</span>  <span style="color:#ff0000">: NEVER </span>  <span style="color:#ff0000">use</span>  <span style="color:#ff0000"> </span>  <span style="color:#ff0000">any</span>  <span style="color:#ff0000"> </span>  <span style="color:#ff0000">information</span>  <span style="color:#ff0000"> </span>  <span style="color:#ff0000">from</span>  <span style="color:#ff0000"> </span>  <span style="color:#ff0000">the</span>  <span style="color:#ff0000"> </span>  <span style="color:#ff0000">test</span>  <span style="color:#ff0000"> </span>  <span style="color:#ff0000">data</span>  <span style="color:#ff0000"> </span>  <span style="color:#ff0000">for</span>  <span style="color:#ff0000"> </span>  <span style="color:#ff0000">model</span>  <span style="color:#ff0000"> </span>  <span style="color:#ff0000">training</span>  <span style="color:#ff0000">\!</span>


---


Addressing the Endogeneity Problem

![](images/slides_278.png)

![](images/slides_279.png)

![](images/slides_280.png)

![](images/slides_282.png)

_Predictive_  _ Model_

_Validation Sample_

![](images/slides_283.png)


---




* Training and test error can be highly variable\, depending on precisely which observations are included in the training set and which observations are included in the validation set \( <span style="color:#ff0000">Selection Bias</span> \)\.
  * Example of differentOLS models as a result of different samples:
  * To avoid such problems\, one can use so\-called resampling methods\.

---


Cross Validation



  * In Data Science cross validation can be used for model selection and adjustment\. In these cases\, cross validation is applied to the training data set\. For every iteration\, k\-1 folds are used for model fitting and the remaining fold for testing the model \(Validation\)\. Every time\, the quality measure \(e\.g\. accuracy\) for the validation fold is captured\. At the endof this step\, the average and the standard deviation of the measures are calculated\. The best model is the one with the best ratio in high average and low standard deviation\.
  * Once the model type and its optimal parameters have been selected\, a final model is trained using these hyper\-parameters on the  _full_  training set\, and the generali\-zation quality is measured on the test set\.


![](images/slides_285.png)


---


Cross Validation and Grid Search



  * Partition the Dataset into a training and test set
  * For every hyperparameter value combination apply cross validation
  * For the combination with the highest \(mean\) quality calculate the final model with the complete training set
  * Test the final model with the test set
  * Compare the accurracies of training and test with regard to overfitting


Calculate the mean quality of the validation folds\, e\.g\. mean accurracy or mean F1


---


Cross Validation and Grid Search in Python



  * Performs cross validation with the given hyperparameter combinations and manages the evaluation process


Using the original libraries and functions

<span style="color:#ff0000">KNeighborsClassifier\(\)</span>

<span style="color:#ff0000">cross\_val\_score\(\)</span>



  * Performs cross validation


<span style="color:#ff0000">DecisionTreeClassifier\(\)</span>

<span style="color:#ff0000">RandomForestCl</span>  <span style="color:#ff0000">…</span>  <span style="color:#ff0000">\(\)</span>


---


Variants of Hyperparameter Optimization



  * 1\. Grid Search
  * Grid search sequentially goes through a preselected list of permutations for each hyperparameter and evaluates the entire search space\.
  * 2\. Random Search
  * Random search selects values for hyperparameters at random within a predefined distribution\.
  * While a grid search is able to find the best model given the provided options\, limited compute resources means that in practice\, the search space selected will have to be limited\. A random search on the other hand does not iterate over the entire search space\.

---


Other Variants of Cross Validation



  * 1\. Repeated Cross Validation
  * 2\. Nested Cross Validation
  * Different composition of the folds by random selection\.


![](images/slides_295.png)


---


Cross Validation in Time Series



  * In the case of time series\, classical cross validation may cause problems\. If we choose random samples and assign them to either the test set or the training set we are quickly in the situation of using values from the future to forecast values in the past\. But we want to avoid future\-looking when we train our model\. If there is a temporal dependency between observations\, we must preserve that relation during training and testing\.
  * A procedure that can be used for cross validating a time series model is cross validation on a rolling basis\. Start with a small subset of data for training purpose\, forecast for the later data points and then check the accuracy for the forecasted data points\. The time frame for the forecast is then included as part of the next training dataset and subsequent data points are forecasted and so on\.
  * Scikit\-learn provides a class _TimeSeriesSplit_  to do this\.


![](images/slides_296.png)


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.1\.1	K\-Nearest Neighbors

4\.4\.1\.2	Evaluating the Quality of Classification

4\.4\.1\.3	Decision Tree Approaches

4\.4\.1\.4	Logistic Regression

4\.4\.1\.5	Neural Networks

4\.4\.1\.6	Resampling

<span style="color:#c00000">4\.4\.1\.7	Ensemble Learning</span>

4\.4\.2	Regression


---


Ensemble Methods

Ensemble methods use different models \(created via different data sets\, feature sets or methods\) that are simultaneously applied to the same problem\. The results are sent to an aggregating operation that produces the final result\.

The most widely used classes of ensemble methods are:

Bagging

Boosting

Stacking


---


<span style="color:#ff0000">Bagging</span>  <span style="color:#000000"> means to build multiple models from different subsamples of the training dataset and/or with different methods\. The results are sent to an \(weighted\) voting operation that produces the final result\.</span>

<span style="color:#000000">Source: http://rasbt\.github\.io/mlxtend/</span>  <span style="color:#000000">user\_guide</span>  <span style="color:#000000">/classifier/</span>  <span style="color:#000000">EnsembleVoteClassifier</span>  <span style="color:#000000">/</span>

![](images/slides_298.png)


---


<span style="color:#ff0000">Boosting</span>  <span style="color:#000000"> involves sequentially building an ensemble by training each new model instance to emphasize the training instances that previous models </span>  <span style="color:#000000">mispredict</span>  <span style="color:#000000">\. Different variants exist\, mostly based on tree methods\. In general\, any method can be used\. This involves the usage of different methods at the different iterations when building the sequence of models\. </span>

<span style="color:#000000">Source: https://blog\.bigml\.com/2017/03/14/introduction\-to\-boosted\-trees/</span>

![](images/slides_299.png)


---


<span style="color:#ff0000">Stacking</span>  <span style="color:#000000"> </span>  <span style="color:#000000">means to build </span>  <span style="color:#000000">multiple models \(typically of differing types\) and a supervisor model that learns how to best combine the predictions of the primary models\. </span>  <span style="color:#000000">The inputs of the </span>  <span style="color:#000000">supervisor model </span>  <span style="color:#000000">\(meta\-classifier\)</span>  <span style="color:#000000">are the outputs of </span>  <span style="color:#000000">the other models:</span>

<span style="color:#000000">Source: http://rasbt\.github\.io/mlxtend/</span>  <span style="color:#000000">user\_guide</span>  <span style="color:#000000">/classifier/</span>  <span style="color:#000000">StackingClassifier</span>  <span style="color:#000000">/</span>

![](images/slides_300.png)


---


Types of Ensembles

Type 1:

consists of only a few models

each is a strong model

like few professional experts

risk of diverging opinions

risk of experts being biased to their experiences

Type 2:

consists of many models

each is a weak model as a principle

based on the idea of the wisdom of the masses

Random Forest and Gradient Boosted Trees are examples

![](images/slides_301.png)


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

<span style="color:#ff0000">4\.4\.2	Regression</span>

4\.4\.2\.1	OLS

4\.4\.2\.2	Ridge Regression

4\.4\.2\.3	Support Vector Regression

4\.4\.2\.4	Neural Networks

4\.4\.2\.5	Decision Trees

4\.4\.2\.6	K\-Nearest Neighbors


---


Predicting using Regression Methods

![](images/slides_303.png)

Example: Predicting House Prices

Function:  Price = f\(SquareFootage\, Bedrooms\, Age\, SchoolRating\)

Source: http://www\.sclgsummit\.org/uploads/presentation/8934b2d0be055a2261f5d0320f5b59bb\.pdf


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.2	Regression

<span style="color:#c00000">4\.4\.2\.1	OLS</span>

4\.4\.2\.2	Ridge Regression

4\.4\.2\.3	Support Vector Regression

4\.4\.2\.4	Neural Networks

4\.4\.2\.5	Decision Trees

4\.4\.2\.6	K\-Nearest Neighbors


---


Traditional OLS Regression Approach

![](images/slides_304.png)

Function:

Price	= ß0 \+ ß1 \* SquareFootage\+ ß2 \* Bedrooms \+ ß3 \* Age \+ ß4 \* SchoolRating

Source: http://www\.sclgsummit\.org/uploads/presentation/8934b2d0be055a2261f5d0320f5b59bb\.pdf


---


Ordinary Least Squares Regression


---


Measuring the Quality of Fit \(I\)

Measuring the quality of fit means to measure how well the predictions of a model match the observed data\.

A commonly\-used measure is the Mean Absolute Error \(MAE\) which can be calculated for the training and the test set

A variant is the Mean Absolute Percentage Error \(MAPE\) which expresses the error in percent

While MAE and MAPE are easily interpretable\, using the absolute value of the error often is not as desirable as squaring this difference\. Depending on how you want your model to treat outliers\, or extreme values\, in your data\, you may want to bring more attention to these outliers or downplay them\.

Consequently\, the most used measure in regression is the Mean Squared Error \(MSE\) or its variant the Root Mean Squared Error  \(RMSE\)\, which is the square root of the MSE\.


---


Measuring the Quality of Fit \(II\)


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.2	Regression

4\.4\.2\.1	OLS

<span style="color:#ff0000">4\.4\.2\.2	Ridge Regression</span>

4\.4\.2\.3	Support Vector Regression

4\.4\.2\.4	Neural Networks

4\.4\.2\.5	Decision Trees

4\.4\.2\.6	K\-Nearest Neighbors


---


Ridge Regression

::: {.notes}

Example of 3 Parameters, free of range, with a range [-50,50], and a range [-5,5]. Count the numbers of combinations of the three alternatives.

:::


---


<span style="color:#000000">Complexity can be measured as the size of the set of possible outputs for a given set of inputs\.</span>

<span style="color:#000000">In this example the interval 0 to x</span>  <span style="color:#000000">\*</span>  <span style="color:#000000"> represents the set of possible inputs\. Function h</span>  <span style="color:#000000">0</span>  <span style="color:#000000"> has the lowest complexity because there is just one output independent of the inputs\. h</span>  <span style="color:#000000">2</span>  <span style="color:#000000"> has the highest complexity because here the set of possible outputs is the biggest one\.</span>


---


Complexity und Generalisation

<span style="color:#000000">Mean</span>  <span style="color:#000000"> </span>  <span style="color:#000000">Squared</span>  <span style="color:#000000"> Error</span>


---


Different Complexities


---


𝜆 → ∞ : Lowest Complexity

the ridge regression coefficients are equal to zero\. For every input\, the result is β0\.

𝜆 = 0 : Relative High Complexity \(linear Model\)

the penalty term has no effect\, and ridge regression will produce the least squares estimates\.

Example:

![](images/slides_305.png)

<span style="color:#000000">Source: </span>

<span style="color:#000000">James et al\. \(2013\): An Introduction to Statistical Learning with R Applications\, p\. 215f\.</span>


---


Handling High\-Dimensionality \(I\)

OLS is not suitable for high\-dimensional data\. Especially when the number of features p is as large as\, or larger than\, the number of observations\, OLS cannot be applied\. _ _ Regardless of whether or not there truly is a relationship between the features and the response\, OLS will yield a set of coefficient estimates that result in a perfect fit to the data\, such that the residuals are 	zero\.

The figure shows two cases\. When there are 20 observations\, n > p and the OLS line does not perfectly fit the data\. When there are only two observations\, then regardless of the values of those observations\, the regression line will fit the data exactly\. This is problematic because this perfect fit will almost certainly lead to overfitting of the data\.

![](images/slides_306.png)

<span style="color:#000000">Source: </span>

<span style="color:#000000">James et al\. \(2013\): An Introduction to Statistical Learning with R Applications\, p\. 239f\.</span>

::: {.notes}

Example: A marketing analyst interested in understanding people’s online shopping patterns could treat as features all of the search terms entered by users of a search engine. This is sometimes known as the "bag-ofwords" model. The same researcher might have access to the search histories of only a few hundred or a few thousand search engine users who have consented to share their information with the researcher. For a given user, each of the p search terms is scored present (0) or absent (1), creating a large binary feature vector. Then n ≈ 1,000 and p is much larger.


:::


---


Handling High\-Dimensionality \(II\)

The figure illustrates the risk of applying OLS when the number of features p is large\. The model R2 increases to 1 as the number of features increases\, and the training set MSE decreases to 0\. At the same time\, the MSE on a test set becomes extremely large as the number of features increases\.

In contrast\, methods like ridge regression are particularly useful for performing regression in the high\-dimensional setting\. Essentially\, these approaches avoid overfitting by using a less flexible fitting approach than least squares\.

![](images/slides_307.png)

<span style="color:#000000">Source: James et al\. \(2013\): An Introduction to Statistical Learning with R Applications\, p\. 240f\.</span>

::: {.notes}

Exercise on page 251!!!!!!!!!!!!!!

:::


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.2	Regression

4\.4\.2\.1	OLS

4\.4\.2\.2	Ridge Regression

<span style="color:#ff0000">4\.4\.2\.3	Support Vector Regression</span>

4\.4\.2\.4	Neural Networks

4\.4\.2\.5	Decision Trees

4\.4\.2\.6	K\-Nearest Neighbors


---


Support Vector Regression

<span style="color:#000000">The Goal is to find a robust model with a high generalization ability\.</span>

<span style="color:#000000">SVR regards two sources of Robustness:</span>

<span style="color:#000000">1\. Eliminating Noise</span>

<span style="color:#000000">2\. Handling Complexity</span>


---


Insensitive Loss Function \(I\)

<span style="color:#000000"> __\-insensitive Loss__ </span>

![](images/slides_308.png)

<span style="color:#000000">does not penalize acceptable deviations \(defined by    \)</span>


---


Insensitive Loss Function \(II\)

Using the e\-insensitive loss function\, only those data objects are considered in the estimation\, which have a distance greater than e from the regression function:

![](images/slides_309.png)

![](images/slides_310.png)

e\-insen\-sitiveRegion

Every object inside the e\-insensitive region is ignored\. It is regarded as noise\.


---


Support Vector Regression \(I\)

![](images/slides_311.png)

![](images/slides_312.png)

![](images/slides_313.png)

Decision criterion:

_Choose the line with the smallest sum of error slopes      with _  _paying attention to the flatness of the regression line\!_


---


Estimating the SVR \(Linear Case\)


---


Nonlinearity \(I\)

<span style="color:#000000"> __The linear __ </span>  <span style="color:#000000"> __case__ </span>  <span style="color:#000000"> __:__ </span>

<span style="color:#000000"> __The __ </span>  <span style="color:#000000"> __nonlinear__ </span>  <span style="color:#000000"> __ __ </span>  <span style="color:#000000"> __case__ </span>  <span style="color:#000000"> __:__ </span>


---


Nonlinearity \(II\)


---


Kernel Functions \(I\)

<span style="color:#000000">Kernel Functions are used to project n\-dimensional input to m\-dimensional input\, where m is higher than n:</span>

<span style="color:#000000">Any point x in the original space is mapped into the higher dimensional space\. For reason of efficiency\, the mapping is not performed in real but instead embedded in the model building process via the kernel function:</span>

<span style="color:#000000">    Instead of  </span>  <span style="color:#000000">ß</span>  <span style="color:#000000">0</span>  <span style="color:#000000"> </span>  <span style="color:#000000">\+ </span>  <span style="color:#000000">ß · x</span>  <span style="color:#000000">  = y </span>  <span style="color:#000000">the following is used </span>  <span style="color:#000000">ß</span>  <span style="color:#000000">0</span>  <span style="color:#000000"> </span>  <span style="color:#000000">\+ </span>  <span style="color:#000000">ß · </span>  <span style="color:#000000">F</span>  <span style="color:#000000">\(x\) </span>  <span style="color:#000000"> = y</span>

<span style="color:#000000">The main idea to use a kernel is: A linear regression curve in higher dimensions becomes a non\-linear regression curve in lower dimensions\.</span>


---


Estimating the SVR \(Nonlinear Case\)


---


Kernel Functions \(II\)

<span style="color:#000000">A frequently used kernel function is the Polynomial Kernel Function:</span>

<span style="color:#000000">where x and z </span>  <span style="color:#000000">are vector points in any fixed dimensional space and n is the order of  the kernel\.</span>

<span style="color:#000000">In the case of order equal to 2\, we get:</span>

Source: https://journals\.plos\.org/ploscompbiol/article?id=10\.1371/journal\.pcbi\.1000173


---


Kernel Functions \(III\)

<span style="color:#000000">A</span>  <span style="color:#000000"> __nother __ </span>  <span style="color:#000000"> __frequently__ </span>  <span style="color:#000000"> __ __ </span>  <span style="color:#000000"> __used__ </span>  <span style="color:#000000"> __ __ </span>  <span style="color:#000000"> __kernel__ </span>  <span style="color:#000000"> __ __ </span>  <span style="color:#000000"> __function__ </span>  <span style="color:#000000"> __ __ </span>  <span style="color:#000000"> __is__ </span>  <span style="color:#000000"> __ __ </span>  <span style="color:#000000"> __the__ </span>  <span style="color:#000000"> __ Radial Basis __ </span>  <span style="color:#000000"> __Function__ </span>  <span style="color:#000000"> __ \(__ </span>  <span style="color:#000000"> __RBF\):__ </span>

<span style="color:#000000">It maps the data according a Gaussian function where Sigma \(</span>  <span style="color:#000000">s</span>  <span style="color:#000000">\) is a streching factor\.</span>

<span style="color:#000000">Different Sigmas</span>

<span style="color:#000000">= Euclidean</span>  <span style="color:#000000"> distance between x and z</span>  <span style="color:#000000"> </span>

Source: https://journals\.plos\.org/ploscompbiol/article?id=10\.1371/journal\.pcbi\.1000173


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.2	Regression

4\.4\.2\.1	OLS

4\.4\.2\.2	Ridge Regression

4\.4\.2\.3	Support Vector Regression

<span style="color:#ff0000">4\.4\.2\.4	Neural Networks</span>

4\.4\.2\.5	Decision Trees

4\.4\.2\.6	K\-Nearest Neighbors


---


Using Neural Network for Regression

Artificial neural networks are often used for classification because of the relationship to logistic regression\. Neural networks typically use a logistic activation function and output values from 0 to 1 like logistic regression\.

But  the continuous output of a net must not be interpreted as a probability\, so neural networks can be used too for regression\, to model complex and non\-linear relationships\.

The Singlelayer Perceptron corresponds to a linear regression while a Multilayer Perceptron is able to approximate nearly any function regard\-less of the complexity and nonlinearity\.

Because of the high complexity of the MLP\, the models are usually very sensitive and have a tendency to overfitting\.

There exist regularization methods\, which make the networks better at generalizing beyond the training data\.\(see http://neuralnetworksanddeeplearning\.com/chap3\.html\)


---


Neural Network \(Multilayer Perceptron\)

![](images/slides_319.png)

Source: http://www\.sclgsummit\.org/uploads/presentation/8934b2d0be055a2261f5d0320f5b59bb\.pdf


---


![](images/slides_320.png)

Source: http://www\.sclgsummit\.org/uploads/presentation/8934b2d0be055a2261f5d0320f5b59bb\.pdf


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.2	Regression

4\.4\.2\.1	OLS

4\.4\.2\.2	Ridge Regression

4\.4\.2\.3	Support Vector Regression

4\.4\.2\.4	Neural Networks

<span style="color:#ff0000">4\.4\.2\.5	Decision Trees</span>

4\.4\.2\.6	K\-Nearest Neighbors


---


Introductory Example

Decision Tree for Predicting Fuel Consumption of Cars\(in Miles\-per\-Gallon \)


---


Regression Trees

Some of the tree approaches can be used for regression too\. They can be used for nonlinear multiple regression\. The output must be numerical\.

The figure shows a regression tree for predicting the salary of a baseball player\, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year\.

The predicted salary is given by the mean value of the salaries in the corresponding leaf\, e\.g\. for the players in the data set with Years<4\.5\, the mean \(log\-scaled\) salary is 5\.11\, and so we make a prediction of e5\.11 thousands of dollars\, i\.e\. $165\,670\, for these players\.

Players with Years>=4\.5 are assigned to the right branch\, and then that group is further subdivided by Hits\. The predicted salaries for the resulting two groups are 1\,000\*e6\.00 =$403\,428 and 1\,000\*e6\.74 =$845\,346\.

![](images/slides_322.png)

<span style="color:#000000">Source: James et al\. \(2013\): An Introduction to Statistical Learning with R Applications\, p\. 304f\.</span>

::: {.notes}

Example to do in R from Cart2.pdf

:::


---


Constructing a Regression Tree \(I\)

![](images/slides_323.png)

<span style="color:#000000">Source: James et al\. \(2013\): An Introduction to Statistical Learning with R Applications\, p\. 305f\.</span>

::: {.notes}

Example to do in R from Cart2.pdf

:::


---


Constructing a Regression Tree \(II\)

::: {.notes}

Example to do in R from Cart2.pdf

:::


---


Random Forests for Regression

Due to the usage of means as predictors a regression tree usually simplifies the true relationship between the inputs and the output\. The advantage over traditional statistical methods is\, that it can give valuable insights about which variables are important and where\. But the prediction ability is poor compared to other regression approaches\.

A much better prediction quality can be achieved with the creation of an ensemble of trees\, use them for prediction and averaging their results\. This is done\, when applying the Random Forests approach to a regression task\.

Regression Forests are an ensemble of different regression trees and are used for nonlinear multiple regression\. The principle is the same as in classification\, except that the output is not the result of a voting but instead of an averaging process\.

The disadvantage of Random Forests is that the analysis\, which aggregates over the results of many bootstrap trees\, does not produce a single\, easily interpretable tree diagram\.


---


Comparing the Fitting Ability of one vs\. many Regression Trees

Single Regression Tree

Average of 100 Regression Trees

![](images/slides_324.png)

![](images/slides_325.png)


---


Limitations of Tree Methods in Regression

When applied to regression problems\, tree methods have the limitation that they cannot exceed the range of values of the target variable used in training\. The reason for this lies in their design principle\, how the leaves of the trees are created\.

Thus\, Random Forests may perform poorly when the target data is out of the range of the original training data\, e\.g\. in the case of data with persistent trends\. A solution may be a frequent re\-training in this case\.

An important strength of Random Forests is that they are able to perform still well in the case of missing data\. According to their construction principle\, not every tree is using the same features\.

If there is any missing value for a feature during the application there usually are enough trees remaining that do not use this feature to produce accurate predictions\.


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.2	Regression

4\.4\.2\.1	OLS

4\.4\.2\.2	Ridge Regression

4\.4\.2\.3	Support Vector Regression

4\.4\.2\.4	Neural Networks

4\.4\.2\.5	Decision Trees

<span style="color:#ff0000">4\.4\.2\.6	K\-Nearest Neighbors</span>


---


k\-Nearest Neighbors for Regression

k\-Nearest Neighbors cannot only be used for classification but also for regression\. The only difference in regression is that the prediction is not the result of a majority vote but of an averaging process\.

A simple implementation of KNN regression is to calculate the average of the numerical target of the k\-nearest neighbors\.  Another approach uses an inverse distance weighted average of the K\-nearest neighbors\. KNN regression uses the same distance functions as KNN classification\.

Example:


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.2	Regression

<span style="color:#c00000">4\.4\.3	Segmentation</span>

4\.4\.3\.1	K\-Means

4\.4\.2\.2	Hierarchical Cluster Analysis


---


Introductory Example

Assume you are a wholesale distributor and each row of your dataset corresponds to a customer showing the following attributes:

1\) FRESH: annual spending on fresh products \(Continuous\); 2\) MILK: annual spending on milk products \(Continuous\); 3\) GROCERY: annual spending on grocery products \(Continuous\); 4\) FROZEN: annual spending on frozen products \(Continuous\) 5\) DETERGENTS\_PAPER: annual spending on detergents and paper products \(Continuous\) 6\) DELICATESSEN: annual spending on delicatessen products \(Continuous\); 7\) CHANNEL: customers buying channel \(Nominal\) 8\) REGION: customers region \(Nominal\)

Your goal is to segment the users\. That means finding similar types of users and bunching them together\.

Why would you want to do this?

You might want to give different users different experiences\. Marketing often does this; for example\, to offer toner to people who are known to own printers\.

You might have a model that works better for specific groups\. Or you might have different models for different groups\.


---


Cluster Analysis

<span style="color:#000000"> __Cluster analysis is a type of multivariate statistical analysis\. It is used to group data into separate clusters\. The main objective of clustering is to find similarities between data objects\, and then group similar objects together to assist in understanding relationships that might exist among them\. Cluster analysis is based on a mathematical formulation of a measure of similarity\. __ </span>

<span style="color:#000000"> __There are different types of cluster analysis methods:__ </span>

<span style="color:#0000ff"> __Clustering Methods__ </span>


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.2	Regression

4\.4\.3	Segmentation

<span style="color:#c00000">4\.4\.3\.1	K\-Means</span>

4\.4\.2\.2	Hierarchical Cluster Analysis


---


Partitioning Cluster Methods

<span style="color:#000000"> __The partitioning cluster methods divide the data into a predetermined number of __ </span>  <span style="color:#000000">clusters\. The most popular technique is the K\-Means algorithm\.</span>

Given a set of observations \( _x_ 1\, _x_ 2\,…\, _x_  _n_ \)\, where each observation is a  _m_ \-dimensional real vector\,  _k_ \-means clustering aims to partition the n observations into \( _k_ ≤ _n_ \) segments  _S_ =\{ _S_ 1\, _S_ 2\,\.\.\.\, _S_  _k_ \} so as to minimize the within\-cluster sum of squares \(WCSS\)\.

The objective is to find

where  _   _  is the mean of points in  _S_  _i_ \.


---


<span style="color:#000000">Procedure of K\-Means:</span>

<span style="color:#000000"> __Step 1:	Randomly partition the data objects into k clusters\.__ </span>

<span style="color:#000000"> __Step 2:	Calculate the cluster centroids\.__ </span>

<span style="color:#000000"> __Step 3:	Calculate the distance from every data point to all centroids__ </span>

<span style="color:#000000"> __Step 4:	If a data point is closest to its own centroid\, leave it where it  __ </span>  <span style="color:#000000"> __	__ </span>  <span style="color:#000000">is\. If the data </span>  <span style="color:#000000"> __point is not closest to its own centroid\, assign   __ </span>  <span style="color:#000000"> __	__ </span>  <span style="color:#000000">it to the cluster with </span>  <span style="color:#000000"> __the closest centroid\.__ </span>

<span style="color:#000000"> __Step 5:	Repeat the step 2 to 4 until a complete pass through of all  __ </span>  <span style="color:#000000"> __	__ </span>  <span style="color:#000000">the data points </span>  <span style="color:#000000"> __results in no data point changing from one __ </span>  <span style="color:#000000"> __	cluster to another\.__ </span>


---


Example of a K\-Means Cluster Analysis

![](images/slides_327.png)


---


__Between cluster variance: __

__Within cluster variance: __

Finding the Optimal Number of Clusters \(I\)

<span style="color:#000000"> __The aim of the cluster analysis is the __ </span>  <span style="color:#000000"> __segmentation of objects into clusters\, __ </span>  <span style="color:#000000"> __which are preferably homogeneous in __ </span>  <span style="color:#000000"> __it selves and heterogeneous to each other\.  __ </span>  <span style="color:#000000"> __The less variance exists within the clusters __ </span>  <span style="color:#000000"> __and the more variance exists between the __ </span>  <span style="color:#000000"> __clusters\, the better is the number of clusters\.__ </span>

<span style="color:#000000"> __Total variance:__ </span>

<span style="color:#000000"> __Accumulated variance within the k clusters:__ </span>

<span style="color:#000000"> __This results in the variance between the clusters: __ </span>

<span style="color:#000000"> __with	n = number of objects__ </span>

<span style="color:#000000"> __	m = number of attributes__ </span>

<span style="color:#000000"> __	__ </span>  <span style="color:#000000"> __n__ </span>  <span style="color:#000000"> __k__ </span>  <span style="color:#000000"> __ = number of objects in cluster k__ </span>

<span style="color:#000000"> __	__ </span>  <span style="color:#000000"> __c__ </span>  <span style="color:#000000"> __k__ </span>  <span style="color:#000000"> __ = cluster k__ </span>


---


Finding the  Optimal Number of Clusters  \(II\)

<span style="color:#000000"> __If you put V__ </span>  <span style="color:#000000"> __in__ </span>  <span style="color:#000000"> __ on the ordinate and the number of cluster k on the abscissa\, it often results in a curve with one or several kinks\. At the point where exists the \(first\) significant kink\, you can find the optimal number of clusters:__ </span>

<span style="color:#000000"> __Total variance __ </span>  <span style="color:#000000"> __V__ </span>  <span style="color:#000000"> __tot__ </span>

<span style="color:#000000"> __Between__ </span>  <span style="color:#000000"> __ __ </span>  <span style="color:#000000"> __cluster variance __ </span>  <span style="color:#000000"> __V__ </span>  <span style="color:#000000"> __betw__ </span>

<span style="color:#000000">Within cluster variance </span>  <span style="color:#000000"> __V__ </span>  <span style="color:#000000"> __in__ </span>

<span style="color:#000000"> __Number of clusters__ </span>


---


Finding the  Optimal Number of Clusters  \(III\)

Instead of visually identifying the optimal cluster number\, we can calculate the distances from the points on the elbow curve to a straight line linking the first and the last point on the curve\. The cluster number with the largest distance is then chosen as the one with the strongest kink\.

<span style="color:#000000"> __Number of clusters__ </span>


---


4	Predictive Analytics

4\.1	Subject of Predictive Analytics

4\.2	The Analytics Process

4\.3	Data Preparation

4\.4	Methods\, Algorithms and Applications

<span style="color:#c00000">	</span> 4\.4\.1	Classification

4\.4\.2	Regression

4\.4\.3	Segmentation

4\.4\.3\.1	K\-Means

<span style="color:#c00000">4\.4\.2\.2	Hierarchical Cluster Analysis</span>


---


Hierarchical Cluster Methods



* <span style="color:#000000"> __There are two types of hierarchical cluster methods: __ </span>
  * <span style="color:#000000"> __Agglomerative hierarchical clustering is a bottom\-up clustering method\. It starts with every single data object in a single cluster\. Then\, in each iteration\, it agglomerates \(merges\) the closest pair of clusters by satisfying some similarity criteria\, until all of the data is in one cluster\. __ </span>
  * <span style="color:#000000"> __Divisive hierarchical clustering is a top\-down clustering method\. It works in a similar way to agglomerative clustering but in the opposite direction\. This method starts with a single cluster containing all data objects\, and then successively splits resulting clusters until only clusters of individual data objects remain\.__ </span>

---


Process of the Hierarchical Cluster Analysis


---


Measuring Similarity between Clusters \(I\)

![](images/slides_328.png)

<span style="color:#000000">Distance between two clusters is the distance between the closest points:</span>

<span style="color:#000000">Complete Linkage:</span>

![](images/slides_329.png)

<span style="color:#000000">Distance between two clusters is the distance between the farthest pair of points:</span>

![](images/slides_330.png)

<span style="color:#000000">Distance between two clusters </span>  <span style="color:#000000">i</span>  <span style="color:#000000"> and j is the distance between their </span>  <span style="color:#000000">cendroids</span>  <span style="color:#000000">:</span>


---


Measuring Similarity between Clusters \(II\)

<span style="color:#000000">Average Linkage:</span>

<span style="color:#000000">Distance between clusters is the average distance between the cluster points:</span>

![](images/slides_331.png)

<span style="color:#000000">Ward’s Method / Minimum Variance Method \(only Agglomerative\):</span>

![](images/slides_332.png)

<span style="color:#000000">Ward's minimum variance criterion minimizes the total within\-cluster variance\. At each step the pair of clusters is merged that leads to minimum increase in total within\-cluster variance after merging\. This can be calculated as the square of the distance between cluster means divided by the sum of the reciprocals of the number of observations in each cluster: </span>

<span style="color:#000000">For a comparison of the methods see:	Ferreira\, L\.; Hitchcock\, D\. B\. \(2009\): A Comparison of Hierarchical Methods for Clustering Functional Data\, http://people\.stat\.sc\.edu/Hitchcock/compare\_hier\_fda\.pdf</span>


---


Single Linkage Example \(I\)

![](images/slides_333.png)

<span style="color:#000000">Source: Fred\, Ana: Unsupervised Learning\, </span>  <span style="color:#000000">Universidade</span>  <span style="color:#000000"> </span>  <span style="color:#000000">Técnica</span>  <span style="color:#000000"> de </span>  <span style="color:#000000">Lisboa</span>


---


Single Linkage Example \(II\)

![](images/slides_334.png)

<span style="color:#000000">Source: Fred\, Ana: Unsupervised Learning\, </span>  <span style="color:#000000">Universidade</span>  <span style="color:#000000"> </span>  <span style="color:#000000">Técnica</span>  <span style="color:#000000"> de </span>  <span style="color:#000000">Lisboa</span>


---


A dendrogram is a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering\. The y\-axis represents the value of this distance metric \(e\.g\. euclidean distance\) between the clusters\.

In a dendrogram the widths of the horizontal lines give an impression about the dissimilarity of the merging object\. Thus\, a good cluster number might be at a point from where the width of the following horizontal lines is significantly smaller in length\. The red line in the graph below shows such a point:

Counting the points that cut this line might be a good answer for the number of clusters the data can have\. It is the number 6 in this case\.

![](images/slides_335.png)

