<!DOCTYPE html>
<html lang="en"><head>
<script src="../site_libs/clipboard/clipboard.min.js"></script>
<script src="../site_libs/quarto-html/tabby.min.js"></script>
<script src="../site_libs/quarto-html/popper.min.js"></script>
<script src="../site_libs/quarto-html/tippy.umd.min.js"></script>
<link href="../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../site_libs/quarto-html/light-border.css" rel="stylesheet">
<link href="../site_libs/quarto-html/quarto-syntax-highlighting-587c61ba64f3a5504c4d52d930310e48.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.8.26">

  <title>Big Data and Analytics – lecture_4_predictive_analytics</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="../site_libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
      vertical-align: middle;
    }
  </style>
  <link rel="stylesheet" href="../site_libs/revealjs/dist/theme/quarto-0fde88a82f0356838740f155f1088782.css">
  <link rel="stylesheet" href="../assets/frankfurt.css">
  <link href="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="../site_libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
<header id="fs-header">
  <h2 id="slide-title-display"></h2>
  <img src="../assets/fs_logo_blue.svg" alt="Frankfurt School Logo">
</header>
  <div class="reveal">
    <div class="slides">


<section id="navigation" class="slide level2">
<h2>Navigation</h2>
<div style="font-size: 2em; display:flex; flex-direction:column; font-weight:700; line-height:1.4;">
<p style="color:#ff0000;margin:0;">
4&nbsp; Predictive Analytics
</p>
<p style="color:#ff0000; margin:0; margin-left:80px;">
4.1&nbsp; Subject of Predictive Analytics
</p>
<p style="margin-left:80px;">
4.2&nbsp; The Analytics Process
</p>
<p style="margin-left:80px;">
4.3&nbsp; Data Preparation
</p>
<p style="margin-left:80px;">
4.4&nbsp; Methods, Algorithms and Applications
</p>
<p style="margin-left:120px;">
4.4.1&nbsp; Classification
</p>
<p style="margin-left:120px;">
4.4.2&nbsp; Regression
</p>
</div>
</section>
<section id="first-application-of-data-analytics" class="slide level2">
<h2>First Application of Data Analytics</h2>
<h2 style="text-align:center">
Dr. John Snow’s Map of the 1854 London Cholera Outbreak
</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_104.png"></p>
</div>
<p style="text-align:center">
Source: http://www.udel.edu/johnmack/frec480/cholera
</p>
</section>
<section id="use-cases-of-data-analytics---examples" class="slide level2">
<h2>Use Cases of Data Analytics - Examples</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_98_new.png"></p>
</div>
</section>
<section id="fundamental-skills-of-data-analytics" class="slide level2">
<h2>Fundamental Skills of Data Analytics</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_105.png"></p>
</div>
</section>
<section id="the-data-analytics-process" class="slide level2">
<h2>The Data Analytics Process</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_106.png"></p>
</div>
<p><span style="color:#000000">Source</span> <span style="color:#000000">: http://www.datasciencecentral.com/profiles/blogs/data-science-simplified-principles-and-process</span></p>
</section>
<section id="need-for-knowledge-about-the-algorithms" class="slide level2">
<h2>Need for Knowledge about the Algorithms</h2>
<p style="font-weight:700; font-size:1.8em;">
The distance between using Excel and VBA for modeling in credit scoring, for example, and using machine learning algorithms and R or Python to enhance the results, is not that great, compared to the distance between someone running a packaged algorithm they don’t really understand and someone who understands the mathematical and statistical operations within an algorithm and can optimize or adapt it as needed – and do so in the context of their deep industry experience.
</p>
<p>Source: Dataiku - Data Science for Banking and Insurance</p>
</section>
<section id="statistics-vs.-machine-learning" class="slide level2">
<h2>Statistics vs. Machine Learning</h2>
<p><span style="font-size:1.7em"><strong>Statistics</strong> about finding valid conclusions about the underlying applied theory, and on the interpretation of parameters in their models. It insists on proper and rigorous methodology, and is comfortable with making and noting assumptions. It cares about how the data was collected and the resulting properties of the estimator or experiment (e.g. p-value). The focus is on hypothesis testing.</span></p>
<p><span style="font-size:1.7em"><strong>Machine Learning (ML)</strong> aims to derive practice-relevant findings from existing data and to apply the trained models to data not previously seen (prediction). It tries to predict or classify with the most accuracy. It cares deeply about scalability and uses the predictions to make decisions. Much of ML is motivated by problems that need to have answers. ML is happy to treat the algorithm as a black box as long as it works. </span></p>
</section>
<section id="statistical-regression-vs.-machine-learning-algorithms" class="slide level2">
<h2>Statistical Regression vs. Machine Learning Algorithms</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_107.png"></p>
</div>
</section>
<section id="explanation-vs.-prediction-i" class="slide level2">
<h2>Explanation vs. Prediction (I)</h2>
<p><span style="color:#000000; font-size:2em">Question 1: I have a headache. If I take an aspirin now, will it go away?</span></p>
<p><span style="color:#000000; font-size:2em; margin-top:0.5em">Question 2: I had a headache, but it passed. Was it because I took an aspirin two hours ago? Had I not taken such an aspirin, would I still have a headache?</span></p>
<div style="color:#000000; font-size:2em;margin-top:2em">
The first case is a typical “predictive” question. You are calculating the effect of a hypothetical intervention.
</div>
<div style="color:#000000; font-size:2em; margin-top:0.5em">
The second case is a typical “explanatory” question. You are calculating the effect of a counterfactual intervention.
</div>
<aside class="notes">
<p>Which one is Explanation, which Prediction?</p>
<p>counterfactual : den Tatsachen widersprechend</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="explanation-vs.-prediction-ii" class="slide level2">
<h2>Explanation vs. Prediction (II)</h2>
<div style="font-size: 1.4em;font-weight:600; line-height:1.2; color:#000000;">
<h3 style=" margin-bottom:10px;">
Explanation :
</h3>
<ul class="disc-list" style="margin-left:70px;">
<li>
Explanation is about understanding relationships and why certain things happen.
</li>
<li>
It requires an understanding of cause and effect.
</li>
<li>
Tests of causal hypotheses are fundamental.
</li>
<li>
Measures of significance are central.
</li>
<li>
A good explanatory model may also have predictive power.
</li>
</ul>
<h3 style=" margin-top:30px; margin-bottom:10px;">
Prediction :
</h3>
<ul class="disc-list" style="margin-left:70px;">
<li>
Prediction is about anticipating and forecasting what may happen in the future.
</li>
<li>
Correlations are important in this context (but correlation does not imply causation).
</li>
<li>
Therefore, predictive models may have no real explanatory power.
</li>
<li>
For robust prediction, knowledge of causality is preferable.
</li>
<li>
The main task is to find a model that optimally approximates reality and minimizes overfitting.
</li>
<li>
Accuracy is measured using out-of-sample data.
</li>
</ul>
</div>
</section>
<section id="correlation" class="slide level2">
<h2>Correlation</h2>
<div style="text-align:center;">
<p><img data-src="../images/slides_112.png"></p>
</div>
<p style="font-size:1.3em;text-align:center">
Source: Organic Trade Association, 2011 Organic lndustry Survey, U.p. Department of Education, Ofﬁce of Special Education Programs, Data Analysis System (DANS)
</p>
<p style="color:#ff0000;font-size:1.6em;text-align:center">
Organic food sales and the rate of autism seem to have a very strong correlation‚ but no one is suggesting that one causes the other!
</p>
</section>
<section id="correlation-vs.-causality-i" class="slide level2">
<h2>Correlation vs. Causality (I)</h2>
<div style="text-align:center;">
<p><img data-src="../images/slides_107_new.png"></p>
</div>
<div style="font-size:1.7em;font-weight:600">
<p>
Correlation: Two data series behave “similar”
</p>
<p>
Causality: Principle of Cause and Effect
</p>
</div>
</section>
<section id="correlation-vs.-causality-ii" class="slide level2">
<h2>Correlation vs. Causality (II)</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_114.png"></p>
</div>
</section>
<section id="correlation-vs.-causality-iii" class="slide level2">
<h2>Correlation vs. Causality (III)</h2>
<div style="font-weight:600;font-size:4em;text-align:center">
<p>
But:
</p>
<p>
Sometimes it is better to know/predict something even if we can not explain it instead of doing nothing!
</p>
</div>
</section>
<section id="statistical-estimation" class="slide level2">
<h2>Statistical Estimation</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_116.png"></p>
</div>
<p>Source: http://www.dxbydt.com/the-size-of-your-sample</p>
</section>
<section id="navigation-1" class="slide level2">
<h2>Navigation</h2>
<div style="font-size: 2em; display:flex; flex-direction:column; font-weight:700; line-height:1.4;">
<p style="color:#ff0000; margin:0;">
4&nbsp; Predictive Analytics
</p>
<p style="margin:0; margin-left:80px;">
4.1&nbsp; Subject of Predictive Analytics
</p>
<p style="margin-left:80px;color:#ff0000;">
4.2&nbsp; The Analytics Process
</p>
<p style="margin-left:80px;">
4.3&nbsp; Data Preparation
</p>
<p style="margin-left:80px;">
4.4&nbsp; Methods, Algorithms and Applications
</p>
<p style="margin-left:120px;">
4.4.1&nbsp; Classification
</p>
<p style="margin-left:120px;">
4.4.2&nbsp; Regression
</p>
</div>
</section>
<section id="definitions" class="slide level2">
<h2>Definitions</h2>
<div style="font-size:1.7em;font-weight:600">
<p>A <span style="color:#ff0000">method</span> is a composition of formalized principles that form the basis for a stringent calculation process.</p>
<p>An <span style="color:#ff0000">algorithm</span> is a procedure or set of steps or rules to accomplish a task. It is usually the implementation of a method. Algorithms are used to build models.</p>
<p>In the given context, a <span style="color:#ff0000">model</span> is the description of the relationship between variables. It is used to create output data from given input data, for example to make predictions.</p>
<p><span style="color:#ff0000">Fitting</span> a model means that you estimate the model using the observed data. You are using your data as evidence to help approximate the real-world mathematical process that generated the data. Fitting the model often involves optimization methods and algorithms, such as maximum likelihood estimation, to help get the parameters.</p>
<p><span style="color:#ff0000">Overfitting</span> is the term used to mean that you used a dataset to estimate the parameters of your model, but your model isn’t that good at capturing reality beyond your sampled data.</p>
</div>
<p><span style="color:#000000">Source: </span> <span style="color:#000000">Schutt</span> <span style="color:#000000">/O’Neil (2013): Doing Data Science.</span></p>
</section>
<section id="traditional-analytics-process" class="slide level2">
<h2>Traditional Analytics Process</h2>
<p><img data-src="../images/slides_117.png"></p>
</section>
<section id="example-regression---fitting-the-model" class="slide level2">
<h2>Example Regression - Fitting the model</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_121.png"></p>
</div>
</section>
<section id="example-regression---testing-the-model" class="slide level2">
<h2>Example Regression - Testing the model</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_123.png"></p>
</div>
</section>
<section id="data-errors-and-their-consequences" class="slide level2">
<h2>Data Errors and their Consequences</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_125_new.png"></p>
</div>
</section>
<section id="modern-analytics-process" class="slide level2">
<h2>Modern Analytics Process</h2>
<p><img data-src="../images/slides_126_new.png"></p>
</section>
<section id="best-fit-vs.-best-generalization" class="slide level2">
<h2>Best Fit vs. Best Generalization</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_129.png"></p>
</div>
</section>
<section id="over--and-underfitting" class="slide level2">
<h2>Over- and Underfitting</h2>
<div style="text-align:center;">
<p><img data-src="../images/slides_131.png"></p>
</div>
<p style="color:#ff0000;font-size:1.6em;border:1px solid #ff0000;padding:10px !important">
Due to the problem of overfitting, the main goal is to maximize the prediction quality and not to fit the data that is used for the model estimation as well as possible. This is equivalent to minimizing the risk that the model will have weak predictive ability.
</p>
<aside class="notes">
<p>Fibonacci Sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21, 34, 55 Measurement Error: 0, 1, 1, 2, 3, 6, 8, 13, 20, 34, 55</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="the-bias-variance-tradeoff" class="slide level2">
<h2>The Bias-Variance Tradeoff</h2>
<div style="display:flex;">
<div style="font-size:1.5em;font-weight:600;width:60%">
<p>The prediction error is influenced by three components:</p>
<p style="text-align:center">
Error = Bias + Variance + Noise
</p>
<p>Bias is the inability of the used method to learn the relevant relations between the inputs and the outputs. It reflects the method quality, e.g. if a method only produces linear models.</p>
<p>Variance is represents the deviation resulting from the sensitivity of the created model to small fluctuations in the data.</p>
<p>Typically, there is a tradeoff between bias and variance.</p>
<p>Noise is everything that arises from random variations in the data. It cannot be controlled.</p>
</div>
<div class="descriptive-wrapper" style="width:40%">
<p><img data-src="../images/slides_132_new.png"></p>
</div>
</div>
</section>
<section id="summarizing-statistics-vs.-data-analytics" class="slide level2">
<h2>Summarizing: Statistics vs. Data Analytics</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_133_new.png"></p>
</div>
</section>
<section id="which-method-should-i-choose" class="slide level2">
<h2>Which Method should I choose?</h2>
<div style="font-size:1.7em;font-weight:700">
<p>The choice of the method of data analysis depends on the one hand on the scope of application, but on the other hand on the interrelationships of the data to be analyzed.</p>
<p>In the Big Data area, data spaces are often highly-dimensional, making it difficult to visualize the interrelationships.</p>
<p>For this reason, the choice of the method can often not be made ex ante. In these cases, different methods are competitively tried to select the most suitable one.</p>
</div>
</section>
<section id="linear-world" class="slide level2">
<h2>Linear World</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_134_new.png"></p>
</div>
</section>
<section id="quadratic-world" class="slide level2">
<h2>Quadratic World</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_135_new.png"></p>
</div>
</section>
<section id="nonlinear-world-type-1" class="slide level2">
<h2>Nonlinear World (Type 1)</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_136_new.png"></p>
</div>
</section>
<section id="nonlinear-world-type-2" class="slide level2">
<h2>Nonlinear World (Type 2)</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_137_new.png"></p>
</div>
</section>
<section id="nonlinear-world-type-3" class="slide level2">
<h2>Nonlinear World (Type 3)</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_138_new.png"></p>
</div>
</section>
<section id="nonlinear-world-type-4" class="slide level2">
<h2>Nonlinear World (Type 4)</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_139_new.png"></p>
</div>
</section>
<section id="the-data-analytics-process---technical-view" class="slide level2">
<h2>The Data Analytics Process - Technical View</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_133.png"></p>
</div>
<p><span style="color:#000000">Source: http://blogs.msdn.microsoft.com/martinkearn/2016/03/01/machine-learning-is-for-muggles-too/</span></p>
</section>
<section id="navigation-2" class="slide level2">
<h2>Navigation</h2>
<div style="font-size: 2em; display:flex; flex-direction:column; font-weight:700; line-height:1.4;">
<p style="color:#ff0000;margin:0;">
4&nbsp; Predictive Analytics
</p>
<p style="margin:0; margin-left:80px;">
4.1&nbsp; Subject of Predictive Analytics
</p>
<p style="margin-left:80px;">
4.2&nbsp; The Analytics Process
</p>
<p style="margin-left:80px;color:#ff0000;">
4.3&nbsp; Data Preparation
</p>
<p style="margin-left:80px;">
4.4&nbsp; Methods, Algorithms and Applications
</p>
<p style="margin-left:120px;">
4.4.1&nbsp; Classification
</p>
<p style="margin-left:120px;">
4.4.2&nbsp; Regression
</p>
</div>
</section>
<section id="data-preparation-and-enrichment" class="slide level2">
<h2>Data Preparation and Enrichment</h2>
<p style="font-weight:600;font-size:1.5em">
The data collection and preparation phase is the most labor-intensive one, consuming on average between 60-80% of a data scientist’s time. It’s critical therefore to select a tool that can automate or at least speed the workflows associated with data preparation.
</p>
<div style="text-align:center">
<p><img data-src="../images/slides_134.png"></p>
</div>
<p style="text-align:right">
2016 Dataiku, Inc.
</p>
</section>
<section id="data-cleaning" class="slide level2">
<h2>Data Cleaning</h2>
<div>
<div>
<h3 style="font-size:1.4em;font-weight-700;">
1.&nbsp;Proof of correctness of the data
</h3>
<ul class="disc-list" style="font-size:1.4em">
<li>
examine for irregular outliers (e.g.&nbsp;Age=236)
</li>
<li>
examine for typographical errors (e.g.&nbsp;Frankfrut)
</li>
<li>
examine for different writing styles (e.g.&nbsp;behavior/behaviour)
</li>
<li>
—
</li>
</ul>
</div>
<div>
<h3 style="font-size:1.4em;font-weight-700;">
2.&nbsp;Handling missing values
</h3>
<p><img data-src="../images/slides_140_new.png"></p>
</div>
</div>
</section>
<section id="missing-values-strategies" class="slide level2">
<h2>Missing Values Strategies</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_141_new.png"></p>
</div>
</section>
<section id="sampling" class="slide level2">
<h2>Sampling</h2>
<div style="font-weight:700;font-size:1.6em">
<p>A population can be defined as including all people or items with the characteristic one wishes to understand.</p>
<p>Sampling is about to find a representative subset of that population.</p>
<p>Data represents the traces of the real-world processes, and exactly which traces we gather are decided by our sampling method.</p>
<p>There are two sources of randomness and uncertainty:</p>
<ol>
<li>
the randomness and uncertainty underlying the process itself, and
</li>
<li>
the uncertainty associated with the underlying sampling method.
</li>
</ol>
</div>
</section>
<section id="sampling-in-times-of-big-data" class="slide level2">
<h2>Sampling in Times of Big Data</h2>
<div style="font-size:1.7em; font-weight:700">
<p>Question:</p>
<p>Is there any need for sampling in times of Big Data? Why not “N=ALL”?</p>
<p>Answer:</p>
<p>Data is not objective! Data does not speak for itself. Data is just a quantitative echo of the events of our society.</p>
<p>Examples:</p>
<ul class="disc-list">
<li>
When analyzing the probability of customers terminating the relationship, a very small proportion of terminating customers (e.g. 0.2%) on the whole may result in a bias.
</li>
<li>
When analyzing political attitudes via social media data, there might be a bias if people with specific attitudes are posting more frequently.
</li>
</ul>
</div>
</section>
<section id="reasons-for-sampling" class="slide level2">
<h2>Reasons for Sampling</h2>
<ul class="disc-list" style="font-size:1.7em;font-weight:600">
<li>
The volume of data is too large to capture and process
</li>
<li>
Design the analytics process using a subset of the data for performance reasons. Later use the complete data set.
</li>
<li>
The data set doesn’t perfectly represent the target population.
</li>
<li>
The data set is imbalanced.
</li>
<li>
Use sampling to partition into training and test data.
</li>
<li>
...
</li>
</ul>
</section>
<section id="popular-methods-of-sampling-i" class="slide level2">
<h2>Popular Methods of Sampling (I)</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_135.png"></p>
</div>
</section>
<section id="popular-methods-of-sampling-ii" class="slide level2">
<h2>Popular Methods of Sampling (II)</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_167.png"></p>
</div>
</section>
<section id="systematic-sampling" class="slide level2">
<h2>Systematic Sampling</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_176.png"></p>
</div>
</section>
<section id="random-sampling" class="slide level2">
<h2>Random Sampling</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_177.png"></p>
</div>
</section>
<section id="proportional-sampling" class="slide level2">
<h2>Proportional Sampling</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_178.png"></p>
</div>
</section>
<section id="downsampling" class="slide level2">
<h2>Downsampling</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_179.png"></p>
</div>
</section>
<section id="smote" class="slide level2">
<h2>SMOTE</h2>
<div style="display:flex">
<div style="width:60%">
<p style="color:#000000; font-size:1.6em;font-weight:700">
SMOTE (Synthetic Minority Oversampling Technique) is an oversampling technique where the synthetic samples are generated for the minority class.
</p>
<p style="color:#000000; font-size:1.6em;font-weight:700">
At first the total number of oversampling observations N is set up. Usually, it is selected such that the resulting class distribution is 1:1. Now, the iteration starts by first selecting a minority class instance at random. Next, the k nearest neighbors for that instance are obtained. For every neighbor calculate the difference as distance and multiply this difference by a random value between 0 and 1.Adding the result to the chosen instance creates a new synthetic instance. This is done until the number of needed instances is reached.
</p>
</div>
<div style="float:right;width:40%">
<p><img data-src="../images/slides_180.png"></p>
</div>
</div>
<p style="float:right">
Source: https://github.com/minoue-xx/Oversampling-Imbalanced-Data
</p>
</section>
<section id="feature-engineering" class="slide level2">
<h2>Feature Engineering</h2>
<div style="font-size:1.4em;font-weight:700;margin-bottom:3em">
<p><span style="color:#000000">Feature engineering is the process of using domain knowledge of the data to create features that make machine learning algorithms work. If feature engineering is done correctly, it increases the predictive power of machine learning algorithms by creating features from raw data that help facilitate the machine learning process. </span></p>
<span style="color:#000000">A feature (variable, attribute) is depicted by a column in a dataset. Considering a generic two-dimensional dataset, each observation is depicted by a row and each feature by a column, which will have a specific value for an observation:</span>
<div style="text-align:center">
<p><img data-src="../images/slides_144_new.png"></p>
</div>
<p><span style="color:#000000">Features can be of two major types. </span> <span style="color:#ff0000">Raw features</span> <span style="color:#000000"> are obtained directly from the dataset with no extra data manipulation or engineering. </span> <span style="color:#ff0000">Derived features</span> <span style="color:#000000"> are usually obtained from feature engineering, where we extract features from existing data attributes. A simple example would be creating a new feature “Age” from an employee dataset containing “Birthdate”.</span></p>
</div>
<p>Sources: Sarkar, D.: Understanding Feature Engineering, towardsdatascience.com and Shekhar, A.: What Is Feature Engineering for Machine Learning?, medium.com.</p>
</section>
<section id="variants-of-feature-engineering" class="slide level2">
<h2>Variants of Feature Engineering</h2>
<div style="font-size:1.2em">
<h3>
1.&nbsp;Transformation
</h3>
<ul class="disc-list">
<li>
convert features (e.g., birth date → age)
</li>
<li>
build lag structures (e.g., time-lags)
</li>
<li>
normalization / standardization / scaling
</li>
</ul>
<h3>
2.&nbsp;Type Conversion
</h3>
<ul class="disc-list">
<li>
if numerical type is needed, transform categorical into numerical data using dummy features
</li>
<li>
if categorical type is needed or more informative, discretize numerical features (e.g., income → poor / rich classes)
</li>
</ul>
<h3>
3.&nbsp;Feature Combination
</h3>
<ul class="disc-list">
<li>
create interaction features (e.g., school_score = num_schools × median_school<br>
with num_schools = number of schools within 5 miles of a property and<br>
median_school = median quality score of those schools)
</li>
<li>
combine categories (e.g., when there are very few observations or too many dummy features)
</li>
</ul>
<h3>
4.&nbsp;Feature Composition
</h3>
<ul class="disc-list">
<li>
build ratios (e.g., returns from prices)
</li>
<li>
Principal Component Analysis (Dimensionality Reduction)
</li>
</ul>
</div>
</section>
<section id="scaling" class="slide level2">
<h2>Scaling</h2>
<div style="font-size:1.4em;font-weight:700">
<p style="color:#000000">
Most datasets contain features highly varying in magnitudes, units and range.
</p>
<p>
Most machine learning algorithms have problems with this because they use distance measures or calculate gradients. The features with high magnitudes will weigh in a lot more in the distance calculations than features with low magnitudes and gradients may end up taking a long time or are not accurately calculable.
</p>
<p style="color:#000000">
To overcome this effect, we scale the features to bring them to the same level of magnitudes. The two most discussed scaling methods are Normalization and Standardization.
</p>
<div>
<p><img data-src="../images/slides_146_new.png"></p>
</div>
</div>
</section>
<section id="type-conversion-encoding" class="slide level2">
<h2>Type Conversion (Encoding)</h2>
<div style="font-size:1.4em;font-weight:700">
<p><span style="color:#000000">Many machine learning algorithms cannot work with categorical data directly. To convert categorical data to numbers, there exist two variants:</span></p>
<p><span style="color:#ff0000">Label encoding</span> <span style="color:#000000"> refers to transforming the word labels into numerical form so that the algorithms can understand how to operate on them. Every categorical value is assigned to one numerical value, e.g. young -&gt; 1, </span> <span style="color:#000000">middle_age</span> <span style="color:#000000"> -&gt; 2, old -&gt; 3. This only works in specific situations where you have somewhat continuous-like data, e.g. if the categorical feature is ordinal.</span></p>
<span style="color:#ff0000">One hot encoding</span> <span style="color:#000000"> is a representation of a categorical variable as binary vectors. Every categorical value is assigned to an artificial binary variable. If the corresponding categorical value occurs in a data row the value of its binary replacement is equal to 1 else 0, e.g. </span>
<div style="text-align:center">
<p><img data-src="../images/slides_147_new.png"></p>
</div>
<p><span style="color:#000000">It is usual when creating dummy variables to have one less variable than the number of categories present to avoid perfect collinearity (dummy variable trap).</span></p>
</div>
</section>
<section id="example-of-feature-engineering-i" class="slide level2">
<h2>Example of Feature Engineering (I)</h2>
<div style="font-size:1.4em;font-weight:700">
<p style="color:#000000">
Data sets often contain date/time features. These features are rarely useful in their original form because they only contain ongoing values. However, they can be useful for extracting cyclical factors, such as weekly or seasonal effects. Suppose, we are given a data “flight date time vs status”. Then, given the date-time data, we have to predict the status of the flight.
</p>
<div style="text-align:center">
<p><img data-src="../images/slides_183.png"></p>
</div>
<p style="color:#000000">
But the status of the flight may depend on the hour of the day, not on the date-time. To analyze this, we will create the new feature ” Hour_Of_Day”. Using the “Hour_Of_Day” feature, the machine will learn better as this feature is directly related to the status of the flight.
</p>
</div>
<p style="float:right">
Source: Shekhar, A.: What Is Feature Engineering for Machine Learning?, medium.com.
</p>
</section>
<section id="example-of-feature-engineering-ii" class="slide level2">
<h2>Example of Feature Engineering (II)</h2>
<div style="font-size:1.4em;font-weight:700">
<p><span style="color:#000000">Suppose we are given the latitude, longitude and other data with the objective to predict the target feature “</span> <span style="color:#000000">Price_Of_House</span> <span style="color:#000000">“. Latitude and longitude are not of use in this context if they are alone. So, we will combine the latitude and the longitude to make one feature. </span></p>
<span style="color:#000000">In other cases, it might be appropriate to transform latitude and longitude into categories which reflect regions, for example</span>
<div style="text-align:center">
<p><img data-src="../images/slides_185.png"></p>
</div>
</div>
</section>
<section id="example-of-feature-engineering-iii" class="slide level2">
<h2>Example of Feature Engineering (III)</h2>
<div style="font-size:1.2em;font-weight:700;">
<p><span style="color:#000000">Suppose we are given a feature “</span> <span style="color:#000000">Marital_Status</span> <span style="color:#000000">” and other data with the objective to classify customers into “Creditworthy” and “</span> <span style="color:#000000">Not_Creditworthy</span> <span style="color:#000000">“. In the data set the martial status has many different values, for example </span></p>
<p><span style="color:#000000">● single living alone</span></p>
<p><span style="color:#000000">● single living with his parents</span></p>
<p><span style="color:#000000">● married living together</span></p>
<p><span style="color:#000000">● married living separately</span></p>
<p><span style="color:#000000">● divorced</span></p>
<p><span style="color:#000000">● divorced but living together</span></p>
<p><span style="color:#000000">● registered partnerships</span></p>
<p><span style="color:#000000">● living in marriage-like community</span></p>
<p><span style="color:#000000">● widowed</span></p>
<p><span style="color:#000000">● ...</span></p>
<p><span style="color:#000000">To avoid a transformation into too many and maybe dominating dummy features, we can group the similar classes, e.g. in single, married, widowed.</span></p>
<p><span style="color:#000000">If there exist some remaining sparse classes which cannot be assigned in a meaningful way they can be joined into a single “other” class.</span></p>
</div>
</section>
<section id="partitioning-the-data" class="slide level2">
<h2>Partitioning the Data</h2>
<div style="font-size:1.5em;font-weight:600">
<p>
The partitioning of the data in <span style="color:#ff0000">Training and Test Data</span> has the aim to proof if the analytical results can be generalized. The analysis (e.g.&nbsp;the development of a classifier) is carried out on the basis of training data. Subsequently, the results are applied to the test data. If the results are significantly worse than the training data, the model is not generalizable, which is called overfitting.
</p>
<div style="text-align:center">
<p><img data-src="../images/slides_186.png"></p>
</div>
<p>
The partitioning of the data in training and test data can be carried out in the following ways:
</p><ul class="disc-list">
<li>
By random/stratified/… sampling (problem with the repeatability)
</li>
<li>
according to a list
</li>
<li>
according to rules (e.g.&nbsp;the first/last 50 records or every twelfth)
</li>
</ul>
<p></p>
</div>
</section>
<section id="applying-training-and-test-data" class="slide level2">
<h2>Applying Training and Test Data</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_152_new.png"></p>
</div>
<p style="text-align:right">
Source: http://www.cs.kent.edu/~jin/BigData/Lecture10-ML-Classification.pptx
</p>
</section>
<section id="partitioning" class="slide level2">
<h2>Partitioning</h2>
<p><img data-src="../images/slides_187.png"></p>
</section>
<section id="exploratory-data-analysis" class="slide level2">
<h2>Exploratory Data Analysis</h2>
<div style="font-size:1.5em;font-weight:600">
<p>In Exploratory Data Analysis (EDA), there is no hypothesis and there is no model.</p>
<p>People are not very good at looking at a column of numbers or a whole data table and then determining important characteristics of the data. EDA techniques have been devised as an aid in this situation.</p>
<p>Reasons for EDA:</p>
<p>● gain intuition about the data</p>
<p>● make comparisons between distributions</p>
<p>● sanity checking (making sure the data is on the scale you expect, in the format you thought it should be)</p>
<p>● find out where data is missing or if there are outliers</p>
<p>● summarize the data</p>
<p>Exploratory data analysis is generally cross-classed in two ways. First, each method is either non-graphical or graphical. And second, each method is either univariate or multivariate.</p>
</div>
</section>
<section id="univariate-non-graphical-eda" class="slide level2">
<h2>Univariate Non-Graphical EDA</h2>
<div style="font-size:1.5em;font-weight:600">
<p>Non-graphical exploratory data analysis is the first step when beginning to analyze the data. This preliminary data analysis step focuses on four points:</p>
<p>● measures of central tendency, i.e. mean and median. The median, known as 50th percentile, is more resistant to outliers.</p>
<p>● measures of spread, i.e. variance, standard deviation, and interquartile range</p>
<p>● the shape of the distribution</p>
<p>● the existence of outliers</p>
<p>The characteristics of interest for a categorical variable are simply the range of values and the frequency of occurrence for each value.</p>
<div style="text-align:center">
<p><img data-src="../images/slides_188.png"></p>
</div>
</div>
</section>
<section id="tests-on-outliers" class="slide level2">
<h2>Tests on Outliers</h2>
<div style="font-size:1.5em;font-weight:600">
<p>Outlier are data objects, which are clearly different from the others.</p>
<p>Usually, the detection of outliers is an unsupervised process, because they are not known before analyses.</p>
In the case of <span style="color:#ff0000">numerical attributes</span> the Interquartil Range can be used. Here, an outlier is defined if the attribute lies outside the interval
<div style="text-align:center">
<p><img data-src="../images/slides_187_new.png"></p>
</div>
<p>Usually, k has a value between 1.5 and 3. The bigger k, the more different the values must be to be classified as outliers.</p>
Can be visualized by a Box-and-Whisker Plot:
<div style="text-align:right">
<p><img data-src="../images/slides_189.png"></p>
</div>
</div>
</section>
<section id="handling-outliers" class="slide level2">
<h2>Handling Outliers</h2>
<div style="font-size:1.4em;font-weight:600">
<ul>
<li>Outlier have to be <span style="color:#ff0000">eliminated</span> if they
<ul>
<li>1.&nbsp;would bias the analysis, e.g. if 9 persons have an age between 20 and 30 and the 10th person is 80 years old.</li>
<li>2.&nbsp;are erogenous data, e.g. as a result of input errors or a defect sensor.</li>
</ul></li>
<li>It is not always acceptable to drop an observation just because it is an outlier. They can be legitimate observations and are sometimes interesting ones. It’s important to investigate the nature of the outlier before deciding.</li>
<li>In those cases where you shouldn’t drop the outlier, one option is to try a transformation. <span style="color:#ff0000">Log transformations </span> pull in high numbers. This can reduce the impact of a single point if the outlier is an independent variable.</li>
</ul>
</div>
<div style="text-align:center">
<p><img data-src="../images/slides_190.png"></p>
</div>
</section>
<section id="univariate-graphical-eda" class="slide level2">
<h2>Univariate Graphical EDA</h2>
<div style="font-size:1.4em;font-weight:600">
<p>Non-graphical and graphical EDA methods complement each other, they have the same focus. While the non-graphical methods are quantitative and objective, they do not give a full picture of the data. The distribution of a variable tells us what values the variable takes and how often each value occurs.</p>
<p>Types of displays:</p>
<p>for numerical variables: Histograms, Boxplots, Quantile-normal plots, …</p>
<p>for categorical variables: Pie charts, Bar graphs, …</p>
</div>
<div style="display:flex;justify-content:center">
<div style="height:600px;width:400px">
<p><img data-src="../images/slides_193.png"></p>
</div>
<div style="height:600px;width:400px">
<p><img data-src="../images/slides_191.png"></p>
</div>
<div style="height:600px;width:400px">
<p><img data-src="../images/slides_192.png"></p>
</div>
</div>
</section>
<section id="multivariate-non-graphical-eda" class="slide level2">
<h2>Multivariate Non-Graphical EDA</h2>
<div style="font-size:1.4em;font-weight:600">
<p>Multivariate non-graphical EDA techniques generally show the relationship between two or more variables in the form of either cross-tabulation for categorical variables or correlation statistics for numerical variables.</p>
</div>
<div style="display:flex;justify-content:center;margin-top:4em">
<div class="Multivariate-wrapper">
<p><img data-src="../images/slides_194.png"></p>
</div>
<div class="Multivariate-wrapper">
<p><img data-src="../images/slides_195.png"></p>
</div>
</div>
</section>
<section id="multivariate-graphical-eda" class="slide level2">
<h2>Multivariate Graphical EDA</h2>
<div style="font-size:1.5em;font-weight:600">
<p>Multivariate graphical EDA techniques are scatterplots for numerical variables, Barcharts for categorical variables, or Boxplots for mixed types.</p>
<div style="text-align:center">
<p><img data-src="../images/slides_196.png"></p>
</div>
</div>
</section>
<section id="touring-diagram" class="slide level2">
<h2>Touring Diagram</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_199.gif"></p>
</div>
</section>
<section id="navigation-3" class="slide level2">
<h2>Navigation</h2>
<div style="font-size: 2em; display:flex; flex-direction:column; font-weight:700; line-height:1.4;">
<p style="color:#ff0000;margin:0;">
4&nbsp; Predictive Analytics
</p>
<p style="margin:0; margin-left:80px;">
4.1&nbsp; Subject of Predictive Analytics
</p>
<p style="margin-left:80px;">
4.2&nbsp; The Analytics Process
</p>
<p style="margin-left:80px;">
4.3&nbsp; Data Preparation
</p>
<p style="margin-left:80px;color:#ff0000;">
4.4&nbsp; Methods, Algorithms and Applications
</p>
<p style="margin-left:120px;">
4.4.1&nbsp; Classification
</p>
<p style="margin-left:120px;">
4.4.2&nbsp; Regression
</p>
</div>
</section>
<section id="categories-in-machine-learning" class="slide level2">
<h2>Categories in Machine Learning</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_200.png"></p>
</div>
</section>
<section id="supervised-learning" class="slide level2">
<h2>Supervised Learning</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_203.png"></p>
</div>
</section>
<section id="unsupervised-learning" class="slide level2">
<h2>Unsupervised Learning</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_207.png"></p>
</div>
</section>
<section id="supervised-and-unsupervised-learning" class="slide level2">
<h2>Supervised and Unsupervised Learning</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_209.png"></p>
</div>
</section>
<section id="use-cases-quiz" class="slide level2">
<h2>Use Cases Quiz</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_210.png"></p>
</div>
</section>
<section id="reinforcement-learning" class="slide level2">
<h2>Reinforcement Learning</h2>
<div style="font-size:1.5em;font-weight:600">
<p>The solution to many of the problems in our lives cannot be automated. This is not because current computers are too slow, but simply because it is too difficult for humans to determine what the program should do.</p>
<p>Supervised learning is a general method for training an approximator. However, supervised learning requires sample input-output pairs from the domain to be learned.</p>
<p>For example, we might not know the best way to program a computer to recognize an infrared picture of a tank, but we do have a large collection of infrared pictures, and we do know whether each picture contains a tank or not. Supervised learning could look at all the examples with answers, and learn how to recognize tanks in general.</p>
<p>Unfortunately, there are many situations where we don’t know the correct answers that supervised learning requires. For example, in a self-driving car, the question would be the set of all sensor readings at a given time, and the answer would be how the controls should react during the next millisecond.</p>
<p>For these cases there exist a different approach known as reinforcement learning.</p>
</div>
</section>
<section id="reinforcement-learning-1" class="slide level2">
<h2>Reinforcement Learning</h2>
<div class="volumeImageWrapper" style="text-align:center">
<p><img data-src="../images/slides_211.jpg"></p>
</div>
<div style="font-size:1.5em;font-weight:600">
<p>The agent learns how to achieve a given goal by trial-and-error interactions with its environment by maximizing a reward.</p>
</div>
</section>
<section id="alphago" class="slide level2">
<h2>AlphaGo</h2>
<div style="font-size:1.6em;font-weight:600">
<p>Go is one of the hardest games in the world for AI because of the huge number of different game scenarios and moves. The number of potential legal board positions is greater than the number of atoms in the universe.</p>
<div style="display:flex;">
<div style="width:50%">
<p>The core of AlphaGo is a deep neural network. It was initially trained to learn playing by using a database of around 30 million recorded historical moves. After the training, the system was cloned and it was trained further playing large numbers of games against other instances of itself, using reinforcement learning to improve its play. During this training AlphaGo learned new strategies which were never played by humans.</p>
<p>A newer version named AlphaGo Zero skips the step of being trained and learns to play simply by playing games against itself, starting from completely random play.</p>
</div>
<div style="width:50%">
<p><img data-src="../images/slides_212.png"></p>
</div>
</div>
</div>
<aside class="notes">
<p>Chess 2^64 legal positions, Go 2^120!!!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="libratus" class="slide level2">
<h2>Libratus</h2>
<div style="font-size:1.4em;font-weight:600">
<p>An artificial intelligence called Libratus has beaten four of the world’s best poker players in a grueling 20-day tournament in January 2017.</p>
<p>Poker is more difficult because it’s a game with imperfect information. With chess and Go, each player can see the entire board, but with poker, players don’t get to see each other’s hands. Furthermore, the AI is required to bluff and correctly interpret misleading information in order to win.</p>
“We didn’t tell Libratus how to play poker. We gave it the rules of poker and said ‘learn on your own’.” The AI started playing randomly but over the course of playing trillions of hands was able to refine its approach and arrive at a winning strategy.
<div class="volumeImageWrapper" style="float:right">
<p><img data-src="../images/slides_214.jpg"></p>
</div>
</div>
</section>
<section id="types-of-artificial-intelligence" class="slide level2">
<h2>Types of Artificial Intelligence</h2>
<div style="font-size:1.7em;font-weight:600">
<p><span style="color:#ff0000">Discriminative AI</span> is designed to differentiate and classify input, but not to create new content. Examples include image or speech recognition, credit scoring or stock price prediction.</p>
<p><span style="color:#ff0000">Generative AI</span> is able to generate new content based on existing information and user specifications. This includes texts, images, videos, program code, etc. The generated content can often hardly be distinguished from human-generated content. As things stand at present, however, they are pure recombinations of learned knowledge.</p>
<p>Well-known examples of generative AI are language models for generating text, such as GPT-3 or GPT-4, and the chatbot ChatGPT based on them, or image generators such as Stable Diffusion and DALL-E.</p>
</div>
</section>
<section id="chatgpt" class="slide level2">
<h2>ChatGPT</h2>
<div style="font-size:1.8em;font-weight:600">
<p>ChatGPT is a generative AI that produces human-like text and communicates with humans.</p>
<p>The “GPT” in ChatGPT comes from the language model of the same name, which was extended for ChatGPT with various components for communication and quality assurance.</p>
<p>GPT is based on a huge neural network that essentially represents the language model. While the first GPT-3 has 175 billion parameters, the newer GPT-4 already has 1 trillion parameters. Compared to GPT-3, GPT-4 is therefore more intelligent, can deal with more extensive questions and conversations and makes fewer factual errors.</p>
</div>
</section>
<section id="chatgpt---approach" class="slide level2">
<h2>ChatGPT - Approach</h2>
<div style="font-size:1.6em;font-weight:600">
<p>ChatGPT generates its response word by word via a sequence of probabilities, with each new word depending on the previous ones.</p>
<div style="text-align:center">
<p><img data-src="../images/slides_216.png"></p>
</div>
The most probable word is not always selected; instead, randomization takes place. This means that different variants can be created for the same task.
<div style="text-align:center">
<p><img data-src="../images/slides_217.png"></p>
</div>
</div>
</section>
<section id="chatgpt---semantic-spaces-i" class="slide level2">
<h2>ChatGPT - Semantic Spaces (I)</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_218.png"></p>
</div>
</section>
<section id="chatgpt---semantic-spaces-ii" class="slide level2">
<h2>ChatGPT - Semantic Spaces (II)</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_176_new.png"></p>
</div>
</section>
<section id="chatgpt---evaluation-component" class="slide level2">
<h2>ChatGPT - Evaluation Component</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_220.png"></p>
</div>
</section>
<section id="navigation-4" class="slide level2">
<h2>Navigation</h2>
<div style="font-size: 1.6em; display:flex; flex-direction:column; font-weight:700; line-height:1;">
<p style="margin:0;">
4&nbsp; Predictive Analytics
</p>
<p style="margin:0; margin-left:80px;">
4.1&nbsp; Subject of Predictive Analytics
</p>
<p style="margin:0;margin-left:80px;">
4.2&nbsp; The Analytics Process
</p>
<p style="margin:0;margin-left:80px;">
4.3&nbsp; Data Preparation
</p>
<p style="margin:0;margin-left:80px;">
4.4&nbsp; Methods, Algorithms and Applications
</p>
<p style="margin:0;margin-left:120px;color:#ff0000;">
4.4.1&nbsp; Classification
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.1&nbsp; K-Nearest Neighbors
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.2&nbsp; Evaluating the Quality of Classification
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.3&nbsp; Decision Tree Approaches
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.4&nbsp; Logistic Regression
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.5&nbsp; Neural Networks
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.6&nbsp; Resampling
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.7&nbsp; Ensemble Learning
</p>
<p style="margin:0;margin-left:120px;">
4.4.2&nbsp; Regression
</p>
</div>
</section>
<section id="introductory-example" class="slide level2">
<h2>Introductory Example</h2>
<div style="font-size:1.4em;font-weight:600">
<p>Credit-Scoring is a typical example for a classification problem. A bank wants to determine the creditworthiness of a customer.</p>
<div style="float:left;width:60%">
<p>Assume you have the age, income, and a creditworthiness category of “yes” or “no” for a bunch of people and you want to use the age and income to predict the creditworthiness for a new person.</p>
<p>You can plot people as points on the plane and label people with an empty circle if they have low credit ratings.</p>
<p>What if a new guy comes in who is 49years old and who makes 53,000 Euro? What is his likely credit rating label?</p>
</div>
<div style="float:right;">
<p><img data-src="../images/slides_221.png"></p>
</div>
</div>
</section>
<section class="slide level2">

<div style="font-size: 1.6em; display:flex; flex-direction:column; font-weight:700; line-height:1;">
<p style="margin:0;">
4&nbsp; Predictive Analytics
</p>
<p style="margin:0; margin-left:80px;">
4.1&nbsp; Subject of Predictive Analytics
</p>
<p style="margin:0;margin-left:80px;">
4.2&nbsp; The Analytics Process
</p>
<p style="margin:0;margin-left:80px;">
4.3&nbsp; Data Preparation
</p>
<p style="margin:0;margin-left:80px;">
4.4&nbsp; Methods, Algorithms and Applications
</p>
<p style="margin:0;margin-left:120px;">
4.4.1&nbsp; Classification
</p>
<p style="margin:0;margin-left:160px;color:#ff0000;">
4.4.1.1&nbsp; K-Nearest Neighbors
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.2&nbsp; Evaluating the Quality of Classification
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.3&nbsp; Decision Tree Approaches
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.4&nbsp; Logistic Regression
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.5&nbsp; Neural Networks
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.6&nbsp; Resampling
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.7&nbsp; Ensemble Learning
</p>
<p style="margin:0;margin-left:120px;">
4.4.2&nbsp; Regression
</p>
</div>
</section>
<section id="k-nearest-neighbors" class="slide level2">
<h2>k-Nearest Neighbors</h2>
<div style="font-size:1.6em;font-weight:600">
<ul>
<li>k-Nearest Neighbors (k-NN) is an algorithm that can be used when you have a bunch of objects that have been classified or labeled in some way, and other similar objects that have not gotten classified or labeled yet, and you want a way to automatically label them.</li>
<li>The intuition behind k-NN is to consider the most similar other items defined in terms of their attributes, look at their labels, and give the unassigned item the majority vote. If there’s a tie, you randomly select among the labels that have tied for first.</li>
<li><span style="color:#1a1a1a">Procedure of k-NN:</span>
<ul>
<li><span style="color:#1a1a1a">1. Determine parameter k (= number of nearest neighbors)</span></li>
<li><span style="color:#1a1a1a">2. Calculate the distances between the new object and all known labeled objects.</span></li>
<li><span style="color:#1a1a1a">3. Choose the k objects from all known labeled objects having the smallest distance to the new object as nearest neighbors.</span></li>
<li><span style="color:#1a1a1a">4. Count the frequencies of the classes of the nearest neighbors.</span></li>
<li><span style="color:#1a1a1a">5. Assign the new object to the most frequent class.</span></li>
</ul></li>
</ul>
</div>
</section>
<section id="measuring-similarity" class="slide level2">
<h2>Measuring Similarity</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_182_new.png"></p>
</div>
</section>
<section id="unnormalized-vs.-normalized" class="slide level2">
<h2>Unnormalized vs.&nbsp;Normalized</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_183_new.png"></p>
</div>
</section>
<section id="example-i" class="slide level2">
<h2>Example (I)</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_184_new.png"></p>
</div>
</section>
<section id="example-ii" class="slide level2">
<h2>Example (II)</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_185_new.png"></p>
</div>
</section>
<section id="example-iii" class="slide level2">
<h2>Example (III)</h2>
<div style="font-size:1.4em;font-weight:600">
3. Choose the k <span style="color:#1a1a1a">nearest neighbors</span>
<div style="font-size:1em !important;font-weight:400!important; border:1px solid; margin:30px">
<table class="caption-top" style="width:100%;">
<colgroup>
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
<col style="width: 16%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><span style="color:#000000">Customer</span></th>
<th style="text-align: center;"><span style="color:#000000">Age</span></th>
<th style="text-align: center;"><span style="color:#000000">Monthly</span> <span style="color:#000000">Income</span></th>
<th style="text-align: center;"><span style="color:#000000">Monthly</span> <span style="color:#000000">Costs</span></th>
<th style="text-align: center;"><span style="color:#000000">Creditworthy</span></th>
<th style="text-align: center;"><span style="color:#000000">Distance</span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span style="color:#000000">A</span></td>
<td style="text-align: center;"><span style="color:#000000">0.0000</span></td>
<td style="text-align: center;"><span style="color:#000000">0.0303</span></td>
<td style="text-align: center;"><span style="color:#000000">0.0400</span></td>
<td style="text-align: center;"><span style="color:#000000">yes</span></td>
<td style="text-align: center;"><span style="color:#000000">0.4347</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span style="color:#000000">C</span></td>
<td style="text-align: center;"><span style="color:#000000">0.1714</span></td>
<td style="text-align: center;"><span style="color:#000000">0.3333</span></td>
<td style="text-align: center;"><span style="color:#000000">0.3600</span></td>
<td style="text-align: center;"><span style="color:#000000">yes</span></td>
<td style="text-align: center;"><span style="color:#000000">0.1726</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span style="color:#000000">E</span></td>
<td style="text-align: center;"><span style="color:#000000">0.3143</span></td>
<td style="text-align: center;"><span style="color:#000000">0.1818</span></td>
<td style="text-align: center;"><span style="color:#000000">0.2000</span></td>
<td style="text-align: center;"><span style="color:#000000">no</span></td>
<td style="text-align: center;"><span style="color:#000000">0.2010</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span style="color:#000000">F</span></td>
<td style="text-align: center;"><span style="color:#000000">0.4286</span></td>
<td style="text-align: center;"><span style="color:#000000">0.3939</span></td>
<td style="text-align: center;"><span style="color:#000000">0.6000</span></td>
<td style="text-align: center;"><span style="color:#000000">no</span></td>
<td style="text-align: center;"><span style="color:#000000">0.4482</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span style="color:#000000">G</span></td>
<td style="text-align: center;"><span style="color:#000000">0.4857</span></td>
<td style="text-align: center;"><span style="color:#000000">0.2121</span></td>
<td style="text-align: center;"><span style="color:#000000">0.1200</span></td>
<td style="text-align: center;"><span style="color:#000000">yes</span></td>
<td style="text-align: center;"><span style="color:#000000">0.3090</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span style="color:#000000">X</span></td>
<td style="text-align: center;"><span style="color:#000000">0.2286</span></td>
<td style="text-align: center;"><span style="color:#000000">0.3636</span></td>
<td style="text-align: center;"><span style="color:#000000">0.2000</span></td>
<td style="text-align: center;"><span style="color:#000000">?</span></td>
<td style="text-align: center;"><span style="color:#000000">&nbsp;</span></td>
</tr>
</tbody>
</table>
</div>
<p>4. Count the numbers of class members</p>
<p><span style="color:#ff3300">3 x yes</span> ; 2 x no</p>
<p>5. Assign object to most frequent class</p>
<p><span style="color:#ff3300">Customer is creditworthy!</span></p>
</div>
</section>
<section id="creation-and-use-of-models" class="slide level2">
<h2>Creation and Use of Models</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_188_new.png"></p>
</div>
</section>
<section id="calculating-accuracies" class="slide level2">
<h2>Calculating Accuracies</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_189_new.png"></p>
</div>
</section>
<section id="determining-parameter-k" class="slide level2">
<h2>Determining Parameter k</h2>
<div style="font-size:1.7em;font-weight:600;float:left;">
<p>1. Split the original labeled dataset into training and test data.</p>
<p>2. Pick an evaluation metric. Misclassification rate or accuracy are good ones.</p>
<p>3. Run k-NN a few times, changing k and checking the evaluation measure.</p>
<p>4. Optimize k by picking the one with the best evaluation measure.</p>
</div>
<div style="float:right">
<table class="caption-top">
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th style="text-align: center;"><span style="color:#1a1a1a">k</span></th>
<th style="text-align: center;"><span style="color:#1a1a1a">Accuracy</span> <span style="color:#1a1a1a"> </span></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: center;"><span style="color:#1a1a1a">1</span></td>
<td style="text-align: center;"><span style="color:#1a1a1a">0.720</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span style="color:#1a1a1a">2</span></td>
<td style="text-align: center;"><span style="color:#1a1a1a">0.685</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span style="color:#1a1a1a">3</span></td>
<td style="text-align: center;"><span style="color:#1a1a1a">0.740</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span style="color:#1a1a1a">4</span></td>
<td style="text-align: center;"><span style="color:#1a1a1a">0.745</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span style="color:#1a1a1a">5</span></td>
<td style="text-align: center;"><span style="color:#ff0000">0.770</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span style="color:#1a1a1a">6</span></td>
<td style="text-align: center;"><span style="color:#1a1a1a">0.740</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span style="color:#1a1a1a">7</span></td>
<td style="text-align: center;"><span style="color:#1a1a1a">0.750</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span style="color:#1a1a1a">8</span></td>
<td style="text-align: center;"><span style="color:#1a1a1a">0.750</span></td>
</tr>
<tr class="odd">
<td style="text-align: center;"><span style="color:#1a1a1a">9</span></td>
<td style="text-align: center;"><span style="color:#1a1a1a">0.765</span></td>
</tr>
<tr class="even">
<td style="text-align: center;"><span style="color:#1a1a1a">10</span></td>
<td style="text-align: center;"><span style="color:#1a1a1a">0.760</span></td>
</tr>
</tbody>
</table>
</div>
</section>
<section id="navigation-5" class="slide level2">
<h2>Navigation</h2>
<div style="font-size: 1.6em; display:flex; flex-direction:column; font-weight:700; line-height:1;">
<p style="margin:0;">
4&nbsp; Predictive Analytics
</p>
<p style="margin:0; margin-left:80px;">
4.1&nbsp; Subject of Predictive Analytics
</p>
<p style="margin:0;margin-left:80px;">
4.2&nbsp; The Analytics Process
</p>
<p style="margin:0;margin-left:80px;">
4.3&nbsp; Data Preparation
</p>
<p style="margin:0;margin-left:80px;">
4.4&nbsp; Methods, Algorithms and Applications
</p>
<p style="margin:0;margin-left:120px;">
4.4.1&nbsp; Classification
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.1&nbsp; K-Nearest Neighbors
</p>
<p style="margin:0;margin-left:160px;color:#ff0000;">
4.4.1.2&nbsp; Evaluating the Quality of Classification
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.3&nbsp; Decision Tree Approaches
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.4&nbsp; Logistic Regression
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.5&nbsp; Neural Networks
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.6&nbsp; Resampling
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.7&nbsp; Ensemble Learning
</p>
<p style="margin:0;margin-left:120px;">
4.4.2&nbsp; Regression
</p>
</div>
</section>
<section id="evaluating-the-quality-of-classification-i" class="slide level2">
<h2>Evaluating the quality of Classification (I)</h2>
<div style="font-size:1.6em;font-weight:600">
<p>True positives (TP), true negatives (TN), false positives (FP), and false negatives (FN), are the four different possible outcomes of a single prediction for a two-class case. A false positive is when the outcome is incorrectly classified as “yes”, when it is in fact “no”. A false negative is when the outcome is incorrectly classified as negative, when it is in fact positive. True positives and true negatives are obviously correct classifications.</p>
</div>
<div style="text-align:center">
<p><img data-src="../images/slides_223.png"></p>
</div>
</section>
<section id="evaluating-the-quality-of-classification-ii" class="slide level2">
<h2>Evaluating the quality of Classification (II)</h2>
<div style="font-size:1.6em;font-weight:600">
Test metrics are used to assess how accurately the model predicts the known values:
<div style="text-align:center">
<p><img data-src="../images/slides_192_new.png"></p>
</div>
Most classification algorithms pursue to minimize the misclassification rate. They implicitly assume that all misclassification errors cost equally. In many real-world applications, this assumption is not true. Cost-sensitive learning takes costs, such as the misclassification cost, into consideration. Using costs, the error rate can be calculated via:
<div style="text-align:center">
<p><img data-src="../images/slides_1922_new.png"></p>
</div>
</div>
</section>
<section id="evaluating-the-quality-of-classification-iii" class="slide level2">
<h2>Evaluating the quality of Classification (III)</h2>
<div style="font-size:1.3em;font-weight:600">
<p>Misclassification rate and accuracy can be misleading, for example in the case of imbalanced samples. Extreme case:</p>
<div style="text-align:center">
<p><img data-src="../images/slides_193_new.png"></p>
</div>
<p>For problems like, this additional measures are required to evaluate a classifier.</p>
<span style="color:#ff0000">Sensitivity</span> (true positive rate, recall) measures the proportion of positives that are correctly identified as such. <span style="color:#ff0000">Specificity</span> (true negative rate) measures the proportion of negatives that are correctly identified as such.
<div style="text-align:center">
<p><img data-src="../images/slides_193_2_new.png"></p>
</div>
Using both measures, we can compute the <span style="color:#ff0000">Balanced Accuracy</span>
<div style="text-align:center">
<p><img data-src="../images/slides_193_3_new.png"></p>
</div>
</div>
</section>
<section id="problem-of-imbalancing-and-accuracy" class="slide level2">
<h2>Problem of Imbalancing and Accuracy</h2>
<div style="font-size:1.6em;font-weight:600">
<p>Assume the following case: A credit card company wants to create a fraud detection system to include it into their transactional systems. The outcomes should be “Accept” (Y) and “Reject” (N). Because fraud rarely occurs, the data set consists of 320 observations for Y and 139 for N. They are partitioned into training and test set. Finally, the model is trained and tested.</p>
<p>Because of the majority of the Y class, the training process concentrates on these cases because their correct classification promises the highest accuracy.</p>
<p>The results of the test of the model is consequently:</p>
<div>
<p><img data-src="../images/slides_194_new.png"></p>
</div>
<p>Thus, the model is blind for the N cases. But these are the ones of primary interest for the company.</p>
</div>
</section>
<section id="evaluating-the-quality-of-classification-iv" class="slide level2">
<h2>Evaluating the quality of Classification (IV)</h2>
<div style="display:flex; font-size:1.3em;font-weight:600">
<div style="width:50%">
<span style="color:#ff0000">Precision</span> measures the proportion of predicted positives who are true positives. A precision of 0.5 means that whenever the model classifies a positive, there is a 50% chance of it really being a positive.The higher the precision the smaller the number of false positives.
<div>
<p><img data-src="../images/slides_195_new.png"></p>
</div>
<p><span style="color:#ff0000">Recall</span> measures the percentage of positives the model is able to catch. It is defined as the number of true positives divided by the total number of positives in the dataset. A recall of 50% would mean that 50% of the positives had been predicted as such by the model while the other 50% of positives have been missed by the model.</p>
</div>
<div style="width:50%;text-align:center;">
<p><img data-src="../images/slides_224.png"></p>
</div>
</div>
<p style="text-align:right">
Source: Wikipedia
</p>
</section>
<section id="evaluating-the-quality-of-classification-v" class="slide level2">
<h2>Evaluating the quality of Classification (V)</h2>
<div style="font-size:1.6em;font-weight:600">
The F1 Score can be interpreted as the weighted average of both precision and recall. The main idea of the F1 Score is to strike a balance between both precision and recall and measure it in a single metric.
<div style="text-align:center">
<p><img data-src="../images/slides_196_new.png"></p>
</div>
<p>A F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.</p>
<p>It is commonly used in cases of high class imbalance.</p>
</div>
</section>
<section id="creation-and-use-of-models-1" class="slide level2">
<h2>Creation and Use of Models</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_197_new.png"></p>
</div>
</section>
<section id="navigation-6" class="slide level2">
<h2>Navigation</h2>
<div style="font-size: 1.6em; display:flex; flex-direction:column; font-weight:700; line-height:1;">
<p style="margin:0;">
4&nbsp; Predictive Analytics
</p>
<p style="margin:0; margin-left:80px;">
4.1&nbsp; Subject of Predictive Analytics
</p>
<p style="margin:0;margin-left:80px;">
4.2&nbsp; The Analytics Process
</p>
<p style="margin:0;margin-left:80px;">
4.3&nbsp; Data Preparation
</p>
<p style="margin:0;margin-left:80px;">
4.4&nbsp; Methods, Algorithms and Applications
</p>
<p style="margin:0;margin-left:120px;">
4.4.1&nbsp; Classification
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.1&nbsp; K-Nearest Neighbors
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.2&nbsp; Evaluating the Quality of Classification
</p>
<p style="margin:0;margin-left:160px;color:#ff0000;">
4.4.1.3&nbsp; Decision Tree Approaches
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.4&nbsp; Logistic Regression
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.5&nbsp; Neural Networks
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.6&nbsp; Resampling
</p>
<p style="margin:0;margin-left:160px;">
4.4.1.7&nbsp; Ensemble Learning
</p>
<p style="margin:0;margin-left:120px;">
4.4.2&nbsp; Regression
</p>
</div>
</section>
<section id="which-one-is-better" class="slide level2">
<h2>Which one is better?</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_199_new.png"></p>
</div>
</section>
<section id="introductory-example-1" class="slide level2">
<h2>Introductory Example</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_225.png"></p>
</div>
<aside class="notes">
<p>To choose the relevant features, you can apply several methods and after this you may train a selection model using for example logistic regression. The whole process would be very time-consuming and even costly. As an alternative, you can apply a decision tree algorithm which is a stepwise or recursive classification mechanism.</p>
<p>Proportions in the leafs can be interpreted as probabilities.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="decision-trees-i" class="slide level2">
<h2>Decision Trees (I)</h2>
<div style="font-size:1.6em;font-weight:600">
<p>Decision trees belong to the hierarchical methods of classification. They analyze step-by-step (recursive partitioning).</p>
<p>A decision tree consists of nodes and borders. The topmost node (without any parent node) is called “root”. A node without a child node is called “leaf”. Nodes that have parent and child nodes are called “interior nodes”. The interior nodes represent the splitting of the included object sets. An interior node has at least two child nodes (sons). If every interior node has exactly two child nodes, the tree is called a “binary tree”.</p>
<p>A decision tree method starts at the root, which includes all objects. The different features are compared (with an adequate measure) regarding their suitability of classification. The most appropriate feature determines the branching of the current set of objects: regarding this feature, the current set of objects is divided into disjoint subsets (partitioning). This method is now used recursively to the created child nodes (subsets).</p>
</div>
</section>
<section id="decision-trees-ii" class="slide level2">
<h2>Decision Trees (II)</h2>
<div style="font-size:1.6em;font-weight:600">
<p>
Graphically, decision tree models divide the dataspace in a large number of subspaces and search for the variables which are able to split the dataspace with the greatest homogeneity. We can think of the decision tree as a map of different path. For a distinct combination of predictor variables and their observed values, we would enter a specific path, which gives the classification in the leaf of the decision tree.
</p>
<div style="float:left;width:50%">
<p>The decision tree approach does not require any assumption about the functional form of variables or distributions. Furthermore in contrast to parametric models like linear regressions, the decision tree algorithm can model multiple structures as well as complex relationships within the data, which would be difficult to replicate in a linear model.</p>
</div>
<div class="volumeImageWrapper" style="float:right">
<p><img data-src="../images/slides_241.png"></p>
</div>
</div>
</section>
<section id="decision-trees-iii" class="slide level2">
<h2>Decision Trees (III)</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_242.png"></p>
</div>
<p style="text-align:center">
Source: http://iopscience.iop.org/article/10.1088/1749-4699/5/1/015004
</p>
</section>
<section id="overview-of-important-decision-tree-methods" class="slide level2">
<h2>Overview of important Decision Tree Methods</h2>
<table class="comparison-table">
<tbody><tr>
<th>
Name
</th>
<th>
CART
</th>
<th>
ID3
</th>
<th>
C5.0
</th>
<th>
CHAID
</th>
<th>
Random Forests
</th>
</tr>
<tr>
<td>
<b>Idea</b>
</td>
<td>
Choose the attribute with the highest information content
</td>
<td>
One of the first methods from Quinlan; uses the concept of information gain
</td>
<td>
Like ID3 based on the concept of information gain
</td>
<td>
Choose the attribute that is most dependent on the target variable
</td>
<td>
Construct many trees with different sets of features and samples (randomly). Result by voting.
</td>
</tr>
<tr>
<td>
<b>Measure used</b>
</td>
<td>
Gini-Index
</td>
<td>
Information gain (entropy)
</td>
<td>
Ratio of information gain
</td>
<td>
Chi-square split
</td>
<td>
Optional, mostly Gini-Index
</td>
</tr>
<tr>
<td>
<b>Type of Splitting</b>
</td>
<td>
Binary
</td>
<td>
Complete, pruning
</td>
<td>
Complete, pruning
</td>
<td>
Complete, pruning
</td>
<td>
Complete
</td>
</tr>
</tbody></table>
</section>
<section id="introductory-example-2" class="slide level2">
<h2>Introductory Example</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_205_new.png"></p>
</div>
</section>
<section id="splitting-with-entropy-in-id3" class="slide level2">
<h2>Splitting with Entropy in ID3</h2>
<div class="descriptive-wrapper">
<p><img data-src="../images/slides_206_new.png"></p>
</div>
</section>
<section id="calculating-the-information-gain" class="slide level2">
<h2>Calculating the Information Gain</h2>
<div style="font-size:1.6em;font-weight:600">
The information gain is a measure, that shows (by combination of the entropies) the appropriateness of an attribute for splitting:
<div style="text-align:center">
<p><img data-src="../images/slides_207_1_new.png"></p>
</div>
where m = number of values (here two: light, strong), ti = number of data sets with strong or light wind (8 resp. 6), t = total number of data sets (14) and entropy(t) = entropy before splitting.
<div style="text-align:center">
<p><img data-src="../images/slides_207_2_new.png"></p>
</div>
</div>
</section>
<section id="decision-using-id3" class="slide level2">
<h2>Decision using ID3</h2>
<div style="font-size:1.6em;font-weight:600">
<p>Information gain (outlook) = 0.246</p>
<p>Information gain (humidity) = 0.151</p>
<p>Information gain (wind) = 0.048</p>
<p>Information gain (temperature) = 0.029</p>
<p>We choose the attribute with the largest information gain (here: outlook) for the first splitting.</p>
<p>As solution we obtain the following tree:</p>
<div style="text-align:center;margin-top:2em">
<p><img data-src="../images/slides_243.png"></p>
</div>
</div>
</section>
<section id="decision-using-c5.0" class="slide level2">
<h2>Decision using C5.0</h2>
<div style="font-size:1.3em;font-weight:600">
<p>
ID3 tends to favor attributes that have a large number of values, resulting in larger trees. For example, if we have an attribute that has a distinct value for each record, then the entropy is 0, thus the information gain is maximal.
</p>
<p>
To compensate for this, C5.0 is a further development that uses the information gain ratio as a splitting criterion:
</p>
<div style="text-align:center">
<p><img data-src="../images/slides_209_new.png"></p>
</div>
<p>
In the case of our example the GainRatio of Windy is
</p>
<div style="text-align:center">
<p><img data-src="../images/slides_209_2_new.png"></p>
</div>
<p>
and the GainRatio of Outlook is
</p>
<div style="text-align:center">
<p><img data-src="../images/slides_209_2_new.png"></p>
</div>
</div>
</section>
<section id="handling-numerical-attributes" class="slide level2">
<h2>Handling Numerical Attributes</h2>
<div style="font-size:1.6em;font-weight:600">
<p>
Numerical attributes are usually splitted binary. In contrast to categorical attributes many possible splitting points exist .
</p>
<p>
The splitting point with the highest information gain is looked for. For this, the potential attribute is sorted according to its values first and then all possible splitting point and the corresponding information gains are calculated. In extreme cases there exists n-1 possibilities.
</p>
<div style="text-align:center">
<p><img data-src="../images/slides_210_new.png"></p>
</div>
</div>
</section>
<section id="the-cart-algorithm" class="slide level2">
<h2>The CART Algorithm</h2>
<div style="font-size:1.6em;font-weight:600">
<p>
The CART algorithm (Classification And Regression Trees) constructs trees that have only binary splits. Like C5.0, it is able to handle categorical and numerical attributes.
</p>
<p>
As a measure for the impurity of a node t, CART uses the Gini Index. In the case of two classes the Gini Index is defined as:
</p>
<div>
<p><img data-src="../images/slides_211_new.png"></p>
</div>
</div>
</section>
<section id="splitting-in-cart" class="slide level2">
<h2>Splitting in CART</h2>
<div style="text-align:center">
<p><img data-src="../images/slides_212_new.png"></p>
</div>
</section>
<section id="coherence-between-entropy-and-gini-index" class="slide level2">
<h2>Coherence between Entropy and Gini Index</h2>
<div style="text-align:center">
<img data-src="../images/slides_213_new.png">
<p style="color:#000000;font-size:1.6em">
<strong>Remark: Entropy has been scaled from (0, 1) to (0, 0.5)!</strong>
</p>
</div>
</section>
<section id="overfitting-i" class="slide level2">
<h2>Overfitting (I)</h2>
<div style="font-size:1.6em;font-weight:600">
<div>
<p>
Most decision tree algorithms partition training data until every node contains objects of a single class, or until further partitioning is impossible because two objects have the same value for each attribute but belong to different classes. If there are no such conflicting objects, the decision tree will correctly classify all training objects.
</p>
</div>
<div>
<p style="float:left;width:65%">
If tree performance is measured from the number of correctly classified cases it is com-mon to find that the training data gives an over-optimistic guide to future performance,i.e.&nbsp;with new data. A tree should exhibit generalization, i.e.&nbsp;work well with data other than those used to generate it. When the tree grows during training it often shows a decrease in generalization. This is because the deeper nodes are fitting noise in the training data not representative over the entire universe from which the training set was sampled. This is called ‘overfitting’.
</p>
<div style="float:right">
<p><img data-src="../images/slides_244.png"></p>
</div>
</div>
</div>
</section>
<section class="slide level2">

<p>Overfitting (II)</p>
<p>The Iearner overfits to correctly classify‚ the noisy data objects</p>
<p>Noisy or dirty data objects</p>
<p><img data-src="../images/slides_245.png"></p>
</section>
<section class="slide level2">

<p>Random Forest (I)</p>
<p><span style="color:#000000">Random forest is an ensemble classifier that consists of many decision trees. </span></p>
<p><span style="color:#000000">For every tree a subset of the data objects and a subset of features is randomly chosen. Then the tree is constructed usually using the Gini Index. </span></p>
<p><span style="color:#000000">In the end, a simple majority vote is taken for prediction.</span></p>
<p><img data-src="../images/slides_246.png"></p>
<p><span style="color:#000000">Algorithm</span> <span style="color:#000000">:</span></p>
<p><span style="color:#000000">1. Create n samples from the original data. Frequent sample size is 2/3.</span></p>
<p><span style="color:#000000">2. For each of the samples, grow a tree, with the following modification: at each node, rather than choosing the best split among all predictors, randomly sample m* of the m predictors and choose the best split from among those variables.</span></p>
<p><span style="color:#000000">3. Predict by aggregating the predictions of the n trees (majority votes).</span></p>
</section>
<section class="slide level2">

<p><span style="color:#000000"> <strong>Random Forest (II)</strong> </span></p>
<p><span style="color:#000000">Voting-Principle of Random Forest:</span></p>
<p><span style="color:#000000">To</span> <span style="color:#000000"> </span> <span style="color:#000000">avoid</span> <span style="color:#000000"> </span> <span style="color:#000000">overfitting</span> <span style="color:#000000"> </span> <span style="color:#000000">effects</span> <span style="color:#000000">, </span> <span style="color:#000000">the</span> <span style="color:#000000"> </span> <span style="color:#000000">size</span> <span style="color:#000000"> </span> <span style="color:#000000">and</span> <span style="color:#000000"> </span> <span style="color:#000000">the</span> <span style="color:#000000"> </span> <span style="color:#000000">depth</span> <span style="color:#000000"> </span> <span style="color:#000000">of</span> <span style="color:#000000"> </span> <span style="color:#000000">the</span> <span style="color:#000000"> </span> <span style="color:#000000">trees</span> <span style="color:#000000"> </span> <span style="color:#000000">can</span> <span style="color:#000000"> </span> <span style="color:#000000">be</span> <span style="color:#000000"> </span> <span style="color:#000000">restricted</span> <span style="color:#000000">. </span></p>
<p><img data-src="../images/slides_247.png"></p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.1.1 K-Nearest Neighbors</p>
<p>4.4.1.2 Evaluating the Quality of Classification</p>
<p>4.4.1.3 Decision Tree Approaches</p>
<p><span style="color:#c00000">4.4.1.4 Logistic Regression</span></p>
<p>4.4.1.5 Neural Networks</p>
<p>4.4.1.6 Resampling</p>
<p>4.4.1.7 Ensemble Learning</p>
<p>4.4.2 Regression</p>
</section>
<section class="slide level2">

<p>Introductory Example</p>
<p><img data-src="../images/slides_248.png"></p>
<p><img data-src="../images/slides_249.png"></p>
<p><span style="color:#000000"> <strong>websites (features)</strong> </span></p>
<p><span style="color:#000000">1=visited</span></p>
<p><span style="color:#000000">0=not visited</span></p>
<p><span style="color:#000000"> <strong>ad (target)</strong> </span></p>
<p><span style="color:#000000">1=clicked</span></p>
<p><span style="color:#000000">0=not clicked</span></p>
<p><span style="color:#ff0000">Giant sparse matrix!</span></p>
<p><span style="color:#ff0000">One matrix for every ad!</span></p>
<aside class="notes">
<p>Giant sparse matrix! A sparse matrix is a matrix in which most of the elements are zero. One matrix for every ad! Can be solved by Naïve Bayes! Looking for an alternative</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<p>Why not classical linear regression?</p>
<p>It is possible to implement a linear regression on such a dataset where Y={0,1}.</p>
<p>Problems:</p>
<p>The predicted values of the linear model can be greater than 1 or less than 0</p>
<p>e is not normally distributed because Y takes on only two values</p>
<p>The error terms are heteroscedastic (the error variance is not constant for all values of X)</p>
<p>Source: Bichler (2015): Course Business Analytics, TU München</p>
</section>
<section class="slide level2">

<p>Logistic regression (I)</p>
<p>Logistic regression is a regression model where the dependent variable is categorical. The classical logistic regression is a binary classifier, where the dependent variable has two states. The output of a logistic regression model ranges between 0 and 1.</p>
<p>Logistic regression uses the logistic function (or Sigmoid function) because it can take an input with any value from negative to positive infinity, whereas the output always takes values between zero and one and hence is interpretable as a probability.</p>
<p>It is defined as:</p>
<p><img data-src="../images/slides_250.png"></p>
</section>
<section class="slide level2">

<p>Logistic regression (II)</p>
<p>If we set</p>
<p>the logistic function can now be written as:</p>
<p>We interpret F(x) as the conditional probability that the class attribute has the value 1 with the given input vector x.</p>
<p>The coefficients ß0 and ß can be estimated via Maximum Likelihood Estimation.</p>
<p>The parameter ß0 represents the unconditional probability of “Y=1” knowing nothing about the feature vector x.</p>
<p>The parameter vector β defines the slope of the logit function. It determines the extent to which certain features contribute for increased or decreased likelihood to “Y=1”.</p>
<p>The output of a logistic model is a probability. To use this for classification purposes:</p>
<p>If the predicted probability is &gt; 0.5 the label is 1</p>
<p>and otherwise 0.</p>
<aside class="notes">
<p>odd = Chance</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.1.1 K-Nearest Neighbors</p>
<p>4.4.1.2 Evaluating the Quality of Classification</p>
<p>4.4.1.3 Decision Tree Approaches</p>
<p>4.4.1.4 Logistic Regression</p>
<p><span style="color:#c00000">4.4.1.5 Neural Networks</span></p>
<p>4.4.1.6 Resampling</p>
<p>4.4.1.7 Ensemble Learning</p>
<p>4.4.2 Regression</p>
</section>
<section class="slide level2">

<p>Functionality of Human Neurons</p>
<p><img data-src="../images/slides_251.png"></p>
<p><img data-src="../images/slides_252.png"></p>
<p>A Look into the Nervous System</p>
<p>Design of a Neuron</p>
</section>
<section class="slide level2">

<p>An Easy Example (I)</p>
<p><span style="color:#000000"> <strong>f(x) = Activation function</strong> </span></p>
<p><span style="color:#000000"> <strong>e.g. </strong> </span></p>
<p><span style="color:#000000"> <strong>where t = Stimulus threshold</strong> </span></p>
</section>
<section class="slide level2">

<p>An Easy Example (II)</p>
<p><span style="color:#000000"> <strong>f(x) = Activation function</strong> </span></p>
<p><span style="color:#000000"> <strong>e.g. </strong> </span></p>
<p><span style="color:#000000"> <strong>where t = Stimulus threshold</strong> </span></p>
</section>
<section class="slide level2">

<p>Functionality of a Neuron</p>
<p><img data-src="../images/slides_253.jpg"></p>
</section>
<section class="slide level2">

<p><span style="color:#000000"> <strong>For the case of n inputs, we can rewrite the neuron’s function to</strong> </span></p>
<p><span style="color:#000000"> <strong>with b = -t. b is known as the perceptron’s bias. The result of this function would then be fed into an activation function to produce a labeling</strong> </span></p>
<p><img data-src="../images/slides_254.png"></p>
<p><span style="color:#000000"> <strong>This results in a linear classifier. Finally, we have to pick a line that best separates the labeled data. The training of the perceptron consists of feeding it multiple training samples and calculating the output for each of them. After each sample, the weights w are adjusted in such a way so as to minimize the output error, defined for example as accuracy or MSE.</strong> </span></p>
<p><span style="color:#000000">Source: http://www.toptal.com/machine-learning/an-introduction-to-deep-learning-from-perceptrons-to-deep-networks</span></p>
</section>
<section class="slide level2">

<p>The Multilayer Perceptron</p>
<p><span style="color:#000000"> <strong>The single perceptron approach has a major drawback: it can only learn linear functions. To address this problem, we’ll need to use a multilayer perceptron, also known as feedforward neural network. Here, we add layers between the input and the output layer, so-called hidden layers</strong> </span> <span style="color:#000000">. The hidden layer is where the network stores it’s internal abstract representation of the training data.</span></p>
<p><span style="color:#000000">Input Neurons</span> <span style="color:#000000">: </span> <span style="color:#000000">receive</span> <span style="color:#000000"> </span> <span style="color:#000000">signals</span> <span style="color:#000000"> </span> <span style="color:#000000">from</span> <span style="color:#000000"> </span> <span style="color:#000000">the</span> <span style="color:#000000"> </span> <span style="color:#000000">outer</span> <span style="color:#000000"> </span> <span style="color:#000000">world</span> <span style="color:#000000">.</span></p>
<p><span style="color:#000000">Hidden Neurons</span> <span style="color:#000000">: </span> <span style="color:#000000">have</span> <span style="color:#000000"> an internal </span> <span style="color:#000000">representation</span> <span style="color:#000000"> </span> <span style="color:#000000">of</span> <span style="color:#000000"> </span> <span style="color:#000000">the</span> <span style="color:#000000"> </span> <span style="color:#000000">outer</span> <span style="color:#000000"> </span> <span style="color:#000000">world</span> <span style="color:#000000">.</span></p>
<p><span style="color:#000000">Output Neurons</span> <span style="color:#000000">: pass </span> <span style="color:#000000">signals</span> <span style="color:#000000"> </span> <span style="color:#000000">to</span> <span style="color:#000000"> </span> <span style="color:#000000">the</span> <span style="color:#000000"> </span> <span style="color:#000000">outer</span> <span style="color:#000000"> </span> <span style="color:#000000">world</span> <span style="color:#000000">.</span></p>
</section>
<section class="slide level2">

<p>Types of Activation Functions</p>
<p><span style="color:#000000">A linear composition of linear functions is still just a linear function, so most neural networks use non-linear activation functions:</span></p>
<p><span style="color:#000000"> <strong>tangens</strong> </span> <span style="color:#000000"> __ __ </span> <span style="color:#000000"> <strong>hyperbolicus</strong> </span></p>
<p><span style="color:#000000">logistic</span> <span style="color:#000000">function</span> <span style="color:#000000">(sigmoid)</span></p>
</section>
<section class="slide level2">

<p>Design of a Multilayer Perceptron</p>
</section>
<section class="slide level2">

<ul>
<li>The Backpropagation algorithm is used for calculating the weights. In a training phase, the weights are iteratively calculated using training data sets in such a way that the difference between the calculated and the expected (true) results is minimized. Because the simultaneous calculation of all weights is not possible, they must be found via a learning process. The backpropagation algorithm looks for the minimum of the error function in weight space using the method of gradient descent.</li>
<li>The procedure in principle:
<ul>
<li>(1) Define the initial weights</li>
<li>(2) Put the training set into the input layer</li>
<li>(3) Calculate the result (value of the output layer) via successive processing one layer after the other</li>
<li>(4) Compare the output values and target values and calculate the difference</li>
<li>(5) Iterate steps (2) to (4) for every training set</li>
<li>(6) Calculate the total error. Adjust the weights beginning with the output layer towards the input layer (backpropagation)</li>
<li>(7) Iterate steps (2) to (6) until the total error reaches the defined error-level or the number of maximum iterations is reached.</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<p>Adjusting the Weights (I)</p>
<p><span style="color:#000000">The error of a training set </span> <span style="color:#000000">i</span> <span style="color:#000000"> is calculated using the quadratic deviation between the values </span> <span style="color:#000000">o</span> <span style="color:#000000">ij</span> <span style="color:#000000"> of the neurons of the output layer and their corresponding true values </span> <span style="color:#000000">t</span> <span style="color:#000000">ij</span> <span style="color:#000000">. </span></p>
<p><span style="color:#000000">The sum of the errors of all h training objects is the total error value E:</span></p>
<p><img data-src="../images/slides_256.png"></p>
</section>
<section class="slide level2">

<p>Adjusting the Weights (II)</p>
<p><span style="color:#000000">The function E has to be minimized. Because it depends on the output neurons </span> <span style="color:#000000">o</span> <span style="color:#000000">j</span> <span style="color:#000000">, it automatically depends on their weights to the </span> <span style="color:#000000">precedent layer(s)</span> <span style="color:#000000">:</span></p>
<p><span style="color:#000000">Thus, the weights have to be found where E is minimal. </span></p>
<p><span style="color:#000000">Examples of Error functions with two weights:</span></p>
<p><img data-src="../images/slides_257.png"></p>
<p><img data-src="../images/slides_258.png"></p>
</section>
<section class="slide level2">

<p>Adjusting the Weights (III)</p>
<p><span style="color:#000000">To minimize the error (cost) function E the backpropagation algorithm uses the method of gradient descent</span> <span style="color:#000000">. This method searches those weights, where the </span> <span style="color:#000000">vector containing the partial first derivatives of the error function </span> <span style="color:#000000">(gradient) </span> <span style="color:#000000">is equal to the zero vector </span> <span style="color:#000000">(minimum):</span></p>
<p><span style="color:#000000">To adjust the weight </span> <span style="color:#000000">w</span> <span style="color:#000000">ij</span> <span style="color:#000000">, which connects neurons </span> <span style="color:#000000">i</span> <span style="color:#000000"> </span> <span style="color:#000000">to</span> <span style="color:#000000"> j, the formula is:</span></p>
<p><span style="color:#000000">where </span> <span style="color:#000000">a </span> <span style="color:#000000">represents a predefined learning rate</span> <span style="color:#000000">, </span> <span style="color:#000000">which defines the step length of each iteration in the negative gradient direction </span> <span style="color:#000000">and x</span> <span style="color:#000000">i</span> <span style="color:#000000"> denote the output value of neuron </span> <span style="color:#000000">i</span> <span style="color:#000000">.</span></p>
<p><span style="color:#000000">The adjusted weight is then computed via</span></p>
<p><img data-src="../images/slides_259.png"></p>
</section>
<section class="slide level2">

<p>Principle of Gradient Descent (I)</p>
<p><span style="color:#000000">Gradient descent is used to find the minimum of the error function</span> <span style="color:#000000">. It works iterative. In an 1-dimensional world, we define the error by</span></p>
<p><span style="color:#000000">The error function is at minimum if the error is equal to zero.</span></p>
<p><span style="color:#000000">The prediction is the result of a combination of input and weight</span></p>
<p><span style="color:#000000">The weight as the dynamic component is now adjusted until the error is at minimum. Starting</span> <span style="color:#000000"> with an initial weight, gradient descent jumps step by step into the minimum by adjusting the weight. The adjustment is done by calculating the direction </span> <span style="color:#000000">and</span> <span style="color:#000000"> the amount for a step via</span></p>
<p><span style="color:#000000">Now, the weight is adjusted via</span></p>
<p><span style="color:#000000">After repeating this several times, the minimum</span> <span style="color:#000000">is reached.</span></p>
</section>
<section class="slide level2">

<p>Principle of Gradient Descent (II)</p>
<p><span style="color:#000000">The formula </span></p>
<p><span style="color:#000000">represents the derivative of the error to the weight.</span></p>
<p><span style="color:#000000">A derivative is a term that is calculated as the slope (or gradient) of a graph at a particular point. The slope is described by drawing a tangent line to the graph at the point. So, if we are able to compute this tangent line, we might be able to compute the desired direction to reach the minima.</span></p>
<p><span style="color:#000000">Since the weight only indirectly affects the error, the chain rule must be applied</span></p>
</section>
<section class="slide level2">

<p>Principle of Gradient Descent (III)</p>
<p><span style="color:#000000">Gradient Descent isn’t perfect. When the gradients are too big we might overshoot so much that we’re even farther away than we started</span></p>
<p><span style="color:#000000">This problem is destructive because overshooting this far means we land at an even steeper slope in the opposite direction. This causes us to overshoot again even farther.</span></p>
<p><span style="color:#000000">If the gradients are too big, we can make them smaller. We do this by multiplying them by a single number between 0 and 1 (such as 0.01). This fraction is typically named alpha.</span></p>
<p><span style="color:#000000">Thus, the adjustment of the weights is done by</span></p>
<p>Source: https://iamtrask.github.io/2015/07/27/python-network-part2/</p>
</section>
<section class="slide level2">

<p>Backpropagation Step by Step (I)</p>
<p><span style="color:#000000">In the following,</span> <span style="color:#000000"> the backpropagation process will be demonstrated using a simple Neural </span> <span style="color:#000000">Network consisting of three layers: Input layer with two inputs neurons, one hidden layer with two neurons, and output layer with a single neuron: </span></p>
<p><span style="color:#000000">Our initial weights will be: w</span> <span style="color:#000000">1</span> <span style="color:#000000"> = 0.11, w</span> <span style="color:#000000">2</span> <span style="color:#000000"> = 0.21, w</span> <span style="color:#000000">3</span> <span style="color:#000000"> = 0.12, w</span> <span style="color:#000000">4</span> <span style="color:#000000"> = 0.08, w</span> <span style="color:#000000">5</span> <span style="color:#000000"> = 0.14 and w</span> <span style="color:#000000">6</span> <span style="color:#000000"> = 0.15.</span></p>
<p><img data-src="../images/slides_262.png"></p>
<p><img data-src="../images/slides_263.png"></p>
<p>Source: http://hmkcode.github.io/ai/backpropagation-step-by-step</p>
</section>
<section class="slide level2">

<p>Backpropagation Step by Step (II)</p>
<p><span style="color:#000000">Our dataset has one sample with two inputs and one output with the values inputs=[2, 3] and output=[</span> <span style="color:#ff0000">1</span> <span style="color:#000000">]. We will use given weights and inputs to predict the output. Inputs are multiplied by weights; the results are then </span> <span style="color:#000000">passed forward to next layer:</span></p>
<p><span style="color:#000000"> </span> <span style="color:#ff0000">For reasons of simplification, </span> <span style="color:#ff0000">no activation function is used </span> <span style="color:#ff0000">in the neurons.</span></p>
<p><img data-src="../images/slides_264.png"></p>
<p>Source: http://hmkcode.github.io/ai/backpropagation-step-by-step</p>
</section>
<section class="slide level2">

<p>Backpropagation Step by Step (III)</p>
<p><span style="color:#000000">The network output, or prediction, is not even close to actual output. We can calculate the difference or the error as following:</span></p>
<p><span style="color:#000000">Our main goal of the training is to reduce the error or the difference between prediction and actual output. Since actual output is constant, “not changing”, the only way to reduce the error is to change prediction value. The question now is, how to change prediction value?</span></p>
<p><img data-src="../images/slides_265.png"></p>
<p>Source: http://hmkcode.github.io/ai/backpropagation-step-by-step</p>
</section>
<section class="slide level2">

<p>Backpropagation Step by Step (IV)</p>
<p><span style="color:#000000">By decomposing prediction into its basic elements we can find that weights are the variable elements affecting prediction value. To change prediction value, we need to adjust the weights:</span></p>
<p><span style="color:#000000">We</span> <span style="color:#000000"> do this </span> <span style="color:#000000">using Backpropagation. To find a local minimum of a function using gradient descent, one takes steps proportional to the negative of the gradient of the function at the current point:</span></p>
<p><span style="color:#000000">For example, we update w</span> <span style="color:#000000">6</span> <span style="color:#000000">:</span></p>
<p><img data-src="../images/slides_266.png"></p>
<p><img data-src="../images/slides_267.png"></p>
<p><em>We can picture gradient descent optimization as a hiker (the weight coefficient) who wants to climb down a mountain (cost function) into a valley (cost minimum), and each step is determined by the steepness of the slope (gradient) and the leg length of the hiker (learning rate).</em></p>
<p><img data-src="../images/slides_269.png"></p>
<p>Source: http://hmkcode.github.io/ai/backpropagation-step-by-step</p>
</section>
<section class="slide level2">

<p>Backpropagation Step by Step (V)</p>
<p><span style="color:#000000">The derivation of the error function is evaluated by applying the chain rule:</span></p>
<p><span style="color:#000000">To update w</span> <span style="color:#000000">6</span> <span style="color:#000000"> we can apply the following formula:</span></p>
<p><span style="color:#000000">Similarly, we can derive the update formula for w5 and any other weights existing between the output and the hidden layer:</span></p>
<p><img data-src="../images/slides_271.png"></p>
<p><img data-src="../images/slides_272.png"></p>
<p>Source: http://hmkcode.github.io/ai/backpropagation-step-by-step</p>
</section>
<section class="slide level2">

<p>Backpropagation Step by Step (VI)</p>
<p><span style="color:#000000">When moving backward to update w</span> <span style="color:#000000">1</span> <span style="color:#000000">, w</span> <span style="color:#000000">2</span> <span style="color:#000000">, w</span> <span style="color:#000000">3</span> <span style="color:#000000"> and w</span> <span style="color:#000000">4</span> <span style="color:#000000"> existing between input and hidden layer, the partial derivative for the error function with respect to w</span> <span style="color:#000000">1</span> <span style="color:#000000">, for example, will be as following:</span></p>
<p><span style="color:#000000">We can find the update formula for the remaining weights w</span> <span style="color:#000000">2</span> <span style="color:#000000">, w</span> <span style="color:#000000">3</span> <span style="color:#000000"> and w</span> <span style="color:#000000">4</span> <span style="color:#000000"> in the same way.</span></p>
<p>Source: http://hmkcode.github.io/ai/backpropagation-step-by-step</p>
</section>
<section class="slide level2">

<p>Backpropagation Step by Step (VII)</p>
<p><span style="color:#000000">In summary, the update formulas for all weights will be:</span></p>
<p><span style="color:#000000">We can rewrite the update formulas in matrices:</span></p>
<p><img data-src="../images/slides_274.png"></p>
<p><img data-src="../images/slides_275.png"></p>
<p>Source: http://hmkcode.github.io/ai/backpropagation-step-by-step</p>
</section>
<section class="slide level2">

<p>Backpropagation Step by Step (VIII)</p>
<p><span style="color:#000000">With the derived formulas we can now adjust the weights:</span></p>
<p><img data-src="../images/slides_276.png"></p>
<p>Source: http://hmkcode.github.io/ai/backpropagation-step-by-step</p>
</section>
<section class="slide level2">

<p>Backpropagation Step by Step (IX)</p>
<p><span style="color:#000000">... and use the new weights to recalculate the example:</span></p>
<p><span style="color:#000000">The new prediction 0.26 is bit closer to the output than the previously predicted one 0.191. We repeat now the same process until error is close or equal to zero.</span></p>
<p><img data-src="../images/slides_277.png"></p>
<p>Source: http://hmkcode.github.io/ai/backpropagation-step-by-step</p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.1.1 K-Nearest Neighbors</p>
<p>4.4.1.2 Evaluating the Quality of Classification</p>
<p>4.4.1.3 Decision Tree Approaches</p>
<p>4.4.1.4 Logistic Regression</p>
<p>4.4.1.5 Neural Networks</p>
<p><span style="color:#c00000">4.4.1.6 Resampling</span></p>
<p>4.4.1.7 Ensemble Learning</p>
<p>4.4.2 Regression</p>
</section>
<section class="slide level2">

<p>Problems with fixed Training and Test Samples</p>
<p>Method 1 optimize</p>
<p>Test data is used for two things:</p>
<p><span style="color:#ff0000">Optimize the model training</span></p>
<p><span style="color:#0070c0">Select the best model via testing the model quality</span></p>
<p>Method 2 optimize</p>
<p>Method 3 optimize</p>
<p>This contradicts the idea of independent testing and results in:</p>
<p>Endogenization of the test data</p>
<p>Selection Bias</p>
<p>… optimize</p>
<p><span style="color:#ff0000">Rule</span> <span style="color:#ff0000">: NEVER </span> <span style="color:#ff0000">use</span> <span style="color:#ff0000"> </span> <span style="color:#ff0000">any</span> <span style="color:#ff0000"> </span> <span style="color:#ff0000">information</span> <span style="color:#ff0000"> </span> <span style="color:#ff0000">from</span> <span style="color:#ff0000"> </span> <span style="color:#ff0000">the</span> <span style="color:#ff0000"> </span> <span style="color:#ff0000">test</span> <span style="color:#ff0000"> </span> <span style="color:#ff0000">data</span> <span style="color:#ff0000"> </span> <span style="color:#ff0000">for</span> <span style="color:#ff0000"> </span> <span style="color:#ff0000">model</span> <span style="color:#ff0000"> </span> <span style="color:#ff0000">training</span> <span style="color:#ff0000">!</span></p>
</section>
<section class="slide level2">

<p>Addressing the Endogeneity Problem</p>
<p><img data-src="../images/slides_278.png"></p>
<p><img data-src="../images/slides_279.png"></p>
<p><img data-src="../images/slides_280.png"></p>
<p><img data-src="../images/slides_282.png"></p>
<p><em>Predictive</em> _ Model_</p>
<p><em>Validation Sample</em></p>
<p><img data-src="../images/slides_283.png"></p>
</section>
<section class="slide level2">

<ul>
<li>Training and test error can be highly variable, depending on precisely which observations are included in the training set and which observations are included in the validation set ( <span style="color:#ff0000">Selection Bias</span> ).
<ul>
<li>Example of differentOLS models as a result of different samples:</li>
<li>To avoid such problems, one can use so-called resampling methods.</li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<p>Cross Validation</p>
<ul>
<li>In Data Science cross validation can be used for model selection and adjustment. In these cases, cross validation is applied to the training data set. For every iteration, k-1 folds are used for model fitting and the remaining fold for testing the model (Validation). Every time, the quality measure (e.g. accuracy) for the validation fold is captured. At the endof this step, the average and the standard deviation of the measures are calculated. The best model is the one with the best ratio in high average and low standard deviation.</li>
<li>Once the model type and its optimal parameters have been selected, a final model is trained using these hyper-parameters on the <em>full</em> training set, and the generali-zation quality is measured on the test set.</li>
</ul>
<p><img data-src="../images/slides_285.png"></p>
</section>
<section class="slide level2">

<p>Cross Validation and Grid Search</p>
<ul>
<li>Partition the Dataset into a training and test set</li>
<li>For every hyperparameter value combination apply cross validation</li>
<li>For the combination with the highest (mean) quality calculate the final model with the complete training set</li>
<li>Test the final model with the test set</li>
<li>Compare the accurracies of training and test with regard to overfitting</li>
</ul>
<p>Calculate the mean quality of the validation folds, e.g. mean accurracy or mean F1</p>
</section>
<section class="slide level2">

<p>Cross Validation and Grid Search in Python</p>
<ul>
<li>Performs cross validation with the given hyperparameter combinations and manages the evaluation process</li>
</ul>
<p>Using the original libraries and functions</p>
<p><span style="color:#ff0000">KNeighborsClassifier()</span></p>
<p><span style="color:#ff0000">cross_val_score()</span></p>
<ul>
<li>Performs cross validation</li>
</ul>
<p><span style="color:#ff0000">DecisionTreeClassifier()</span></p>
<p><span style="color:#ff0000">RandomForestCl</span> <span style="color:#ff0000">…</span> <span style="color:#ff0000">()</span></p>
</section>
<section class="slide level2">

<p>Variants of Hyperparameter Optimization</p>
<ul>
<li>1. Grid Search</li>
<li>Grid search sequentially goes through a preselected list of permutations for each hyperparameter and evaluates the entire search space.</li>
<li>2. Random Search</li>
<li>Random search selects values for hyperparameters at random within a predefined distribution.</li>
<li>While a grid search is able to find the best model given the provided options, limited compute resources means that in practice, the search space selected will have to be limited. A random search on the other hand does not iterate over the entire search space.</li>
</ul>
</section>
<section class="slide level2">

<p>Other Variants of Cross Validation</p>
<ul>
<li>1. Repeated Cross Validation</li>
<li>2. Nested Cross Validation</li>
<li>Different composition of the folds by random selection.</li>
</ul>
<p><img data-src="../images/slides_295.png"></p>
</section>
<section class="slide level2">

<p>Cross Validation in Time Series</p>
<ul>
<li>In the case of time series, classical cross validation may cause problems. If we choose random samples and assign them to either the test set or the training set we are quickly in the situation of using values from the future to forecast values in the past. But we want to avoid future-looking when we train our model. If there is a temporal dependency between observations, we must preserve that relation during training and testing.</li>
<li>A procedure that can be used for cross validating a time series model is cross validation on a rolling basis. Start with a small subset of data for training purpose, forecast for the later data points and then check the accuracy for the forecasted data points. The time frame for the forecast is then included as part of the next training dataset and subsequent data points are forecasted and so on.</li>
<li>Scikit-learn provides a class <em>TimeSeriesSplit</em> to do this.</li>
</ul>
<p><img data-src="../images/slides_296.png"></p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.1.1 K-Nearest Neighbors</p>
<p>4.4.1.2 Evaluating the Quality of Classification</p>
<p>4.4.1.3 Decision Tree Approaches</p>
<p>4.4.1.4 Logistic Regression</p>
<p>4.4.1.5 Neural Networks</p>
<p>4.4.1.6 Resampling</p>
<p><span style="color:#c00000">4.4.1.7 Ensemble Learning</span></p>
<p>4.4.2 Regression</p>
</section>
<section class="slide level2">

<p>Ensemble Methods</p>
<p>Ensemble methods use different models (created via different data sets, feature sets or methods) that are simultaneously applied to the same problem. The results are sent to an aggregating operation that produces the final result.</p>
<p>The most widely used classes of ensemble methods are:</p>
<p>Bagging</p>
<p>Boosting</p>
<p>Stacking</p>
</section>
<section class="slide level2">

<p><span style="color:#ff0000">Bagging</span> <span style="color:#000000"> means to build multiple models from different subsamples of the training dataset and/or with different methods. The results are sent to an (weighted) voting operation that produces the final result.</span></p>
<p><span style="color:#000000">Source: http://rasbt.github.io/mlxtend/</span> <span style="color:#000000">user_guide</span> <span style="color:#000000">/classifier/</span> <span style="color:#000000">EnsembleVoteClassifier</span> <span style="color:#000000">/</span></p>
<p><img data-src="../images/slides_298.png"></p>
</section>
<section class="slide level2">

<p><span style="color:#ff0000">Boosting</span> <span style="color:#000000"> involves sequentially building an ensemble by training each new model instance to emphasize the training instances that previous models </span> <span style="color:#000000">mispredict</span> <span style="color:#000000">. Different variants exist, mostly based on tree methods. In general, any method can be used. This involves the usage of different methods at the different iterations when building the sequence of models. </span></p>
<p><span style="color:#000000">Source: https://blog.bigml.com/2017/03/14/introduction-to-boosted-trees/</span></p>
<p><img data-src="../images/slides_299.png"></p>
</section>
<section class="slide level2">

<p><span style="color:#ff0000">Stacking</span> <span style="color:#000000"> </span> <span style="color:#000000">means to build </span> <span style="color:#000000">multiple models (typically of differing types) and a supervisor model that learns how to best combine the predictions of the primary models. </span> <span style="color:#000000">The inputs of the </span> <span style="color:#000000">supervisor model </span> <span style="color:#000000">(meta-classifier)</span> <span style="color:#000000">are the outputs of </span> <span style="color:#000000">the other models:</span></p>
<p><span style="color:#000000">Source: http://rasbt.github.io/mlxtend/</span> <span style="color:#000000">user_guide</span> <span style="color:#000000">/classifier/</span> <span style="color:#000000">StackingClassifier</span> <span style="color:#000000">/</span></p>
<p><img data-src="../images/slides_300.png"></p>
</section>
<section class="slide level2">

<p>Types of Ensembles</p>
<p>Type 1:</p>
<p>consists of only a few models</p>
<p>each is a strong model</p>
<p>like few professional experts</p>
<p>risk of diverging opinions</p>
<p>risk of experts being biased to their experiences</p>
<p>Type 2:</p>
<p>consists of many models</p>
<p>each is a weak model as a principle</p>
<p>based on the idea of the wisdom of the masses</p>
<p>Random Forest and Gradient Boosted Trees are examples</p>
<p><img data-src="../images/slides_301.png"></p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p><span style="color:#ff0000">4.4.2 Regression</span></p>
<p>4.4.2.1 OLS</p>
<p>4.4.2.2 Ridge Regression</p>
<p>4.4.2.3 Support Vector Regression</p>
<p>4.4.2.4 Neural Networks</p>
<p>4.4.2.5 Decision Trees</p>
<p>4.4.2.6 K-Nearest Neighbors</p>
</section>
<section class="slide level2">

<p>Predicting using Regression Methods</p>
<p><img data-src="../images/slides_303.png"></p>
<p>Example: Predicting House Prices</p>
<p>Function: Price = f(SquareFootage, Bedrooms, Age, SchoolRating)</p>
<p>Source: http://www.sclgsummit.org/uploads/presentation/8934b2d0be055a2261f5d0320f5b59bb.pdf</p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.2 Regression</p>
<p><span style="color:#c00000">4.4.2.1 OLS</span></p>
<p>4.4.2.2 Ridge Regression</p>
<p>4.4.2.3 Support Vector Regression</p>
<p>4.4.2.4 Neural Networks</p>
<p>4.4.2.5 Decision Trees</p>
<p>4.4.2.6 K-Nearest Neighbors</p>
</section>
<section class="slide level2">

<p>Traditional OLS Regression Approach</p>
<p><img data-src="../images/slides_304.png"></p>
<p>Function:</p>
<p>Price = ß0 + ß1 * SquareFootage+ ß2 * Bedrooms + ß3 * Age + ß4 * SchoolRating</p>
<p>Source: http://www.sclgsummit.org/uploads/presentation/8934b2d0be055a2261f5d0320f5b59bb.pdf</p>
</section>
<section class="slide level2">

<p>Ordinary Least Squares Regression</p>
</section>
<section class="slide level2">

<p>Measuring the Quality of Fit (I)</p>
<p>Measuring the quality of fit means to measure how well the predictions of a model match the observed data.</p>
<p>A commonly-used measure is the Mean Absolute Error (MAE) which can be calculated for the training and the test set</p>
<p>A variant is the Mean Absolute Percentage Error (MAPE) which expresses the error in percent</p>
<p>While MAE and MAPE are easily interpretable, using the absolute value of the error often is not as desirable as squaring this difference. Depending on how you want your model to treat outliers, or extreme values, in your data, you may want to bring more attention to these outliers or downplay them.</p>
<p>Consequently, the most used measure in regression is the Mean Squared Error (MSE) or its variant the Root Mean Squared Error (RMSE), which is the square root of the MSE.</p>
</section>
<section class="slide level2">

<p>Measuring the Quality of Fit (II)</p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.2 Regression</p>
<p>4.4.2.1 OLS</p>
<p><span style="color:#ff0000">4.4.2.2 Ridge Regression</span></p>
<p>4.4.2.3 Support Vector Regression</p>
<p>4.4.2.4 Neural Networks</p>
<p>4.4.2.5 Decision Trees</p>
<p>4.4.2.6 K-Nearest Neighbors</p>
</section>
<section class="slide level2">

<p>Ridge Regression</p>
<aside class="notes">
<p>Example of 3 Parameters, free of range, with a range [-50,50], and a range [-5,5]. Count the numbers of combinations of the three alternatives.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<p><span style="color:#000000">Complexity can be measured as the size of the set of possible outputs for a given set of inputs.</span></p>
<p><span style="color:#000000">In this example the interval 0 to x</span> <span style="color:#000000">*</span> <span style="color:#000000"> represents the set of possible inputs. Function h</span> <span style="color:#000000">0</span> <span style="color:#000000"> has the lowest complexity because there is just one output independent of the inputs. h</span> <span style="color:#000000">2</span> <span style="color:#000000"> has the highest complexity because here the set of possible outputs is the biggest one.</span></p>
</section>
<section class="slide level2">

<p>Complexity und Generalisation</p>
<p><span style="color:#000000">Mean</span> <span style="color:#000000"> </span> <span style="color:#000000">Squared</span> <span style="color:#000000"> Error</span></p>
</section>
<section class="slide level2">

<p>Different Complexities</p>
</section>
<section class="slide level2">

<p>𝜆 → ∞ : Lowest Complexity</p>
<p>the ridge regression coefficients are equal to zero. For every input, the result is β0.</p>
<p>𝜆 = 0 : Relative High Complexity (linear Model)</p>
<p>the penalty term has no effect, and ridge regression will produce the least squares estimates.</p>
<p>Example:</p>
<p><img data-src="../images/slides_305.png"></p>
<p><span style="color:#000000">Source: </span></p>
<p><span style="color:#000000">James et al. (2013): An Introduction to Statistical Learning with R Applications, p. 215f.</span></p>
</section>
<section class="slide level2">

<p>Handling High-Dimensionality (I)</p>
<p>OLS is not suitable for high-dimensional data. Especially when the number of features p is as large as, or larger than, the number of observations, OLS cannot be applied. _ _ Regardless of whether or not there truly is a relationship between the features and the response, OLS will yield a set of coefficient estimates that result in a perfect fit to the data, such that the residuals are zero.</p>
<p>The figure shows two cases. When there are 20 observations, n &gt; p and the OLS line does not perfectly fit the data. When there are only two observations, then regardless of the values of those observations, the regression line will fit the data exactly. This is problematic because this perfect fit will almost certainly lead to overfitting of the data.</p>
<p><img data-src="../images/slides_306.png"></p>
<p><span style="color:#000000">Source: </span></p>
<p><span style="color:#000000">James et al. (2013): An Introduction to Statistical Learning with R Applications, p. 239f.</span></p>
<aside class="notes">
<p>Example: A marketing analyst interested in understanding people’s online shopping patterns could treat as features all of the search terms entered by users of a search engine. This is sometimes known as the “bag-ofwords” model. The same researcher might have access to the search histories of only a few hundred or a few thousand search engine users who have consented to share their information with the researcher. For a given user, each of the p search terms is scored present (0) or absent (1), creating a large binary feature vector. Then n ≈ 1,000 and p is much larger.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<p>Handling High-Dimensionality (II)</p>
<p>The figure illustrates the risk of applying OLS when the number of features p is large. The model R2 increases to 1 as the number of features increases, and the training set MSE decreases to 0. At the same time, the MSE on a test set becomes extremely large as the number of features increases.</p>
<p>In contrast, methods like ridge regression are particularly useful for performing regression in the high-dimensional setting. Essentially, these approaches avoid overfitting by using a less flexible fitting approach than least squares.</p>
<p><img data-src="../images/slides_307.png"></p>
<p><span style="color:#000000">Source: James et al. (2013): An Introduction to Statistical Learning with R Applications, p. 240f.</span></p>
<aside class="notes">
<p>Exercise on page 251!!!!!!!!!!!!!!</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.2 Regression</p>
<p>4.4.2.1 OLS</p>
<p>4.4.2.2 Ridge Regression</p>
<p><span style="color:#ff0000">4.4.2.3 Support Vector Regression</span></p>
<p>4.4.2.4 Neural Networks</p>
<p>4.4.2.5 Decision Trees</p>
<p>4.4.2.6 K-Nearest Neighbors</p>
</section>
<section class="slide level2">

<p>Support Vector Regression</p>
<p><span style="color:#000000">The Goal is to find a robust model with a high generalization ability.</span></p>
<p><span style="color:#000000">SVR regards two sources of Robustness:</span></p>
<p><span style="color:#000000">1. Eliminating Noise</span></p>
<p><span style="color:#000000">2. Handling Complexity</span></p>
</section>
<section class="slide level2">

<p>Insensitive Loss Function (I)</p>
<p><span style="color:#000000"> <strong>-insensitive Loss</strong> </span></p>
<p><img data-src="../images/slides_308.png"></p>
<p><span style="color:#000000">does not penalize acceptable deviations (defined by )</span></p>
</section>
<section class="slide level2">

<p>Insensitive Loss Function (II)</p>
<p>Using the e-insensitive loss function, only those data objects are considered in the estimation, which have a distance greater than e from the regression function:</p>
<p><img data-src="../images/slides_309.png"></p>
<p><img data-src="../images/slides_310.png"></p>
<p>e-insen-sitiveRegion</p>
<p>Every object inside the e-insensitive region is ignored. It is regarded as noise.</p>
</section>
<section class="slide level2">

<p>Support Vector Regression (I)</p>
<p><img data-src="../images/slides_311.png"></p>
<p><img data-src="../images/slides_312.png"></p>
<p><img data-src="../images/slides_313.png"></p>
<p>Decision criterion:</p>
<p><em>Choose the line with the smallest sum of error slopes with </em> <em>paying attention to the flatness of the regression line!</em></p>
</section>
<section class="slide level2">

<p>Estimating the SVR (Linear Case)</p>
</section>
<section class="slide level2">

<p>Nonlinearity (I)</p>
<p><span style="color:#000000"> <strong>The linear </strong> </span> <span style="color:#000000"> <strong>case</strong> </span> <span style="color:#000000"> <strong>:</strong> </span></p>
<p><span style="color:#000000"> <strong>The </strong> </span> <span style="color:#000000"> <strong>nonlinear</strong> </span> <span style="color:#000000"> __ __ </span> <span style="color:#000000"> <strong>case</strong> </span> <span style="color:#000000"> <strong>:</strong> </span></p>
</section>
<section class="slide level2">

<p>Nonlinearity (II)</p>
</section>
<section class="slide level2">

<p>Kernel Functions (I)</p>
<p><span style="color:#000000">Kernel Functions are used to project n-dimensional input to m-dimensional input, where m is higher than n:</span></p>
<p><span style="color:#000000">Any point x in the original space is mapped into the higher dimensional space. For reason of efficiency, the mapping is not performed in real but instead embedded in the model building process via the kernel function:</span></p>
<p><span style="color:#000000"> Instead of </span> <span style="color:#000000">ß</span> <span style="color:#000000">0</span> <span style="color:#000000"> </span> <span style="color:#000000">+ </span> <span style="color:#000000">ß · x</span> <span style="color:#000000"> = y </span> <span style="color:#000000">the following is used </span> <span style="color:#000000">ß</span> <span style="color:#000000">0</span> <span style="color:#000000"> </span> <span style="color:#000000">+ </span> <span style="color:#000000">ß · </span> <span style="color:#000000">F</span> <span style="color:#000000">(x) </span> <span style="color:#000000"> = y</span></p>
<p><span style="color:#000000">The main idea to use a kernel is: A linear regression curve in higher dimensions becomes a non-linear regression curve in lower dimensions.</span></p>
</section>
<section class="slide level2">

<p>Estimating the SVR (Nonlinear Case)</p>
</section>
<section class="slide level2">

<p>Kernel Functions (II)</p>
<p><span style="color:#000000">A frequently used kernel function is the Polynomial Kernel Function:</span></p>
<p><span style="color:#000000">where x and z </span> <span style="color:#000000">are vector points in any fixed dimensional space and n is the order of the kernel.</span></p>
<p><span style="color:#000000">In the case of order equal to 2, we get:</span></p>
<p>Source: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000173</p>
</section>
<section class="slide level2">

<p>Kernel Functions (III)</p>
<p><span style="color:#000000">A</span> <span style="color:#000000"> <strong>nother </strong> </span> <span style="color:#000000"> <strong>frequently</strong> </span> <span style="color:#000000"> __ __ </span> <span style="color:#000000"> <strong>used</strong> </span> <span style="color:#000000"> __ __ </span> <span style="color:#000000"> <strong>kernel</strong> </span> <span style="color:#000000"> __ __ </span> <span style="color:#000000"> <strong>function</strong> </span> <span style="color:#000000"> __ __ </span> <span style="color:#000000"> <strong>is</strong> </span> <span style="color:#000000"> __ __ </span> <span style="color:#000000"> <strong>the</strong> </span> <span style="color:#000000"> __ Radial Basis __ </span> <span style="color:#000000"> <strong>Function</strong> </span> <span style="color:#000000"> __ (__ </span> <span style="color:#000000"> <strong>RBF):</strong> </span></p>
<p><span style="color:#000000">It maps the data according a Gaussian function where Sigma (</span> <span style="color:#000000">s</span> <span style="color:#000000">) is a streching factor.</span></p>
<p><span style="color:#000000">Different Sigmas</span></p>
<p><span style="color:#000000">= Euclidean</span> <span style="color:#000000"> distance between x and z</span> <span style="color:#000000"> </span></p>
<p>Source: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1000173</p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.2 Regression</p>
<p>4.4.2.1 OLS</p>
<p>4.4.2.2 Ridge Regression</p>
<p>4.4.2.3 Support Vector Regression</p>
<p><span style="color:#ff0000">4.4.2.4 Neural Networks</span></p>
<p>4.4.2.5 Decision Trees</p>
<p>4.4.2.6 K-Nearest Neighbors</p>
</section>
<section class="slide level2">

<p>Using Neural Network for Regression</p>
<p>Artificial neural networks are often used for classification because of the relationship to logistic regression. Neural networks typically use a logistic activation function and output values from 0 to 1 like logistic regression.</p>
<p>But the continuous output of a net must not be interpreted as a probability, so neural networks can be used too for regression, to model complex and non-linear relationships.</p>
<p>The Singlelayer Perceptron corresponds to a linear regression while a Multilayer Perceptron is able to approximate nearly any function regard-less of the complexity and nonlinearity.</p>
<p>Because of the high complexity of the MLP, the models are usually very sensitive and have a tendency to overfitting.</p>
<p>There exist regularization methods, which make the networks better at generalizing beyond the training data.(see http://neuralnetworksanddeeplearning.com/chap3.html)</p>
</section>
<section class="slide level2">

<p>Neural Network (Multilayer Perceptron)</p>
<p><img data-src="../images/slides_319.png"></p>
<p>Source: http://www.sclgsummit.org/uploads/presentation/8934b2d0be055a2261f5d0320f5b59bb.pdf</p>
</section>
<section class="slide level2">

<p><img data-src="../images/slides_320.png"></p>
<p>Source: http://www.sclgsummit.org/uploads/presentation/8934b2d0be055a2261f5d0320f5b59bb.pdf</p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.2 Regression</p>
<p>4.4.2.1 OLS</p>
<p>4.4.2.2 Ridge Regression</p>
<p>4.4.2.3 Support Vector Regression</p>
<p>4.4.2.4 Neural Networks</p>
<p><span style="color:#ff0000">4.4.2.5 Decision Trees</span></p>
<p>4.4.2.6 K-Nearest Neighbors</p>
</section>
<section class="slide level2">

<p>Introductory Example</p>
<p>Decision Tree for Predicting Fuel Consumption of Cars(in Miles-per-Gallon )</p>
</section>
<section class="slide level2">

<p>Regression Trees</p>
<p>Some of the tree approaches can be used for regression too. They can be used for nonlinear multiple regression. The output must be numerical.</p>
<p>The figure shows a regression tree for predicting the salary of a baseball player, based on the number of years that he has played in the major leagues and the number of hits that he made in the previous year.</p>
<p>The predicted salary is given by the mean value of the salaries in the corresponding leaf, e.g. for the players in the data set with Years&lt;4.5, the mean (log-scaled) salary is 5.11, and so we make a prediction of e5.11 thousands of dollars, i.e. $165,670, for these players.</p>
<p>Players with Years&gt;=4.5 are assigned to the right branch, and then that group is further subdivided by Hits. The predicted salaries for the resulting two groups are 1,000*e6.00 =$403,428 and 1,000*e6.74 =$845,346.</p>
<p><img data-src="../images/slides_322.png"></p>
<p><span style="color:#000000">Source: James et al. (2013): An Introduction to Statistical Learning with R Applications, p. 304f.</span></p>
<aside class="notes">
<p>Example to do in R from Cart2.pdf</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<p>Constructing a Regression Tree (I)</p>
<p><img data-src="../images/slides_323.png"></p>
<p><span style="color:#000000">Source: James et al. (2013): An Introduction to Statistical Learning with R Applications, p. 305f.</span></p>
<aside class="notes">
<p>Example to do in R from Cart2.pdf</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<p>Constructing a Regression Tree (II)</p>
<aside class="notes">
<p>Example to do in R from Cart2.pdf</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section class="slide level2">

<p>Random Forests for Regression</p>
<p>Due to the usage of means as predictors a regression tree usually simplifies the true relationship between the inputs and the output. The advantage over traditional statistical methods is, that it can give valuable insights about which variables are important and where. But the prediction ability is poor compared to other regression approaches.</p>
<p>A much better prediction quality can be achieved with the creation of an ensemble of trees, use them for prediction and averaging their results. This is done, when applying the Random Forests approach to a regression task.</p>
<p>Regression Forests are an ensemble of different regression trees and are used for nonlinear multiple regression. The principle is the same as in classification, except that the output is not the result of a voting but instead of an averaging process.</p>
<p>The disadvantage of Random Forests is that the analysis, which aggregates over the results of many bootstrap trees, does not produce a single, easily interpretable tree diagram.</p>
</section>
<section class="slide level2">

<p>Comparing the Fitting Ability of one vs. many Regression Trees</p>
<p>Single Regression Tree</p>
<p>Average of 100 Regression Trees</p>
<p><img data-src="../images/slides_324.png"></p>
<p><img data-src="../images/slides_325.png"></p>
</section>
<section class="slide level2">

<p>Limitations of Tree Methods in Regression</p>
<p>When applied to regression problems, tree methods have the limitation that they cannot exceed the range of values of the target variable used in training. The reason for this lies in their design principle, how the leaves of the trees are created.</p>
<p>Thus, Random Forests may perform poorly when the target data is out of the range of the original training data, e.g. in the case of data with persistent trends. A solution may be a frequent re-training in this case.</p>
<p>An important strength of Random Forests is that they are able to perform still well in the case of missing data. According to their construction principle, not every tree is using the same features.</p>
<p>If there is any missing value for a feature during the application there usually are enough trees remaining that do not use this feature to produce accurate predictions.</p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.2 Regression</p>
<p>4.4.2.1 OLS</p>
<p>4.4.2.2 Ridge Regression</p>
<p>4.4.2.3 Support Vector Regression</p>
<p>4.4.2.4 Neural Networks</p>
<p>4.4.2.5 Decision Trees</p>
<p><span style="color:#ff0000">4.4.2.6 K-Nearest Neighbors</span></p>
</section>
<section class="slide level2">

<p>k-Nearest Neighbors for Regression</p>
<p>k-Nearest Neighbors cannot only be used for classification but also for regression. The only difference in regression is that the prediction is not the result of a majority vote but of an averaging process.</p>
<p>A simple implementation of KNN regression is to calculate the average of the numerical target of the k-nearest neighbors. Another approach uses an inverse distance weighted average of the K-nearest neighbors. KNN regression uses the same distance functions as KNN classification.</p>
<p>Example:</p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.2 Regression</p>
<p><span style="color:#c00000">4.4.3 Segmentation</span></p>
<p>4.4.3.1 K-Means</p>
<p>4.4.2.2 Hierarchical Cluster Analysis</p>
</section>
<section class="slide level2">

<p>Introductory Example</p>
<p>Assume you are a wholesale distributor and each row of your dataset corresponds to a customer showing the following attributes:</p>
<p>1) FRESH: annual spending on fresh products (Continuous); 2) MILK: annual spending on milk products (Continuous); 3) GROCERY: annual spending on grocery products (Continuous); 4) FROZEN: annual spending on frozen products (Continuous) 5) DETERGENTS_PAPER: annual spending on detergents and paper products (Continuous) 6) DELICATESSEN: annual spending on delicatessen products (Continuous); 7) CHANNEL: customers buying channel (Nominal) 8) REGION: customers region (Nominal)</p>
<p>Your goal is to segment the users. That means finding similar types of users and bunching them together.</p>
<p>Why would you want to do this?</p>
<p>You might want to give different users different experiences. Marketing often does this; for example, to offer toner to people who are known to own printers.</p>
<p>You might have a model that works better for specific groups. Or you might have different models for different groups.</p>
</section>
<section class="slide level2">

<p>Cluster Analysis</p>
<p><span style="color:#000000"> <strong>Cluster analysis is a type of multivariate statistical analysis. It is used to group data into separate clusters. The main objective of clustering is to find similarities between data objects, and then group similar objects together to assist in understanding relationships that might exist among them. Cluster analysis is based on a mathematical formulation of a measure of similarity. </strong> </span></p>
<p><span style="color:#000000"> <strong>There are different types of cluster analysis methods:</strong> </span></p>
<p><span style="color:#0000ff"> <strong>Clustering Methods</strong> </span></p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.2 Regression</p>
<p>4.4.3 Segmentation</p>
<p><span style="color:#c00000">4.4.3.1 K-Means</span></p>
<p>4.4.2.2 Hierarchical Cluster Analysis</p>
</section>
<section class="slide level2">

<p>Partitioning Cluster Methods</p>
<p><span style="color:#000000"> <strong>The partitioning cluster methods divide the data into a predetermined number of </strong> </span> <span style="color:#000000">clusters. The most popular technique is the K-Means algorithm.</span></p>
<p>Given a set of observations ( <em>x</em> 1, <em>x</em> 2,…, <em>x</em> <em>n</em> ), where each observation is a <em>m</em> -dimensional real vector, <em>k</em> -means clustering aims to partition the n observations into ( <em>k</em> ≤ <em>n</em> ) segments <em>S</em> ={ <em>S</em> 1, <em>S</em> 2,..., <em>S</em> <em>k</em> } so as to minimize the within-cluster sum of squares (WCSS).</p>
<p>The objective is to find</p>
<p>where _ _ is the mean of points in <em>S</em> <em>i</em> .</p>
</section>
<section class="slide level2">

<p><span style="color:#000000">Procedure of K-Means:</span></p>
<p><span style="color:#000000"> <strong>Step 1: Randomly partition the data objects into k clusters.</strong> </span></p>
<p><span style="color:#000000"> <strong>Step 2: Calculate the cluster centroids.</strong> </span></p>
<p><span style="color:#000000"> <strong>Step 3: Calculate the distance from every data point to all centroids</strong> </span></p>
<p><span style="color:#000000"> <strong>Step 4: If a data point is closest to its own centroid, leave it where it </strong> </span> <span style="color:#000000"> __ __ </span> <span style="color:#000000">is. If the data </span> <span style="color:#000000"> <strong>point is not closest to its own centroid, assign </strong> </span> <span style="color:#000000"> __ __ </span> <span style="color:#000000">it to the cluster with </span> <span style="color:#000000"> <strong>the closest centroid.</strong> </span></p>
<p><span style="color:#000000"> <strong>Step 5: Repeat the step 2 to 4 until a complete pass through of all </strong> </span> <span style="color:#000000"> __ __ </span> <span style="color:#000000">the data points </span> <span style="color:#000000"> <strong>results in no data point changing from one </strong> </span> <span style="color:#000000"> __ cluster to another.__ </span></p>
</section>
<section class="slide level2">

<p>Example of a K-Means Cluster Analysis</p>
<p><img data-src="../images/slides_327.png"></p>
</section>
<section class="slide level2">

<p><strong>Between cluster variance: </strong></p>
<p><strong>Within cluster variance: </strong></p>
<p>Finding the Optimal Number of Clusters (I)</p>
<p><span style="color:#000000"> <strong>The aim of the cluster analysis is the </strong> </span> <span style="color:#000000"> <strong>segmentation of objects into clusters, </strong> </span> <span style="color:#000000"> <strong>which are preferably homogeneous in </strong> </span> <span style="color:#000000"> <strong>it selves and heterogeneous to each other. </strong> </span> <span style="color:#000000"> <strong>The less variance exists within the clusters </strong> </span> <span style="color:#000000"> <strong>and the more variance exists between the </strong> </span> <span style="color:#000000"> <strong>clusters, the better is the number of clusters.</strong> </span></p>
<p><span style="color:#000000"> <strong>Total variance:</strong> </span></p>
<p><span style="color:#000000"> <strong>Accumulated variance within the k clusters:</strong> </span></p>
<p><span style="color:#000000"> <strong>This results in the variance between the clusters: </strong> </span></p>
<p><span style="color:#000000"> <strong>with n = number of objects</strong> </span></p>
<p><span style="color:#000000"> __ m = number of attributes__ </span></p>
<p><span style="color:#000000"> __ __ </span> <span style="color:#000000"> <strong>n</strong> </span> <span style="color:#000000"> <strong>k</strong> </span> <span style="color:#000000"> __ = number of objects in cluster k__ </span></p>
<p><span style="color:#000000"> __ __ </span> <span style="color:#000000"> <strong>c</strong> </span> <span style="color:#000000"> <strong>k</strong> </span> <span style="color:#000000"> __ = cluster k__ </span></p>
</section>
<section class="slide level2">

<p>Finding the Optimal Number of Clusters (II)</p>
<p><span style="color:#000000"> <strong>If you put V</strong> </span> <span style="color:#000000"> <strong>in</strong> </span> <span style="color:#000000"> __ on the ordinate and the number of cluster k on the abscissa, it often results in a curve with one or several kinks. At the point where exists the (first) significant kink, you can find the optimal number of clusters:__ </span></p>
<p><span style="color:#000000"> <strong>Total variance </strong> </span> <span style="color:#000000"> <strong>V</strong> </span> <span style="color:#000000"> <strong>tot</strong> </span></p>
<p><span style="color:#000000"> <strong>Between</strong> </span> <span style="color:#000000"> __ __ </span> <span style="color:#000000"> <strong>cluster variance </strong> </span> <span style="color:#000000"> <strong>V</strong> </span> <span style="color:#000000"> <strong>betw</strong> </span></p>
<p><span style="color:#000000">Within cluster variance </span> <span style="color:#000000"> <strong>V</strong> </span> <span style="color:#000000"> <strong>in</strong> </span></p>
<p><span style="color:#000000"> <strong>Number of clusters</strong> </span></p>
</section>
<section class="slide level2">

<p>Finding the Optimal Number of Clusters (III)</p>
<p>Instead of visually identifying the optimal cluster number, we can calculate the distances from the points on the elbow curve to a straight line linking the first and the last point on the curve. The cluster number with the largest distance is then chosen as the one with the strongest kink.</p>
<p><span style="color:#000000"> <strong>Number of clusters</strong> </span></p>
</section>
<section class="slide level2">

<p>4 Predictive Analytics</p>
<p>4.1 Subject of Predictive Analytics</p>
<p>4.2 The Analytics Process</p>
<p>4.3 Data Preparation</p>
<p>4.4 Methods, Algorithms and Applications</p>
<p><span style="color:#c00000"> </span> 4.4.1 Classification</p>
<p>4.4.2 Regression</p>
<p>4.4.3 Segmentation</p>
<p>4.4.3.1 K-Means</p>
<p><span style="color:#c00000">4.4.2.2 Hierarchical Cluster Analysis</span></p>
</section>
<section class="slide level2">

<p>Hierarchical Cluster Methods</p>
<ul>
<li><span style="color:#000000"> <strong>There are two types of hierarchical cluster methods: </strong> </span>
<ul>
<li><span style="color:#000000"> <strong>Agglomerative hierarchical clustering is a bottom-up clustering method. It starts with every single data object in a single cluster. Then, in each iteration, it agglomerates (merges) the closest pair of clusters by satisfying some similarity criteria, until all of the data is in one cluster. </strong> </span></li>
<li><span style="color:#000000"> <strong>Divisive hierarchical clustering is a top-down clustering method. It works in a similar way to agglomerative clustering but in the opposite direction. This method starts with a single cluster containing all data objects, and then successively splits resulting clusters until only clusters of individual data objects remain.</strong> </span></li>
</ul></li>
</ul>
</section>
<section class="slide level2">

<p>Process of the Hierarchical Cluster Analysis</p>
</section>
<section class="slide level2">

<p>Measuring Similarity between Clusters (I)</p>
<p><img data-src="../images/slides_328.png"></p>
<p><span style="color:#000000">Distance between two clusters is the distance between the closest points:</span></p>
<p><span style="color:#000000">Complete Linkage:</span></p>
<p><img data-src="../images/slides_329.png"></p>
<p><span style="color:#000000">Distance between two clusters is the distance between the farthest pair of points:</span></p>
<p><img data-src="../images/slides_330.png"></p>
<p><span style="color:#000000">Distance between two clusters </span> <span style="color:#000000">i</span> <span style="color:#000000"> and j is the distance between their </span> <span style="color:#000000">cendroids</span> <span style="color:#000000">:</span></p>
</section>
<section class="slide level2">

<p>Measuring Similarity between Clusters (II)</p>
<p><span style="color:#000000">Average Linkage:</span></p>
<p><span style="color:#000000">Distance between clusters is the average distance between the cluster points:</span></p>
<p><img data-src="../images/slides_331.png"></p>
<p><span style="color:#000000">Ward’s Method / Minimum Variance Method (only Agglomerative):</span></p>
<p><img data-src="../images/slides_332.png"></p>
<p><span style="color:#000000">Ward’s minimum variance criterion minimizes the total within-cluster variance. At each step the pair of clusters is merged that leads to minimum increase in total within-cluster variance after merging. This can be calculated as the square of the distance between cluster means divided by the sum of the reciprocals of the number of observations in each cluster: </span></p>
<p><span style="color:#000000">For a comparison of the methods see: Ferreira, L.; Hitchcock, D. B. (2009): A Comparison of Hierarchical Methods for Clustering Functional Data, http://people.stat.sc.edu/Hitchcock/compare_hier_fda.pdf</span></p>
</section>
<section class="slide level2">

<p>Single Linkage Example (I)</p>
<p><img data-src="../images/slides_333.png"></p>
<p><span style="color:#000000">Source: Fred, Ana: Unsupervised Learning, </span> <span style="color:#000000">Universidade</span> <span style="color:#000000"> </span> <span style="color:#000000">Técnica</span> <span style="color:#000000"> de </span> <span style="color:#000000">Lisboa</span></p>
</section>
<section class="slide level2">

<p>Single Linkage Example (II)</p>
<p><img data-src="../images/slides_334.png"></p>
<p><span style="color:#000000">Source: Fred, Ana: Unsupervised Learning, </span> <span style="color:#000000">Universidade</span> <span style="color:#000000"> </span> <span style="color:#000000">Técnica</span> <span style="color:#000000"> de </span> <span style="color:#000000">Lisboa</span></p>
</section>
<section class="slide level2">

<p>A dendrogram is a tree diagram frequently used to illustrate the arrangement of the clusters produced by hierarchical clustering. The y-axis represents the value of this distance metric (e.g. euclidean distance) between the clusters.</p>
<p>In a dendrogram the widths of the horizontal lines give an impression about the dissimilarity of the merging object. Thus, a good cluster number might be at a point from where the width of the following horizontal lines is significantly smaller in length. The red line in the graph below shows such a point:</p>
<p>Counting the points that cut this line might be a good answer for the number of clusters the data can have. It is the number 6 in this case.</p>
<p><img data-src="../images/slides_335.png"></p>


</section>
    </div>
  <div class="quarto-auto-generated-content" style="display: none;">
<div class="footer footer-default">

</div>
</div></div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="../site_libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="../site_libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="../site_libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="../site_libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="../site_libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="../site_libs/revealjs/plugin/notes/notes.js"></script>
  <script src="../site_libs/revealjs/plugin/search/search.js"></script>
  <script src="../site_libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="../site_libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'jumpToSlide': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.togglePdfExport(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleScrollView(event)\"><kbd>r</kbd> Scroll View Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1600,

        height: 900,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdn.jsdelivr.net/npm/mathjax@2.7.9/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
      window.document.addEventListener("DOMContentLoaded", function (event) {
        const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
        tabsets.forEach(function(tabset) {
          const tabby = new Tabby('#' + tabset.id);
        });
        const isCodeAnnotation = (el) => {
          for (const clz of el.classList) {
            if (clz.startsWith('code-annotation-')) {                     
              return true;
            }
          }
          return false;
        }
        const onCopySuccess = function(e) {
          // button target
          const button = e.trigger;
          // don't keep focus
          button.blur();
          // flash "checked"
          button.classList.add('code-copy-button-checked');
          var currentTitle = button.getAttribute("title");
          button.setAttribute("title", "Copied!");
          let tooltip;
          if (window.bootstrap) {
            button.setAttribute("data-bs-toggle", "tooltip");
            button.setAttribute("data-bs-placement", "left");
            button.setAttribute("data-bs-title", "Copied!");
            tooltip = new bootstrap.Tooltip(button, 
              { trigger: "manual", 
                customClass: "code-copy-button-tooltip",
                offset: [0, -8]});
            tooltip.show();    
          }
          setTimeout(function() {
            if (tooltip) {
              tooltip.hide();
              button.removeAttribute("data-bs-title");
              button.removeAttribute("data-bs-toggle");
              button.removeAttribute("data-bs-placement");
            }
            button.setAttribute("title", currentTitle);
            button.classList.remove('code-copy-button-checked');
          }, 1000);
          // clear code selection
          e.clearSelection();
        }
        const getTextToCopy = function(trigger) {
          const outerScaffold = trigger.parentElement.cloneNode(true);
          const codeEl = outerScaffold.querySelector('code');
          for (const childEl of codeEl.children) {
            if (isCodeAnnotation(childEl)) {
              childEl.remove();
            }
          }
          return codeEl.innerText;
        }
        const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
          text: getTextToCopy
        });
        clipboard.on('success', onCopySuccess);
        if (window.document.getElementById('quarto-embedded-source-code-modal')) {
          const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
            text: getTextToCopy,
            container: window.document.getElementById('quarto-embedded-source-code-modal')
          });
          clipboardModal.on('success', onCopySuccess);
        }
          var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
          var mailtoRegex = new RegExp(/^mailto:/);
            var filterRegex = new RegExp("https:\/\/fs-ise\.github\.io\/big-data-analytics\/");
          var isInternal = (href) => {
              return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
          }
          // Inspect non-navigation links and adorn them if external
         var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
          for (var i=0; i<links.length; i++) {
            const link = links[i];
            if (!isInternal(link.href)) {
              // undo the damage that might have been done by quarto-nav.js in the case of
              // links that we want to consider external
              if (link.dataset.originalHref !== undefined) {
                link.href = link.dataset.originalHref;
              }
            }
          }
        function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
          const config = {
            allowHTML: true,
            maxWidth: 500,
            delay: 100,
            arrow: false,
            appendTo: function(el) {
                return el.closest('section.slide') || el.parentElement;
            },
            interactive: true,
            interactiveBorder: 10,
            theme: 'light-border',
            placement: 'bottom-start',
          };
          if (contentFn) {
            config.content = contentFn;
          }
          if (onTriggerFn) {
            config.onTrigger = onTriggerFn;
          }
          if (onUntriggerFn) {
            config.onUntrigger = onUntriggerFn;
          }
            config['offset'] = [0,0];
            config['maxWidth'] = 700;
          window.tippy(el, config); 
        }
        const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
        for (var i=0; i<noterefs.length; i++) {
          const ref = noterefs[i];
          tippyHover(ref, function() {
            // use id or data attribute instead here
            let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
            try { href = new URL(href).hash; } catch {}
            const id = href.replace(/^#\/?/, "");
            const note = window.document.getElementById(id);
            if (note) {
              return note.innerHTML;
            } else {
              return "";
            }
          });
        }
        const findCites = (el) => {
          const parentEl = el.parentElement;
          if (parentEl) {
            const cites = parentEl.dataset.cites;
            if (cites) {
              return {
                el,
                cites: cites.split(' ')
              };
            } else {
              return findCites(el.parentElement)
            }
          } else {
            return undefined;
          }
        };
        var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
        for (var i=0; i<bibliorefs.length; i++) {
          const ref = bibliorefs[i];
          const citeInfo = findCites(ref);
          if (citeInfo) {
            tippyHover(citeInfo.el, function() {
              var popup = window.document.createElement('div');
              citeInfo.cites.forEach(function(cite) {
                var citeDiv = window.document.createElement('div');
                citeDiv.classList.add('hanging-indent');
                citeDiv.classList.add('csl-entry');
                var biblioDiv = window.document.getElementById('ref-' + cite);
                if (biblioDiv) {
                  citeDiv.innerHTML = biblioDiv.innerHTML;
                }
                popup.appendChild(citeDiv);
              });
              return popup.innerHTML;
            });
          }
        }
      });
      </script>
    

</body></html>